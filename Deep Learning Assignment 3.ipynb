{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3Fg5hWUA83b"
      },
      "source": [
        "# Imports and Downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14GVo-_uA_ib"
      },
      "source": [
        "# Global libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.io import arff\n",
        "import time\n",
        "import pandas as pd\n",
        "from numpy import vstack\n",
        "import random\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dD_3_tF2qQq"
      },
      "source": [
        "# Sklearn - Preprocessa, RF\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, OrdinalEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJsqThLlAVCc",
        "outputId": "104c69ee-532b-41c7-c7d8-46f127f4dcad"
      },
      "source": [
        "pip install arff"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting arff\n",
            "  Downloading https://files.pythonhosted.org/packages/50/de/62d4446c5a6e459052c2f2d9490c370ddb6abc0766547b4cef585913598d/arff-0.9.tar.gz\n",
            "Building wheels for collected packages: arff\n",
            "  Building wheel for arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for arff: filename=arff-0.9-cp37-none-any.whl size=4970 sha256=deea545785560cc0e1fa457556f07b01e7701f11af7c9793ec10d7dd838a3231\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/d0/70/2c73afedd3ac25c6085b528742c69b9587cbdfa67e5194583b\n",
            "Successfully built arff\n",
            "Installing collected packages: arff\n",
            "Successfully installed arff-0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ7qYDAu7LFA"
      },
      "source": [
        "# GAN libraries\n",
        "import arff\n",
        "from scipy.io.arff import loadarff\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Activation, Flatten, Dropout, LeakyReLU, Dropout, Concatenate, Input, Embedding\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJif3LH7-Pfh"
      },
      "source": [
        "# BB Model libraries\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNrrLL4wl8_p"
      },
      "source": [
        "# Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPWL3uUFBCxb"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go4u57TQz6WD",
        "outputId": "449f8acb-592f-4131-f26f-32368d255e77"
      },
      "source": [
        "# load content from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoMLXS9oA1kl"
      },
      "source": [
        "data_base_path = '/content/drive/MyDrive/DeepLearningAss4/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxHkrq3QBL6d"
      },
      "source": [
        "# load data into dataframe\n",
        "def load_data(path_arff):\n",
        "  dataset = arff.loadarff(data_base_path + path_arff)\n",
        "  data = pd.DataFrame(dataset[0])\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "cljMauDfBcjL",
        "outputId": "075911b5-ea32-48c3-b772-fafb549648cb"
      },
      "source": [
        "# Load data_diabetes \n",
        "data_diabetes = load_data('diabetes.arff')\n",
        "data_diabetes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preg</th>\n",
              "      <th>plas</th>\n",
              "      <th>pres</th>\n",
              "      <th>skin</th>\n",
              "      <th>insu</th>\n",
              "      <th>mass</th>\n",
              "      <th>pedi</th>\n",
              "      <th>age</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50.0</td>\n",
              "      <td>b'tested_positive'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31.0</td>\n",
              "      <td>b'tested_negative'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32.0</td>\n",
              "      <td>b'tested_positive'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21.0</td>\n",
              "      <td>b'tested_negative'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33.0</td>\n",
              "      <td>b'tested_positive'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63.0</td>\n",
              "      <td>b'tested_negative'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27.0</td>\n",
              "      <td>b'tested_negative'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30.0</td>\n",
              "      <td>b'tested_negative'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47.0</td>\n",
              "      <td>b'tested_positive'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23.0</td>\n",
              "      <td>b'tested_negative'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     preg   plas  pres  skin   insu  mass   pedi   age               class\n",
              "0     6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0  b'tested_positive'\n",
              "1     1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0  b'tested_negative'\n",
              "2     8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0  b'tested_positive'\n",
              "3     1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0  b'tested_negative'\n",
              "4     0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0  b'tested_positive'\n",
              "..    ...    ...   ...   ...    ...   ...    ...   ...                 ...\n",
              "763  10.0  101.0  76.0  48.0  180.0  32.9  0.171  63.0  b'tested_negative'\n",
              "764   2.0  122.0  70.0  27.0    0.0  36.8  0.340  27.0  b'tested_negative'\n",
              "765   5.0  121.0  72.0  23.0  112.0  26.2  0.245  30.0  b'tested_negative'\n",
              "766   1.0  126.0  60.0   0.0    0.0  30.1  0.349  47.0  b'tested_positive'\n",
              "767   1.0   93.0  70.0  31.0    0.0  30.4  0.315  23.0  b'tested_negative'\n",
              "\n",
              "[768 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5MFuyeKGE0I",
        "outputId": "7b2936b5-df36-4ad4-9029-b3e7369b9d26"
      },
      "source": [
        "data_diabetes.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "1Qd7Imy16t7B",
        "outputId": "79f5543d-cc71-402f-a041-95dde4f21143"
      },
      "source": [
        "# load data_german_credit\n",
        "data_german_credit = load_data('german_credit.arff')\n",
        "data_german_credit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>6.0</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>1169.0</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>67.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b'A12'</td>\n",
              "      <td>48.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>5951.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A92'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>22.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'2'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b'A14'</td>\n",
              "      <td>12.0</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A46'</td>\n",
              "      <td>2096.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A74'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>3.0</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>49.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A172'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>42.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A42'</td>\n",
              "      <td>7882.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A74'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A103'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A122'</td>\n",
              "      <td>45.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A153'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>24.0</td>\n",
              "      <td>b'A33'</td>\n",
              "      <td>b'A40'</td>\n",
              "      <td>4870.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>3.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A124'</td>\n",
              "      <td>53.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A153'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'2'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>b'A14'</td>\n",
              "      <td>12.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A42'</td>\n",
              "      <td>1736.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A74'</td>\n",
              "      <td>3.0</td>\n",
              "      <td>b'A92'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>31.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A172'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>30.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A41'</td>\n",
              "      <td>3857.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A91'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A122'</td>\n",
              "      <td>40.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A174'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>b'A14'</td>\n",
              "      <td>12.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>804.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A123'</td>\n",
              "      <td>38.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>45.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>1845.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A124'</td>\n",
              "      <td>23.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A153'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'2'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>b'A12'</td>\n",
              "      <td>45.0</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A41'</td>\n",
              "      <td>4576.0</td>\n",
              "      <td>b'A62'</td>\n",
              "      <td>b'A71'</td>\n",
              "      <td>3.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A123'</td>\n",
              "      <td>27.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          1     2       3       4       5  ...       17   18       19       20    21\n",
              "0    b'A11'   6.0  b'A34'  b'A43'  1169.0  ...  b'A173'  1.0  b'A192'  b'A201'  b'1'\n",
              "1    b'A12'  48.0  b'A32'  b'A43'  5951.0  ...  b'A173'  1.0  b'A191'  b'A201'  b'2'\n",
              "2    b'A14'  12.0  b'A34'  b'A46'  2096.0  ...  b'A172'  2.0  b'A191'  b'A201'  b'1'\n",
              "3    b'A11'  42.0  b'A32'  b'A42'  7882.0  ...  b'A173'  2.0  b'A191'  b'A201'  b'1'\n",
              "4    b'A11'  24.0  b'A33'  b'A40'  4870.0  ...  b'A173'  2.0  b'A191'  b'A201'  b'2'\n",
              "..      ...   ...     ...     ...     ...  ...      ...  ...      ...      ...   ...\n",
              "995  b'A14'  12.0  b'A32'  b'A42'  1736.0  ...  b'A172'  1.0  b'A191'  b'A201'  b'1'\n",
              "996  b'A11'  30.0  b'A32'  b'A41'  3857.0  ...  b'A174'  1.0  b'A192'  b'A201'  b'1'\n",
              "997  b'A14'  12.0  b'A32'  b'A43'   804.0  ...  b'A173'  1.0  b'A191'  b'A201'  b'1'\n",
              "998  b'A11'  45.0  b'A32'  b'A43'  1845.0  ...  b'A173'  1.0  b'A192'  b'A201'  b'2'\n",
              "999  b'A12'  45.0  b'A34'  b'A41'  4576.0  ...  b'A173'  1.0  b'A191'  b'A201'  b'1'\n",
              "\n",
              "[1000 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT9LEcFbjzZk",
        "outputId": "bdf97181-a660-4a3a-99b9-cfbfa7290733"
      },
      "source": [
        "data_german_credit.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHOvsmpIDrqp",
        "outputId": "c41866c7-93a6-4861-a94e-1e8034bc650e"
      },
      "source": [
        "# data german credit data types\n",
        "data_german_credit.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 21 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   1       1000 non-null   object \n",
            " 1   2       1000 non-null   float64\n",
            " 2   3       1000 non-null   object \n",
            " 3   4       1000 non-null   object \n",
            " 4   5       1000 non-null   float64\n",
            " 5   6       1000 non-null   object \n",
            " 6   7       1000 non-null   object \n",
            " 7   8       1000 non-null   float64\n",
            " 8   9       1000 non-null   object \n",
            " 9   10      1000 non-null   object \n",
            " 10  11      1000 non-null   float64\n",
            " 11  12      1000 non-null   object \n",
            " 12  13      1000 non-null   float64\n",
            " 13  14      1000 non-null   object \n",
            " 14  15      1000 non-null   object \n",
            " 15  16      1000 non-null   float64\n",
            " 16  17      1000 non-null   object \n",
            " 17  18      1000 non-null   float64\n",
            " 18  19      1000 non-null   object \n",
            " 19  20      1000 non-null   object \n",
            " 20  21      1000 non-null   object \n",
            "dtypes: float64(7), object(14)\n",
            "memory usage: 164.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5XFnUmikTLw"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9xHMys1Sh0t"
      },
      "source": [
        "# transform categorical columns to one hot vectors\n",
        "def one_hot_encoding(df):\n",
        "  return pd.get_dummies(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tindYkOyFb8o"
      },
      "source": [
        "# transform categorical-ordinal columns to ordinal numbers\n",
        "def ordinal_encoding(df):\n",
        "  ord_enc = OrdinalEncoder()\n",
        "  df_encoded = ord_enc.fit_transform(df)\n",
        "  return df_encoded, ord_enc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRPcyr_rkSn3"
      },
      "source": [
        "# scale values between -1 and 1\n",
        "def scale(df):\n",
        "  mms = MinMaxScaler(feature_range=(-1,1))\n",
        "  data_scaled = mms.fit_transform(df)\n",
        "  return data_scaled, mms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3kHigQgPzju"
      },
      "source": [
        "##### Diabetes Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dg74GXYSayU"
      },
      "source": [
        "# pre process data diabetes data - numerical values only so used only scaling\n",
        "data_diabetes_scaled, mms_diabetes = scale(data_diabetes.iloc[:,:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4klB92MPxed"
      },
      "source": [
        "##### German Credit Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfTZGggNFkDZ"
      },
      "source": [
        "# german credit data - datatypes of columns according to datasets documnetation\n",
        "categorical = [2,3,8,9,11,13,14,16,18,19]\n",
        "ordinal = [0,5,6]\n",
        "numerical = [1,4,7,10,12,15,17]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du1mnSySFpXA"
      },
      "source": [
        "data_german_credit_encoded = pd.DataFrame()\n",
        "# encode categorical features to one hot vectors\n",
        "data_german_credit_encoded = one_hot_encoding(data_german_credit.iloc[:,categorical])\n",
        "# encode categorical-ordinal features to numberic\n",
        "data_german_credit_encoded[ordinal], ord_enc_german = ordinal_encoding(data_german_credit.iloc[:, ordinal])\n",
        "# scale all values between -1 to 1\n",
        "data_german_credit_encoded[numerical] = data_german_credit.iloc[:, numerical]\n",
        "numeric_col = ordinal + numerical\n",
        "data_german_credit_encoded, mms_german = scale(data_german_credit_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ASHOqM1axY"
      },
      "source": [
        "##### Shuffle & Batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbTdf5Stu6-U"
      },
      "source": [
        "# split dataset into batches and shuffle\n",
        "def shuffle_batch(df):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(df)\n",
        "  train_dataset = dataset.shuffle(len(dataset)).batch(32)\n",
        "  return train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCskLevkSIle"
      },
      "source": [
        "# split datasets to batches and shuffle\n",
        "train_data_diabetes = shuffle_batch(data_diabetes_scaled)\n",
        "train_data_german = shuffle_batch(data_german_credit_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlFE5JU4Apnh"
      },
      "source": [
        "## Diabets Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfTQYDvfqmN9"
      },
      "source": [
        "#### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkALaSGBis_m"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "latent_dim = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh3djHv8GYVS"
      },
      "source": [
        "### GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfApNnknVYwg"
      },
      "source": [
        "https://arxiv.org/pdf/1904.09135.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JCalf_O7A9w"
      },
      "source": [
        "#### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H20fvydp7ADn"
      },
      "source": [
        "# build discriminator model\n",
        "def build_disc(input_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(256, input_shape=(input_shape,)))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(1, activation='sigmoid')) \n",
        "\t# compile model\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])  \n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unDUnwWV7DE3"
      },
      "source": [
        "#### Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvMvsZcZGX4r"
      },
      "source": [
        "# build generator model\n",
        "def build_gen(output_shape, noise_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(512, input_shape=(noise_shape,)))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(256))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(output_shape, activation='tanh')) \n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQH0gSO7rov9"
      },
      "source": [
        "#### Test Discrimnator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g0HyOjkqZPU",
        "outputId": "c8cd6942-b08a-4b70-bca7-3290b855e448"
      },
      "source": [
        "# test discriminator without training the generatorto see if it succeeds in the classification task\n",
        "model_disc = build_disc(8)\n",
        "model_gen = build_gen(8, latent_dim)\n",
        "noise = tf.random.normal([128, latent_dim])\n",
        "X_fake = model_gen.predict(noise)\n",
        "x, y = vstack((data_diabetes_scaled, X_fake)), vstack((np.ones((data_diabetes_scaled.shape[0],1)), np.zeros((128,1))))\n",
        "history = model_disc.fit(x, y, epochs = 30) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 256)               2304      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 134,401\n",
            "Trainable params: 134,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 512)               4608      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 8)                 2056      \n",
            "=================================================================\n",
            "Total params: 137,992\n",
            "Trainable params: 137,992\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.5578 - accuracy: 0.8594\n",
            "Epoch 2/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3513 - accuracy: 0.8839\n",
            "Epoch 3/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.2324 - accuracy: 0.8929\n",
            "Epoch 4/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1708 - accuracy: 0.9185\n",
            "Epoch 5/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1370 - accuracy: 0.9408\n",
            "Epoch 6/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.1117 - accuracy: 0.9688\n",
            "Epoch 7/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0918 - accuracy: 0.9833\n",
            "Epoch 8/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0738 - accuracy: 0.9900\n",
            "Epoch 9/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0595 - accuracy: 0.9967\n",
            "Epoch 10/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0474 - accuracy: 0.9989\n",
            "Epoch 11/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0376 - accuracy: 1.0000\n",
            "Epoch 12/30\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 1.0000\n",
            "Epoch 13/30\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 1.0000\n",
            "Epoch 14/30\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 1.0000\n",
            "Epoch 15/30\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 1.0000\n",
            "Epoch 16/30\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0151 - accuracy: 1.0000\n",
            "Epoch 17/30\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.9989\n",
            "Epoch 18/30\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 1.0000\n",
            "Epoch 19/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9989\n",
            "Epoch 20/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0091 - accuracy: 1.0000\n",
            "Epoch 21/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 22/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 23/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 24/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 25/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 26/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 27/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 28/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 29/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 30/30\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g532T6N_rUve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a402a3-386f-4fab-9141-b5a7a835ad5b"
      },
      "source": [
        "# evaluate on real data\n",
        "model_disc.evaluate(data_diabetes_scaled, np.ones((data_diabetes_scaled.shape[0],1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0014943027636036277, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WIXEVxLrfKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0a17318-ffdb-453f-fcdf-001dd0e17c55"
      },
      "source": [
        "# evaluate on noise\n",
        "noise = tf.random.normal([128, latent_dim])\n",
        "X_fake = model_gen.predict(noise)\n",
        "model_disc.evaluate(X_fake, np.zeros((128,1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.012430811300873756, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrBcxS3a_KgS"
      },
      "source": [
        "### Run Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj8gm7T86NFD"
      },
      "source": [
        "##### plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rgkyjhWEt4W"
      },
      "source": [
        "# create a line plot of loss for the gan and save to file\n",
        "def plot_history(d_hist, g_hist, d_acc_hist, adv_acc_hist):\n",
        "  # plot loss\n",
        "  pyplot.subplot(2, 1, 1)\n",
        "  pyplot.plot(d_hist, label='discriminator loss')\n",
        "  pyplot.plot(g_hist, label='generetor loss')\n",
        "  pyplot.legend()\n",
        "  pyplot.title(\"Discriminator and Generator Loss\")\n",
        "  pyplot.xlabel(\"steps\")\n",
        "  pyplot.ylabel(\"loss\")\n",
        "  # plot discriminator accuracy\n",
        "  pyplot.subplot(2, 1, 2)\n",
        "  pyplot.plot(d_acc_hist, label='disc_acc')\n",
        "  pyplot.plot(adv_acc_hist, label='adv_acc')\n",
        "  pyplot.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOqeSsJ-6Oxb"
      },
      "source": [
        "#### losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_rq_c6W_KEd"
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "\n",
        "# calcuate the discriminator loss based on https://www.tensorflow.org/tutorials/generative/dcgan\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss, real_loss, fake_loss\n",
        "\n",
        "# calcuate the generator loss based on https://www.tensorflow.org/tutorials/generative/dcgan\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS31jnL0rorM"
      },
      "source": [
        "# define optimizers and metrics\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2*1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2*1e-4)\n",
        "\n",
        "disc_accuracy_tracker = tf.keras.metrics.Accuracy(name=\"disc_accuracy\")\n",
        "adv_accuracy_tracker = tf.keras.metrics.Accuracy(name=\"adv_accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncEybhgTuiWW"
      },
      "source": [
        "# calculate discriminator loss on real and fake samples separately - while interval is close to 1 and 0\n",
        "def discriminator_real_loss(real_output):\n",
        "    real_loss = cross_entropy(tf.random.uniform((real_output.shape[0],1),minval=0.7, maxval=1.2), real_output)\n",
        "    return real_loss\n",
        "\n",
        "def discriminator_fake_loss(fake_output):\n",
        "    fake_loss = cross_entropy(tf.random.uniform((fake_output.shape[0],1),minval=0.0, maxval=0.3), fake_output)\n",
        "    return fake_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQIgw2O96QTs"
      },
      "source": [
        "#### train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBCSRdyP_g_r"
      },
      "source": [
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(records):\n",
        "    num_sampels = len(records)\n",
        "    \n",
        "    noise = tf.random.normal([num_sampels, latent_dim]) # random noise\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape, tf.GradientTape() as disc_fake_tape:  \n",
        "      generated_records = generator(noise, training=True) # generate samples from noise\n",
        "\n",
        "      real_output = discriminator(records, training=True) # feed disc with real data\n",
        "      fake_output = discriminator(generated_records, training=True) # feed disc with fake data\n",
        "      \n",
        "      gen_loss = generator_loss(fake_output)  # calculate gen loss on fake data\n",
        "      total_disc_loss, real_loss, fake_loss  = discriminator_loss(real_output, fake_output) # calculate disc loss on both fake and real data\n",
        "      real_new_loss = discriminator_real_loss(real_output)  # calculate disc loss on real data\n",
        "      fake_new_loss = discriminator_fake_loss(fake_output)  # calculate disc loss on fake data\n",
        "\n",
        "    # caluclate gradients of gen and disc\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator_real = disc_tape.gradient(real_new_loss, discriminator.trainable_variables)\n",
        "    gradients_of_discriminator_fake = disc_fake_tape.gradient(fake_new_loss, discriminator.trainable_variables)\n",
        "\n",
        "    # apply gradients to both of the models\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator_real, discriminator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator_fake, discriminator.trainable_variables))\n",
        "\n",
        "    # round prediction\n",
        "    rounded_real = tf.math.round(real_output)\n",
        "    rounded_fake = tf.math.round(fake_output)\n",
        "    \n",
        "    # cocatenate disc prediction\n",
        "    disc_output = tf.concat([rounded_real,rounded_fake], 0)\n",
        "    disc_real = tf.concat([np.ones((num_sampels, 1)), np.zeros((num_sampels, 1))], axis =0)\n",
        "    \n",
        "    # calculate disc accuracy\n",
        "    disc_accuracy_tracker.update_state(disc_real, disc_output)\n",
        "    adv_accuracy_tracker.update_state(np.ones((num_sampels, 1)), rounded_fake)\n",
        "\n",
        "    return total_disc_loss, real_loss, fake_loss, gen_loss, disc_accuracy_tracker.result(), adv_accuracy_tracker.result()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vqLQpDa_i0l"
      },
      "source": [
        "d_hist, d_r_hist, d_f_hist, g_hist, d_acc_hist, adv_acc_hist = list(), list(), list(), list(), list(), list()\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    for image_batch in dataset:\n",
        "      d_loss, real_loss, fake_loss, g_loss, d_accuracy, g_accuracy =  train_step(image_batch)\n",
        "      # record history\n",
        "      d_hist.append(d_loss.numpy())\n",
        "      d_r_hist.append(real_loss.numpy())\n",
        "      d_f_hist.append(fake_loss.numpy())\n",
        "      g_hist.append(g_loss.numpy())\n",
        "      d_acc_hist.append(d_accuracy.numpy())\n",
        "      adv_acc_hist.append(g_accuracy.numpy())\n",
        "    print('>%d, d_loss=%.3f,d_R_loss=%.3f,d_f_loss=%.3f, g_loss=%.3f, d_acc=%d, aadv_acc=%d' %\n",
        "\t\t\t(epoch, d_hist[-1], d_r_hist[-1], d_f_hist[-1],  g_hist[-1], int(100*d_acc_hist[-1]), int(100*adv_acc_hist[-1])))\n",
        "  \n",
        "  plot_history(d_r_hist, d_f_hist, d_acc_hist, adv_acc_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CfXkJUJLBZhc",
        "outputId": "ac3280fb-3ebf-48f7-8458-6a2455a06e32"
      },
      "source": [
        "discriminator = build_disc(8)\n",
        "generator = build_gen(8, latent_dim)\n",
        "train(train_data_diabetes, 200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_42 (Dense)             (None, 256)               2304      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_28 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_29 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 134,401\n",
            "Trainable params: 134,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_45 (Dense)             (None, 512)               4608      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_30 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_31 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 8)                 2056      \n",
            "=================================================================\n",
            "Total params: 137,992\n",
            "Trainable params: 137,992\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            ">0, d_loss=1.357,d_R_loss=0.259,d_f_loss=1.099, g_loss=0.414, d_acc=50, aadv_acc=98\n",
            ">1, d_loss=1.630,d_R_loss=0.642,d_f_loss=0.988, g_loss=0.469, d_acc=48, aadv_acc=99\n",
            ">2, d_loss=1.042,d_R_loss=0.576,d_f_loss=0.466, g_loss=0.993, d_acc=54, aadv_acc=79\n",
            ">3, d_loss=1.262,d_R_loss=0.510,d_f_loss=0.752, g_loss=0.667, d_acc=60, aadv_acc=68\n",
            ">4, d_loss=1.370,d_R_loss=0.794,d_f_loss=0.577, g_loss=0.830, d_acc=56, aadv_acc=70\n",
            ">5, d_loss=0.821,d_R_loss=0.469,d_f_loss=0.352, g_loss=1.222, d_acc=61, aadv_acc=58\n",
            ">6, d_loss=0.649,d_R_loss=0.250,d_f_loss=0.398, g_loss=1.150, d_acc=66, aadv_acc=50\n",
            ">7, d_loss=1.555,d_R_loss=0.525,d_f_loss=1.030, g_loss=0.561, d_acc=68, aadv_acc=47\n",
            ">8, d_loss=1.518,d_R_loss=0.592,d_f_loss=0.926, g_loss=0.568, d_acc=67, aadv_acc=48\n",
            ">9, d_loss=0.954,d_R_loss=0.418,d_f_loss=0.536, g_loss=1.139, d_acc=67, aadv_acc=45\n",
            ">10, d_loss=0.711,d_R_loss=0.400,d_f_loss=0.311, g_loss=1.383, d_acc=69, aadv_acc=42\n",
            ">11, d_loss=0.839,d_R_loss=0.559,d_f_loss=0.280, g_loss=1.466, d_acc=71, aadv_acc=39\n",
            ">12, d_loss=0.513,d_R_loss=0.287,d_f_loss=0.226, g_loss=1.762, d_acc=73, aadv_acc=36\n",
            ">13, d_loss=0.965,d_R_loss=0.558,d_f_loss=0.406, g_loss=1.333, d_acc=73, aadv_acc=35\n",
            ">14, d_loss=1.158,d_R_loss=0.403,d_f_loss=0.755, g_loss=1.007, d_acc=73, aadv_acc=35\n",
            ">15, d_loss=1.465,d_R_loss=0.785,d_f_loss=0.681, g_loss=0.815, d_acc=72, aadv_acc=36\n",
            ">16, d_loss=0.768,d_R_loss=0.456,d_f_loss=0.311, g_loss=1.359, d_acc=72, aadv_acc=35\n",
            ">17, d_loss=0.711,d_R_loss=0.310,d_f_loss=0.401, g_loss=1.345, d_acc=73, aadv_acc=33\n",
            ">18, d_loss=1.118,d_R_loss=0.399,d_f_loss=0.719, g_loss=0.787, d_acc=74, aadv_acc=32\n",
            ">19, d_loss=2.048,d_R_loss=0.905,d_f_loss=1.143, g_loss=0.403, d_acc=73, aadv_acc=34\n",
            ">20, d_loss=1.408,d_R_loss=0.724,d_f_loss=0.685, g_loss=0.713, d_acc=70, aadv_acc=36\n",
            ">21, d_loss=1.168,d_R_loss=0.585,d_f_loss=0.582, g_loss=0.852, d_acc=71, aadv_acc=35\n",
            ">22, d_loss=1.233,d_R_loss=0.500,d_f_loss=0.733, g_loss=0.679, d_acc=71, aadv_acc=35\n",
            ">23, d_loss=1.488,d_R_loss=0.631,d_f_loss=0.858, g_loss=0.572, d_acc=70, aadv_acc=37\n",
            ">24, d_loss=1.389,d_R_loss=0.619,d_f_loss=0.770, g_loss=0.643, d_acc=69, aadv_acc=39\n",
            ">25, d_loss=1.277,d_R_loss=0.500,d_f_loss=0.777, g_loss=0.627, d_acc=69, aadv_acc=40\n",
            ">26, d_loss=1.338,d_R_loss=0.512,d_f_loss=0.826, g_loss=0.599, d_acc=68, aadv_acc=41\n",
            ">27, d_loss=1.287,d_R_loss=0.452,d_f_loss=0.835, g_loss=0.594, d_acc=68, aadv_acc=42\n",
            ">28, d_loss=1.469,d_R_loss=0.581,d_f_loss=0.889, g_loss=0.543, d_acc=67, aadv_acc=43\n",
            ">29, d_loss=1.490,d_R_loss=0.542,d_f_loss=0.947, g_loss=0.509, d_acc=66, aadv_acc=45\n",
            ">30, d_loss=1.585,d_R_loss=0.620,d_f_loss=0.965, g_loss=0.503, d_acc=65, aadv_acc=47\n",
            ">31, d_loss=1.444,d_R_loss=0.519,d_f_loss=0.926, g_loss=0.584, d_acc=65, aadv_acc=48\n",
            ">32, d_loss=1.381,d_R_loss=0.541,d_f_loss=0.840, g_loss=0.583, d_acc=65, aadv_acc=48\n",
            ">33, d_loss=1.327,d_R_loss=0.523,d_f_loss=0.805, g_loss=0.605, d_acc=64, aadv_acc=50\n",
            ">34, d_loss=1.483,d_R_loss=0.636,d_f_loss=0.847, g_loss=0.580, d_acc=64, aadv_acc=50\n",
            ">35, d_loss=1.478,d_R_loss=0.626,d_f_loss=0.852, g_loss=0.565, d_acc=63, aadv_acc=51\n",
            ">36, d_loss=1.415,d_R_loss=0.603,d_f_loss=0.812, g_loss=0.590, d_acc=63, aadv_acc=53\n",
            ">37, d_loss=1.392,d_R_loss=0.561,d_f_loss=0.832, g_loss=0.580, d_acc=62, aadv_acc=54\n",
            ">38, d_loss=1.386,d_R_loss=0.569,d_f_loss=0.817, g_loss=0.591, d_acc=62, aadv_acc=55\n",
            ">39, d_loss=1.311,d_R_loss=0.537,d_f_loss=0.774, g_loss=0.630, d_acc=61, aadv_acc=55\n",
            ">40, d_loss=1.277,d_R_loss=0.546,d_f_loss=0.730, g_loss=0.667, d_acc=61, aadv_acc=56\n",
            ">41, d_loss=1.374,d_R_loss=0.605,d_f_loss=0.769, g_loss=0.631, d_acc=61, aadv_acc=56\n",
            ">42, d_loss=1.475,d_R_loss=0.655,d_f_loss=0.820, g_loss=0.596, d_acc=61, aadv_acc=56\n",
            ">43, d_loss=1.396,d_R_loss=0.638,d_f_loss=0.758, g_loss=0.648, d_acc=60, aadv_acc=57\n",
            ">44, d_loss=1.132,d_R_loss=0.541,d_f_loss=0.590, g_loss=0.813, d_acc=61, aadv_acc=56\n",
            ">45, d_loss=1.271,d_R_loss=0.546,d_f_loss=0.725, g_loss=0.680, d_acc=61, aadv_acc=55\n",
            ">46, d_loss=1.345,d_R_loss=0.569,d_f_loss=0.776, g_loss=0.632, d_acc=61, aadv_acc=56\n",
            ">47, d_loss=1.539,d_R_loss=0.720,d_f_loss=0.819, g_loss=0.617, d_acc=60, aadv_acc=56\n",
            ">48, d_loss=1.296,d_R_loss=0.608,d_f_loss=0.688, g_loss=0.765, d_acc=60, aadv_acc=56\n",
            ">49, d_loss=1.299,d_R_loss=0.489,d_f_loss=0.810, g_loss=0.650, d_acc=60, aadv_acc=55\n",
            ">50, d_loss=1.365,d_R_loss=0.552,d_f_loss=0.813, g_loss=0.625, d_acc=60, aadv_acc=55\n",
            ">51, d_loss=1.485,d_R_loss=0.569,d_f_loss=0.916, g_loss=0.540, d_acc=60, aadv_acc=55\n",
            ">52, d_loss=1.339,d_R_loss=0.611,d_f_loss=0.728, g_loss=0.693, d_acc=60, aadv_acc=56\n",
            ">53, d_loss=1.430,d_R_loss=0.593,d_f_loss=0.837, g_loss=0.611, d_acc=60, aadv_acc=56\n",
            ">54, d_loss=1.358,d_R_loss=0.512,d_f_loss=0.846, g_loss=0.587, d_acc=60, aadv_acc=56\n",
            ">55, d_loss=1.480,d_R_loss=0.595,d_f_loss=0.885, g_loss=0.556, d_acc=60, aadv_acc=56\n",
            ">56, d_loss=1.417,d_R_loss=0.628,d_f_loss=0.788, g_loss=0.617, d_acc=60, aadv_acc=57\n",
            ">57, d_loss=1.331,d_R_loss=0.578,d_f_loss=0.753, g_loss=0.661, d_acc=59, aadv_acc=57\n",
            ">58, d_loss=1.370,d_R_loss=0.577,d_f_loss=0.793, g_loss=0.616, d_acc=59, aadv_acc=57\n",
            ">59, d_loss=1.372,d_R_loss=0.638,d_f_loss=0.733, g_loss=0.668, d_acc=59, aadv_acc=58\n",
            ">60, d_loss=1.372,d_R_loss=0.611,d_f_loss=0.761, g_loss=0.638, d_acc=59, aadv_acc=58\n",
            ">61, d_loss=1.328,d_R_loss=0.605,d_f_loss=0.723, g_loss=0.669, d_acc=59, aadv_acc=58\n",
            ">62, d_loss=1.312,d_R_loss=0.574,d_f_loss=0.738, g_loss=0.658, d_acc=59, aadv_acc=58\n",
            ">63, d_loss=1.252,d_R_loss=0.503,d_f_loss=0.749, g_loss=0.650, d_acc=59, aadv_acc=58\n",
            ">64, d_loss=1.287,d_R_loss=0.536,d_f_loss=0.751, g_loss=0.654, d_acc=59, aadv_acc=58\n",
            ">65, d_loss=1.490,d_R_loss=0.627,d_f_loss=0.864, g_loss=0.570, d_acc=59, aadv_acc=58\n",
            ">66, d_loss=1.527,d_R_loss=0.661,d_f_loss=0.866, g_loss=0.555, d_acc=58, aadv_acc=59\n",
            ">67, d_loss=1.354,d_R_loss=0.629,d_f_loss=0.725, g_loss=0.673, d_acc=58, aadv_acc=59\n",
            ">68, d_loss=1.137,d_R_loss=0.480,d_f_loss=0.657, g_loss=0.750, d_acc=58, aadv_acc=59\n",
            ">69, d_loss=1.398,d_R_loss=0.522,d_f_loss=0.875, g_loss=0.552, d_acc=58, aadv_acc=59\n",
            ">70, d_loss=1.429,d_R_loss=0.593,d_f_loss=0.836, g_loss=0.583, d_acc=58, aadv_acc=60\n",
            ">71, d_loss=1.479,d_R_loss=0.665,d_f_loss=0.814, g_loss=0.601, d_acc=58, aadv_acc=60\n",
            ">72, d_loss=1.385,d_R_loss=0.573,d_f_loss=0.812, g_loss=0.614, d_acc=58, aadv_acc=60\n",
            ">73, d_loss=1.407,d_R_loss=0.566,d_f_loss=0.840, g_loss=0.585, d_acc=58, aadv_acc=60\n",
            ">74, d_loss=1.349,d_R_loss=0.637,d_f_loss=0.712, g_loss=0.684, d_acc=57, aadv_acc=61\n",
            ">75, d_loss=1.281,d_R_loss=0.598,d_f_loss=0.684, g_loss=0.713, d_acc=57, aadv_acc=61\n",
            ">76, d_loss=1.348,d_R_loss=0.590,d_f_loss=0.758, g_loss=0.635, d_acc=57, aadv_acc=61\n",
            ">77, d_loss=1.354,d_R_loss=0.583,d_f_loss=0.771, g_loss=0.624, d_acc=57, aadv_acc=61\n",
            ">78, d_loss=1.379,d_R_loss=0.626,d_f_loss=0.753, g_loss=0.648, d_acc=57, aadv_acc=61\n",
            ">79, d_loss=1.429,d_R_loss=0.606,d_f_loss=0.823, g_loss=0.596, d_acc=57, aadv_acc=61\n",
            ">80, d_loss=1.312,d_R_loss=0.583,d_f_loss=0.729, g_loss=0.666, d_acc=57, aadv_acc=62\n",
            ">81, d_loss=1.191,d_R_loss=0.479,d_f_loss=0.712, g_loss=0.696, d_acc=57, aadv_acc=61\n",
            ">82, d_loss=1.402,d_R_loss=0.519,d_f_loss=0.883, g_loss=0.560, d_acc=57, aadv_acc=61\n",
            ">83, d_loss=1.424,d_R_loss=0.648,d_f_loss=0.776, g_loss=0.631, d_acc=57, aadv_acc=62\n",
            ">84, d_loss=1.392,d_R_loss=0.664,d_f_loss=0.729, g_loss=0.670, d_acc=57, aadv_acc=62\n",
            ">85, d_loss=1.366,d_R_loss=0.641,d_f_loss=0.725, g_loss=0.680, d_acc=57, aadv_acc=62\n",
            ">86, d_loss=1.440,d_R_loss=0.652,d_f_loss=0.788, g_loss=0.624, d_acc=57, aadv_acc=62\n",
            ">87, d_loss=1.369,d_R_loss=0.608,d_f_loss=0.761, g_loss=0.669, d_acc=57, aadv_acc=62\n",
            ">88, d_loss=1.364,d_R_loss=0.572,d_f_loss=0.792, g_loss=0.632, d_acc=57, aadv_acc=62\n",
            ">89, d_loss=1.381,d_R_loss=0.630,d_f_loss=0.751, g_loss=0.647, d_acc=57, aadv_acc=62\n",
            ">90, d_loss=1.323,d_R_loss=0.647,d_f_loss=0.677, g_loss=0.717, d_acc=57, aadv_acc=62\n",
            ">91, d_loss=1.353,d_R_loss=0.530,d_f_loss=0.823, g_loss=0.594, d_acc=57, aadv_acc=62\n",
            ">92, d_loss=1.376,d_R_loss=0.527,d_f_loss=0.849, g_loss=0.578, d_acc=57, aadv_acc=62\n",
            ">93, d_loss=1.486,d_R_loss=0.592,d_f_loss=0.895, g_loss=0.551, d_acc=56, aadv_acc=62\n",
            ">94, d_loss=1.356,d_R_loss=0.545,d_f_loss=0.812, g_loss=0.605, d_acc=57, aadv_acc=62\n",
            ">95, d_loss=1.304,d_R_loss=0.498,d_f_loss=0.805, g_loss=0.602, d_acc=57, aadv_acc=62\n",
            ">96, d_loss=1.379,d_R_loss=0.527,d_f_loss=0.852, g_loss=0.569, d_acc=56, aadv_acc=62\n",
            ">97, d_loss=1.437,d_R_loss=0.659,d_f_loss=0.779, g_loss=0.643, d_acc=56, aadv_acc=63\n",
            ">98, d_loss=1.449,d_R_loss=0.593,d_f_loss=0.856, g_loss=0.576, d_acc=56, aadv_acc=63\n",
            ">99, d_loss=1.571,d_R_loss=0.658,d_f_loss=0.913, g_loss=0.522, d_acc=56, aadv_acc=63\n",
            ">100, d_loss=1.358,d_R_loss=0.609,d_f_loss=0.749, g_loss=0.645, d_acc=56, aadv_acc=63\n",
            ">101, d_loss=1.339,d_R_loss=0.625,d_f_loss=0.714, g_loss=0.679, d_acc=56, aadv_acc=63\n",
            ">102, d_loss=1.378,d_R_loss=0.557,d_f_loss=0.821, g_loss=0.583, d_acc=56, aadv_acc=63\n",
            ">103, d_loss=1.449,d_R_loss=0.667,d_f_loss=0.782, g_loss=0.616, d_acc=56, aadv_acc=64\n",
            ">104, d_loss=1.355,d_R_loss=0.591,d_f_loss=0.764, g_loss=0.633, d_acc=56, aadv_acc=64\n",
            ">105, d_loss=1.400,d_R_loss=0.623,d_f_loss=0.777, g_loss=0.623, d_acc=56, aadv_acc=64\n",
            ">106, d_loss=1.376,d_R_loss=0.587,d_f_loss=0.790, g_loss=0.620, d_acc=56, aadv_acc=64\n",
            ">107, d_loss=1.370,d_R_loss=0.529,d_f_loss=0.840, g_loss=0.573, d_acc=56, aadv_acc=64\n",
            ">108, d_loss=1.442,d_R_loss=0.579,d_f_loss=0.863, g_loss=0.558, d_acc=56, aadv_acc=64\n",
            ">109, d_loss=1.382,d_R_loss=0.580,d_f_loss=0.802, g_loss=0.603, d_acc=56, aadv_acc=65\n",
            ">110, d_loss=1.311,d_R_loss=0.579,d_f_loss=0.732, g_loss=0.661, d_acc=56, aadv_acc=65\n",
            ">111, d_loss=1.293,d_R_loss=0.511,d_f_loss=0.783, g_loss=0.622, d_acc=56, aadv_acc=65\n",
            ">112, d_loss=1.505,d_R_loss=0.642,d_f_loss=0.862, g_loss=0.557, d_acc=56, aadv_acc=65\n",
            ">113, d_loss=1.438,d_R_loss=0.645,d_f_loss=0.793, g_loss=0.609, d_acc=55, aadv_acc=65\n",
            ">114, d_loss=1.307,d_R_loss=0.644,d_f_loss=0.663, g_loss=0.728, d_acc=55, aadv_acc=65\n",
            ">115, d_loss=1.260,d_R_loss=0.579,d_f_loss=0.681, g_loss=0.713, d_acc=55, aadv_acc=65\n",
            ">116, d_loss=1.336,d_R_loss=0.558,d_f_loss=0.777, g_loss=0.634, d_acc=55, aadv_acc=65\n",
            ">117, d_loss=1.398,d_R_loss=0.598,d_f_loss=0.801, g_loss=0.602, d_acc=55, aadv_acc=65\n",
            ">118, d_loss=1.340,d_R_loss=0.603,d_f_loss=0.737, g_loss=0.658, d_acc=55, aadv_acc=65\n",
            ">119, d_loss=1.307,d_R_loss=0.559,d_f_loss=0.748, g_loss=0.657, d_acc=55, aadv_acc=65\n",
            ">120, d_loss=1.455,d_R_loss=0.582,d_f_loss=0.873, g_loss=0.554, d_acc=55, aadv_acc=65\n",
            ">121, d_loss=1.392,d_R_loss=0.570,d_f_loss=0.822, g_loss=0.598, d_acc=55, aadv_acc=65\n",
            ">122, d_loss=1.406,d_R_loss=0.611,d_f_loss=0.795, g_loss=0.623, d_acc=55, aadv_acc=65\n",
            ">123, d_loss=1.309,d_R_loss=0.575,d_f_loss=0.734, g_loss=0.690, d_acc=55, aadv_acc=65\n",
            ">124, d_loss=1.219,d_R_loss=0.583,d_f_loss=0.636, g_loss=0.775, d_acc=55, aadv_acc=65\n",
            ">125, d_loss=1.533,d_R_loss=0.649,d_f_loss=0.884, g_loss=0.552, d_acc=55, aadv_acc=65\n",
            ">126, d_loss=1.553,d_R_loss=0.710,d_f_loss=0.844, g_loss=0.584, d_acc=55, aadv_acc=65\n",
            ">127, d_loss=1.310,d_R_loss=0.624,d_f_loss=0.685, g_loss=0.717, d_acc=55, aadv_acc=65\n",
            ">128, d_loss=1.378,d_R_loss=0.600,d_f_loss=0.778, g_loss=0.632, d_acc=55, aadv_acc=65\n",
            ">129, d_loss=1.327,d_R_loss=0.599,d_f_loss=0.728, g_loss=0.666, d_acc=55, aadv_acc=65\n",
            ">130, d_loss=1.277,d_R_loss=0.560,d_f_loss=0.717, g_loss=0.711, d_acc=55, aadv_acc=65\n",
            ">131, d_loss=1.412,d_R_loss=0.614,d_f_loss=0.798, g_loss=0.619, d_acc=55, aadv_acc=65\n",
            ">132, d_loss=1.497,d_R_loss=0.649,d_f_loss=0.848, g_loss=0.567, d_acc=55, aadv_acc=65\n",
            ">133, d_loss=1.411,d_R_loss=0.605,d_f_loss=0.806, g_loss=0.605, d_acc=55, aadv_acc=66\n",
            ">134, d_loss=1.367,d_R_loss=0.564,d_f_loss=0.804, g_loss=0.606, d_acc=55, aadv_acc=66\n",
            ">135, d_loss=1.336,d_R_loss=0.536,d_f_loss=0.799, g_loss=0.601, d_acc=55, aadv_acc=66\n",
            ">136, d_loss=1.446,d_R_loss=0.608,d_f_loss=0.838, g_loss=0.580, d_acc=55, aadv_acc=66\n",
            ">137, d_loss=1.484,d_R_loss=0.625,d_f_loss=0.860, g_loss=0.561, d_acc=55, aadv_acc=66\n",
            ">138, d_loss=1.397,d_R_loss=0.619,d_f_loss=0.778, g_loss=0.621, d_acc=55, aadv_acc=66\n",
            ">139, d_loss=1.345,d_R_loss=0.595,d_f_loss=0.751, g_loss=0.642, d_acc=55, aadv_acc=66\n",
            ">140, d_loss=1.362,d_R_loss=0.593,d_f_loss=0.768, g_loss=0.627, d_acc=55, aadv_acc=67\n",
            ">141, d_loss=1.416,d_R_loss=0.644,d_f_loss=0.772, g_loss=0.631, d_acc=55, aadv_acc=67\n",
            ">142, d_loss=1.330,d_R_loss=0.623,d_f_loss=0.707, g_loss=0.705, d_acc=55, aadv_acc=66\n",
            ">143, d_loss=1.302,d_R_loss=0.580,d_f_loss=0.722, g_loss=0.687, d_acc=55, aadv_acc=66\n",
            ">144, d_loss=1.210,d_R_loss=0.538,d_f_loss=0.672, g_loss=0.726, d_acc=55, aadv_acc=66\n",
            ">145, d_loss=1.384,d_R_loss=0.547,d_f_loss=0.837, g_loss=0.587, d_acc=55, aadv_acc=66\n",
            ">146, d_loss=1.359,d_R_loss=0.592,d_f_loss=0.767, g_loss=0.631, d_acc=55, aadv_acc=66\n",
            ">147, d_loss=1.327,d_R_loss=0.577,d_f_loss=0.751, g_loss=0.666, d_acc=55, aadv_acc=66\n",
            ">148, d_loss=1.305,d_R_loss=0.552,d_f_loss=0.753, g_loss=0.641, d_acc=55, aadv_acc=66\n",
            ">149, d_loss=1.412,d_R_loss=0.615,d_f_loss=0.797, g_loss=0.619, d_acc=55, aadv_acc=66\n",
            ">150, d_loss=1.291,d_R_loss=0.592,d_f_loss=0.699, g_loss=0.694, d_acc=55, aadv_acc=66\n",
            ">151, d_loss=1.211,d_R_loss=0.501,d_f_loss=0.709, g_loss=0.685, d_acc=55, aadv_acc=66\n",
            ">152, d_loss=1.326,d_R_loss=0.507,d_f_loss=0.820, g_loss=0.627, d_acc=55, aadv_acc=66\n",
            ">153, d_loss=1.483,d_R_loss=0.581,d_f_loss=0.902, g_loss=0.540, d_acc=55, aadv_acc=66\n",
            ">154, d_loss=1.487,d_R_loss=0.629,d_f_loss=0.858, g_loss=0.568, d_acc=55, aadv_acc=67\n",
            ">155, d_loss=1.236,d_R_loss=0.549,d_f_loss=0.687, g_loss=0.709, d_acc=55, aadv_acc=67\n",
            ">156, d_loss=1.336,d_R_loss=0.588,d_f_loss=0.748, g_loss=0.652, d_acc=55, aadv_acc=67\n",
            ">157, d_loss=1.463,d_R_loss=0.599,d_f_loss=0.864, g_loss=0.556, d_acc=55, aadv_acc=67\n",
            ">158, d_loss=1.365,d_R_loss=0.612,d_f_loss=0.752, g_loss=0.645, d_acc=55, aadv_acc=67\n",
            ">159, d_loss=1.244,d_R_loss=0.552,d_f_loss=0.693, g_loss=0.706, d_acc=55, aadv_acc=67\n",
            ">160, d_loss=1.323,d_R_loss=0.514,d_f_loss=0.809, g_loss=0.614, d_acc=55, aadv_acc=67\n",
            ">161, d_loss=1.497,d_R_loss=0.540,d_f_loss=0.957, g_loss=0.501, d_acc=55, aadv_acc=67\n",
            ">162, d_loss=1.433,d_R_loss=0.560,d_f_loss=0.873, g_loss=0.544, d_acc=55, aadv_acc=67\n",
            ">163, d_loss=1.345,d_R_loss=0.550,d_f_loss=0.795, g_loss=0.605, d_acc=55, aadv_acc=67\n",
            ">164, d_loss=1.258,d_R_loss=0.536,d_f_loss=0.722, g_loss=0.675, d_acc=55, aadv_acc=67\n",
            ">165, d_loss=1.297,d_R_loss=0.507,d_f_loss=0.790, g_loss=0.617, d_acc=55, aadv_acc=67\n",
            ">166, d_loss=1.366,d_R_loss=0.556,d_f_loss=0.811, g_loss=0.597, d_acc=55, aadv_acc=67\n",
            ">167, d_loss=1.406,d_R_loss=0.609,d_f_loss=0.797, g_loss=0.606, d_acc=54, aadv_acc=67\n",
            ">168, d_loss=1.349,d_R_loss=0.608,d_f_loss=0.740, g_loss=0.654, d_acc=54, aadv_acc=67\n",
            ">169, d_loss=1.428,d_R_loss=0.652,d_f_loss=0.776, g_loss=0.622, d_acc=54, aadv_acc=67\n",
            ">170, d_loss=1.354,d_R_loss=0.626,d_f_loss=0.729, g_loss=0.670, d_acc=54, aadv_acc=68\n",
            ">171, d_loss=1.276,d_R_loss=0.559,d_f_loss=0.717, g_loss=0.679, d_acc=54, aadv_acc=68\n",
            ">172, d_loss=1.357,d_R_loss=0.649,d_f_loss=0.709, g_loss=0.690, d_acc=54, aadv_acc=67\n",
            ">173, d_loss=1.349,d_R_loss=0.579,d_f_loss=0.771, g_loss=0.658, d_acc=54, aadv_acc=67\n",
            ">174, d_loss=1.512,d_R_loss=0.618,d_f_loss=0.894, g_loss=0.550, d_acc=54, aadv_acc=67\n",
            ">175, d_loss=1.512,d_R_loss=0.622,d_f_loss=0.889, g_loss=0.548, d_acc=54, aadv_acc=67\n",
            ">176, d_loss=1.241,d_R_loss=0.516,d_f_loss=0.725, g_loss=0.733, d_acc=54, aadv_acc=67\n",
            ">177, d_loss=1.304,d_R_loss=0.525,d_f_loss=0.779, g_loss=0.648, d_acc=54, aadv_acc=68\n",
            ">178, d_loss=1.473,d_R_loss=0.620,d_f_loss=0.854, g_loss=0.569, d_acc=54, aadv_acc=68\n",
            ">179, d_loss=1.447,d_R_loss=0.642,d_f_loss=0.805, g_loss=0.603, d_acc=54, aadv_acc=68\n",
            ">180, d_loss=1.507,d_R_loss=0.698,d_f_loss=0.809, g_loss=0.599, d_acc=54, aadv_acc=68\n",
            ">181, d_loss=1.415,d_R_loss=0.625,d_f_loss=0.790, g_loss=0.610, d_acc=54, aadv_acc=68\n",
            ">182, d_loss=1.301,d_R_loss=0.583,d_f_loss=0.717, g_loss=0.676, d_acc=54, aadv_acc=68\n",
            ">183, d_loss=1.347,d_R_loss=0.571,d_f_loss=0.776, g_loss=0.628, d_acc=54, aadv_acc=68\n",
            ">184, d_loss=1.490,d_R_loss=0.657,d_f_loss=0.834, g_loss=0.573, d_acc=54, aadv_acc=68\n",
            ">185, d_loss=1.387,d_R_loss=0.624,d_f_loss=0.763, g_loss=0.631, d_acc=54, aadv_acc=68\n",
            ">186, d_loss=1.380,d_R_loss=0.641,d_f_loss=0.739, g_loss=0.654, d_acc=54, aadv_acc=68\n",
            ">187, d_loss=1.356,d_R_loss=0.642,d_f_loss=0.714, g_loss=0.675, d_acc=54, aadv_acc=68\n",
            ">188, d_loss=1.358,d_R_loss=0.620,d_f_loss=0.738, g_loss=0.656, d_acc=54, aadv_acc=68\n",
            ">189, d_loss=1.390,d_R_loss=0.621,d_f_loss=0.769, g_loss=0.632, d_acc=54, aadv_acc=68\n",
            ">190, d_loss=1.408,d_R_loss=0.581,d_f_loss=0.827, g_loss=0.589, d_acc=54, aadv_acc=68\n",
            ">191, d_loss=1.452,d_R_loss=0.587,d_f_loss=0.865, g_loss=0.555, d_acc=54, aadv_acc=69\n",
            ">192, d_loss=1.303,d_R_loss=0.552,d_f_loss=0.750, g_loss=0.647, d_acc=54, aadv_acc=69\n",
            ">193, d_loss=1.361,d_R_loss=0.569,d_f_loss=0.792, g_loss=0.613, d_acc=54, aadv_acc=69\n",
            ">194, d_loss=1.331,d_R_loss=0.561,d_f_loss=0.770, g_loss=0.629, d_acc=54, aadv_acc=69\n",
            ">195, d_loss=1.400,d_R_loss=0.590,d_f_loss=0.810, g_loss=0.594, d_acc=54, aadv_acc=69\n",
            ">196, d_loss=1.375,d_R_loss=0.633,d_f_loss=0.742, g_loss=0.652, d_acc=54, aadv_acc=69\n",
            ">197, d_loss=1.370,d_R_loss=0.592,d_f_loss=0.778, g_loss=0.627, d_acc=54, aadv_acc=69\n",
            ">198, d_loss=1.310,d_R_loss=0.535,d_f_loss=0.775, g_loss=0.627, d_acc=54, aadv_acc=69\n",
            ">199, d_loss=1.362,d_R_loss=0.512,d_f_loss=0.849, g_loss=0.569, d_acc=54, aadv_acc=69\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2ReCgpCkoGs"
      },
      "source": [
        "### Validate Performance Diabetes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7gWemwcwQGr"
      },
      "source": [
        "#### Evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1bzinAOd92w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b67da9-f322-4499-f77e-bd2a5dcbdafe"
      },
      "source": [
        "# evaluate real data accuracy\n",
        "prediction = tf.math.round(discriminator.predict(train_data_diabetes))\n",
        "real_acc = tf.keras.metrics.Accuracy()\n",
        "real_acc.update_state(np.ones(data_diabetes.shape[0]), prediction)\n",
        "result_real = real_acc.result()\n",
        "print(f\"Real data accuracy: {result_real.numpy()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Real data accuracy: 0.8463541865348816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyL3H16TWX04",
        "outputId": "ec42fe4a-dc83-4cad-f10b-8d5f26624a16"
      },
      "source": [
        "# evaluate fake data accuracy - 100 samples\n",
        "num_examples_to_generate = 100\n",
        "seed = tf.random.normal([num_examples_to_generate, latent_dim])\n",
        "prediction_gen = generator(seed, training=False)\n",
        "prediction_disc = discriminator.predict(prediction_gen)\n",
        "rounded_prediction = tf.math.round(prediction_disc)\n",
        "fake_acc = tf.keras.metrics.Accuracy()\n",
        "fake_acc.update_state(np.zeros((num_examples_to_generate,1)), rounded_prediction)\n",
        "result_fake = fake_acc.result()\n",
        "print(f\"Fake data accuracy: {result_fake.numpy()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fake data accuracy: 0.15000000596046448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTdPKrM2GAPT"
      },
      "source": [
        "# inverse to real values\n",
        "predictions_inverse = mms_diabetes.inverse_transform(prediction_gen)\n",
        "real_data = mms_diabetes.inverse_transform(data_diabetes_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f4AKogaEiWF",
        "outputId": "22bdf668-5e47-4380-9b24-4bf7e1a1237a"
      },
      "source": [
        "# samples which fooled the detector\n",
        "indices_fooled = [i for i in range(100) if rounded_prediction[i] == 1]\n",
        "chosen_samples = random.sample(indices_fooled, 5)\n",
        "for i in chosen_samples:\n",
        "  distances_scale = []\n",
        "  for item in data_diabetes_scaled:\n",
        "    distances_scale.append(np.linalg.norm(prediction_gen[i] - item))\n",
        "  closest_index = np.argmin(distances_scale)\n",
        "  print(f\"sample number {i}:\")\n",
        "  print(f\"Fake sample : {predictions_inverse[i].astype(int)}\")\n",
        "  print(f\"Closest real : {real_data[closest_index].astype(int)}\")\n",
        "  print(f\"Euclidean Distance: {distances_scale[closest_index]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample number 88:\n",
            "Fake sample : [10 93 86 15  4 33  0 53]\n",
            "Closest real : [ 8 99 84  0  0 35  0 50]\n",
            "Euclidean Distance: 0.4593472182750702\n",
            "sample number 85:\n",
            "Fake sample : [  0 140  90  37 231  39   0  21]\n",
            "Closest real : [  0 127  80  37 210  36   0  23]\n",
            "Euclidean Distance: 0.30372029542922974\n",
            "sample number 75:\n",
            "Fake sample : [ 7 89 42 20 22 27  0 30]\n",
            "Closest real : [  7 106  60  24   0  26   0  29]\n",
            "Euclidean Distance: 0.3633643388748169\n",
            "sample number 57:\n",
            "Fake sample : [  3 121  74  45 145  37   0  31]\n",
            "Closest real : [  2 104  80  45 191  33   0  29]\n",
            "Euclidean Distance: 0.3165197968482971\n",
            "sample number 66:\n",
            "Fake sample : [  1 118  79  50 209  39   0  26]\n",
            "Closest real : [  2 104  80  45 191  33   0  29]\n",
            "Euclidean Distance: 0.2642476260662079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NFsgqmMGykl",
        "outputId": "92f51d78-e614-42ed-aa26-9ed7e3e642c6"
      },
      "source": [
        "# samples which did not fool the detector\n",
        "indices_not_fooled = [i for i in range(100) if rounded_prediction[i] == 0]\n",
        "chosen_samples = random.sample(indices_not_fooled, 5)\n",
        "for i in chosen_samples:\n",
        "  distances_scale = []\n",
        "  for item in data_diabetes_scaled:\n",
        "    distances_scale.append(np.linalg.norm(prediction_gen[i] - item))\n",
        "  closest_index = np.argmin(distances_scale)\n",
        "  print(f\"sample number {i}:\")\n",
        "  print(f\"Fake sample : {predictions_inverse[i].astype(int)}\")\n",
        "  print(f\"Closest real : {real_data[closest_index].astype(int)}\")\n",
        "  print(f\"Euclidean Distance: {distances_scale[closest_index]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample number 87:\n",
            "Fake sample : [  3 191 114   5  48  36   0  31]\n",
            "Closest real : [  4 189 110  31   0  28   0  37]\n",
            "Euclidean Distance: 0.664002001285553\n",
            "sample number 60:\n",
            "Fake sample : [  3 193  99  30 143  32   0  32]\n",
            "Closest real : [  4 189 110  31   0  28   0  37]\n",
            "Euclidean Distance: 0.44953539967536926\n",
            "sample number 4:\n",
            "Fake sample : [  1 187  90  42 268  35   0  28]\n",
            "Closest real : [  4 184  78  39 277  37   0  30]\n",
            "Euclidean Distance: 0.4064830541610718\n",
            "sample number 16:\n",
            "Fake sample : [  3 183  84  24 350  29   0  31]\n",
            "Closest real : [  2 158  70  30 328  35   0  35]\n",
            "Euclidean Distance: 0.44706934690475464\n",
            "sample number 3:\n",
            "Fake sample : [12 96 50  1  0 22  0 41]\n",
            "Closest real : [13 76 60  0  0 32  0 41]\n",
            "Euclidean Distance: 0.4179590940475464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvTF_9GsIaUM",
        "outputId": "0eea3ae7-198c-4ebf-8891-030fcc3e7255"
      },
      "source": [
        "# fooled successfully samples - mean distance\n",
        "indices_fooled = [i for i in range(100) if rounded_prediction[i] == 1]\n",
        "closest_distances = []\n",
        "for i in indices_fooled:\n",
        "  distances_scale = []\n",
        "  for item in data_diabetes_scaled:\n",
        "    distances_scale.append(np.linalg.norm(prediction_gen[i] - item))\n",
        "  closest_distances.append(min(distances_scale))\n",
        "print(f\"The mean distance of samples which fooled the detector: {np.mean(closest_distances)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean distance of samples which fooled the detector: 0.3419110178947449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoEYVzOmJVpg",
        "outputId": "d5a37155-04f7-4a8b-d331-dcf9cf88507f"
      },
      "source": [
        "# Samples which did not fooled - mean distance\n",
        "indices_not_fooled = [i for i in range(100) if rounded_prediction[i] == 0]\n",
        "closest_distances = []\n",
        "for i in indices_not_fooled:\n",
        "  distances_scale = []\n",
        "  for item in data_diabetes_scaled:\n",
        "    distances_scale.append(np.linalg.norm(prediction_gen[i] - item))\n",
        "  closest_distances.append(min(distances_scale))\n",
        "print(f\"The mean distance of samples which did not fooled the detector: {np.mean(closest_distances)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean distance of samples which did not fooled the detector: 0.492653489112854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2U8YOBcW8EN"
      },
      "source": [
        "#### Dimensionality Reudction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lub92p7W-dQ"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(real_data)\n",
        "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
        "principalDf['target'] = 'real'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiqGTmRrdEdk"
      },
      "source": [
        "# all fake data\n",
        "a = pca.transform(predictions_inverse)\n",
        "b = pd.DataFrame(a, columns=['principal component 1', 'principal component 2'])\n",
        "b['target'] = 'Fooled'\n",
        "principalDf = principalDf.append(b, ignore_index=True)\n",
        "principalDf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSMxZz2K12ed"
      },
      "source": [
        "# fake data which did not fool the detector\n",
        "indices_not_fooled = [i for i in range(100) if rounded_prediction[i] == 0]\n",
        "index_0 = len(real_data)\n",
        "for i in indices_not_fooled:\n",
        "  principalDf[\"target\"].iloc[i+index_0] = 'NotFooled'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "EJ5Kkyf8a5_U",
        "outputId": "8a1b2aa1-0a9e-422e-a029-226632717ab3"
      },
      "source": [
        "# plot real and fake data\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = ['real','NotFooled', 'Fooled']\n",
        "colors = ['r', 'black', 'b']\n",
        "for target, color in zip(targets, colors):\n",
        "    if target ==\"NotFooled\":\n",
        "      s = 60\n",
        "    else:\n",
        "      s=50\n",
        "    indicesToKeep = principalDf[\"target\"] == target\n",
        "    ax.scatter(principalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , principalDf.loc[indicesToKeep, 'principal component 2']\n",
        "               , c = color\n",
        "               , s = s)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "vzKCqaRSi1n5",
        "outputId": "789d39ce-9760-4bde-c0a3-1a6f183f6dd4"
      },
      "source": [
        "# plot real data only\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = ['real']\n",
        "colors = ['r']\n",
        "for target, color in zip(targets, colors):\n",
        "    indicesToKeep = principalDf[\"target\"] == target\n",
        "    ax.scatter(principalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , principalDf.loc[indicesToKeep, 'principal component 2']\n",
        "               , c = color\n",
        "               , s = s)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoUIWOz2RcWD"
      },
      "source": [
        "## German Credit Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpHgVHGxxWrB"
      },
      "source": [
        "#### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoL4HUl9xYiC"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "latent_dim = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hCsEr8iR1OJ"
      },
      "source": [
        "### GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSmU-XmQR1OJ"
      },
      "source": [
        "https://arxiv.org/pdf/1904.09135.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IISNamFPR1OJ"
      },
      "source": [
        "#### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4rRLQt8R1OJ"
      },
      "source": [
        "# build discriminator model\n",
        "def build_disc(input_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(256, input_shape=(input_shape,)))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])  \n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onJWEui3R1OJ"
      },
      "source": [
        "#### Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6FB4lHQR1OK"
      },
      "source": [
        "# build generator model\n",
        "def build_gen(output_shape, noise_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(512, input_shape=(noise_shape,)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dense(256))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dense(output_shape, activation='tanh')) \n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdv9QRIHR8B2"
      },
      "source": [
        "### Run Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLYvcsxlRcWE"
      },
      "source": [
        "###### plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUW_YSkvRcWE"
      },
      "source": [
        "# create a line plot of loss for the gan and save to file\n",
        "def plot_history(d_hist, g_hist, d_acc_hist, adv_acc_hist):\n",
        "  # plot loss\n",
        "  pyplot.subplot(2, 1, 1)\n",
        "  pyplot.plot(d_hist, label='discriminator loss')\n",
        "  pyplot.plot(g_hist, label='generetor loss')\n",
        "  pyplot.legend()\n",
        "  pyplot.title(\"Discriminator and Generator Loss\")\n",
        "  pyplot.xlabel(\"steps\")\n",
        "  pyplot.ylabel(\"loss\")\n",
        "  # plot discriminator accuracy\n",
        "  pyplot.subplot(2, 1, 2)\n",
        "  pyplot.plot(d_acc_hist, label='disc_acc')\n",
        "  pyplot.plot(adv_acc_hist, label='adv_acc')\n",
        "  pyplot.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4uZCUndRcWE"
      },
      "source": [
        "##### losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shWIJvuX8nUC"
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8z2uc69RcWF"
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss, real_loss, fake_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asX4DsQ-RcWF"
      },
      "source": [
        "def discriminator_real_loss(real_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    return real_loss\n",
        "    \n",
        "def discriminator_fake_loss(fake_output):\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    return fake_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFlHR7Bl8l-f"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5W853PX8k86"
      },
      "source": [
        "disc_accuracy_tracker = tf.keras.metrics.Accuracy(name=\"disc_accuracy\")\n",
        "disc_accuracy_real_tracker = tf.keras.metrics.Accuracy(name=\"disc_accuracy_real\")\n",
        "disc_accuracy_fake_tracker = tf.keras.metrics.Accuracy(name=\"disc_accuracy_fake\")\n",
        "adv_accuracy_tracker = tf.keras.metrics.Accuracy(name=\"adv_accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4BoduqrRcWF"
      },
      "source": [
        "##### train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOjwwEbZRcWG"
      },
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(records):\n",
        "    num_sampels = len(records)\n",
        "\n",
        "    noise = tf.random.normal([num_sampels, latent_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape, tf.GradientTape() as disc_fake_tape, tf.GradientTape() as gen_tape_2:  \n",
        "      \n",
        "      generated_records = generator(noise, training=True) # generate samples from noise \n",
        "     \n",
        "      real_output = discriminator(records, training=True) # feed dsic with real data\n",
        "      fake_output = discriminator(generated_records, training=True)# feed disc with fake data\n",
        "      \n",
        "      # Generator train\n",
        "      gen_loss = generator_loss(fake_output)  # calc gen loss\n",
        "\n",
        "      #discriminator train\n",
        "      total_disc_loss, real_loss, fake_loss  = discriminator_loss(real_output, fake_output)\n",
        "      real_new_loss = discriminator_real_loss(real_output)\n",
        "      fake_new_loss = discriminator_fake_loss(fake_output)\n",
        "\n",
        "    # calculate grads\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator_real = disc_tape.gradient(real_new_loss, discriminator.trainable_variables)\n",
        "    gradients_of_discriminator_fake = disc_fake_tape.gradient(fake_new_loss, discriminator.trainable_variables)\n",
        "   \n",
        "    # apply grads\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator_real, discriminator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator_fake, discriminator.trainable_variables))\n",
        "    \n",
        "    # evaluate accuracy\n",
        "    rounded_real = tf.math.round(real_output)\n",
        "    rounded_fake = tf.math.round(fake_output)\n",
        "\n",
        "    disc_output = tf.concat([rounded_real, rounded_fake], axis=0)\n",
        "    disc_real = tf.concat([tf.ones_like(rounded_real), tf.zeros_like(rounded_fake)], axis =0)\n",
        "\n",
        "    disc_accuracy_tracker.update_state(disc_real, disc_output)\n",
        "    adv_accuracy_tracker.update_state(tf.ones_like(rounded_fake), rounded_fake)\n",
        "\n",
        "    disc_accuracy_real_tracker.update_state(tf.ones_like(rounded_real), rounded_real)\n",
        "    disc_accuracy_fake_tracker.update_state(tf.zeros_like(rounded_fake), rounded_fake)\n",
        "\n",
        "    return total_disc_loss, real_loss, fake_loss, disc_accuracy_real_tracker.result(), disc_accuracy_fake_tracker.result(), \\\n",
        "    gen_loss, disc_accuracy_tracker.result(), adv_accuracy_tracker.result()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbPyveq9RcWG"
      },
      "source": [
        "d_hist, d_r_hist, d_f_hist, g_hist, d_acc_hist, adv_acc_hist, real_acc_hist, fake_acc_hist= list(), list(), list(), list(), list(), list(), list(), list()\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    for image_batch in dataset:\n",
        "      d_loss, real_loss, fake_loss, real_acc, fake_acc, g_loss, d_accuracy, g_accuracy, =  train_step(image_batch)\n",
        "      \n",
        "      # record history\n",
        "      d_hist.append(d_loss.numpy())\n",
        "      d_r_hist.append(real_loss.numpy())\n",
        "      d_f_hist.append(fake_loss.numpy())\n",
        "      g_hist.append(g_loss.numpy())\n",
        "      d_acc_hist.append(d_accuracy.numpy())\n",
        "      adv_acc_hist.append(g_accuracy.numpy())\n",
        "      real_acc_hist.append(real_acc.numpy())\n",
        "      fake_acc_hist.append(fake_acc.numpy())\n",
        "\n",
        "    print('>%d, d_loss=%.3f, d_R_loss=%.3f, d_f_loss=%.3f, real_acc=%d, fake_acc=%d, g_loss=%.3f, d_acc=%d, aadv_acc=%d' %\n",
        "\t\t\t(epoch, d_hist[-1], d_r_hist[-1], d_f_hist[-1], int(100*real_acc_hist[-1]), int(100*fake_acc_hist[-1]),\\\n",
        "     g_hist[-1], int(100*d_acc_hist[-1]), int(100*adv_acc_hist[-1])))\n",
        "  \n",
        "  plot_history(d_r_hist, d_f_hist, d_acc_hist, adv_acc_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k2fQkvu7RcWH",
        "outputId": "5fb64fdb-c6ce-4998-a4b9-9417c7e6b1b1"
      },
      "source": [
        "discriminator = build_disc(50)\n",
        "generator = build_gen(50, latent_dim)\n",
        "train(train_data_german, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_48 (Dense)             (None, 256)               13056     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_32 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_33 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 145,153\n",
            "Trainable params: 145,153\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_51 (Dense)             (None, 512)               10752     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_34 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_35 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 50)                12850     \n",
            "=================================================================\n",
            "Total params: 158,002\n",
            "Trainable params: 156,466\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n",
            ">0, d_loss=0.641, d_R_loss=0.101, d_f_loss=0.540, real_acc=98, fake_acc=65, g_loss=0.925, d_acc=81, aadv_acc=34\n",
            ">1, d_loss=0.603, d_R_loss=0.193, d_f_loss=0.410, real_acc=99, fake_acc=76, g_loss=1.157, d_acc=87, aadv_acc=23\n",
            ">2, d_loss=1.124, d_R_loss=0.318, d_f_loss=0.806, real_acc=98, fake_acc=74, g_loss=0.660, d_acc=86, aadv_acc=25\n",
            ">3, d_loss=1.134, d_R_loss=0.243, d_f_loss=0.891, real_acc=95, fake_acc=66, g_loss=0.565, d_acc=81, aadv_acc=33\n",
            ">4, d_loss=1.657, d_R_loss=0.814, d_f_loss=0.843, real_acc=92, fake_acc=59, g_loss=0.616, d_acc=75, aadv_acc=40\n",
            ">5, d_loss=1.522, d_R_loss=0.545, d_f_loss=0.976, real_acc=88, fake_acc=51, g_loss=0.490, d_acc=69, aadv_acc=48\n",
            ">6, d_loss=1.552, d_R_loss=0.669, d_f_loss=0.883, real_acc=84, fake_acc=45, g_loss=0.544, d_acc=65, aadv_acc=54\n",
            ">7, d_loss=1.483, d_R_loss=0.659, d_f_loss=0.824, real_acc=82, fake_acc=42, g_loss=0.598, d_acc=62, aadv_acc=57\n",
            ">8, d_loss=1.162, d_R_loss=0.437, d_f_loss=0.725, real_acc=81, fake_acc=40, g_loss=0.668, d_acc=60, aadv_acc=59\n",
            ">9, d_loss=1.169, d_R_loss=0.452, d_f_loss=0.717, real_acc=80, fake_acc=39, g_loss=0.679, d_acc=60, aadv_acc=60\n",
            ">10, d_loss=1.428, d_R_loss=0.609, d_f_loss=0.819, real_acc=80, fake_acc=39, g_loss=0.625, d_acc=59, aadv_acc=60\n",
            ">11, d_loss=1.360, d_R_loss=0.499, d_f_loss=0.861, real_acc=80, fake_acc=38, g_loss=0.579, d_acc=59, aadv_acc=61\n",
            ">12, d_loss=1.283, d_R_loss=0.465, d_f_loss=0.818, real_acc=80, fake_acc=38, g_loss=0.615, d_acc=59, aadv_acc=61\n",
            ">13, d_loss=1.142, d_R_loss=0.357, d_f_loss=0.785, real_acc=79, fake_acc=39, g_loss=0.660, d_acc=59, aadv_acc=60\n",
            ">14, d_loss=1.300, d_R_loss=0.613, d_f_loss=0.687, real_acc=79, fake_acc=39, g_loss=0.758, d_acc=59, aadv_acc=60\n",
            ">15, d_loss=1.193, d_R_loss=0.446, d_f_loss=0.747, real_acc=79, fake_acc=39, g_loss=0.679, d_acc=59, aadv_acc=60\n",
            ">16, d_loss=1.262, d_R_loss=0.566, d_f_loss=0.697, real_acc=79, fake_acc=40, g_loss=0.714, d_acc=59, aadv_acc=59\n",
            ">17, d_loss=1.490, d_R_loss=0.543, d_f_loss=0.948, real_acc=78, fake_acc=40, g_loss=0.518, d_acc=59, aadv_acc=59\n",
            ">18, d_loss=1.364, d_R_loss=0.591, d_f_loss=0.773, real_acc=78, fake_acc=40, g_loss=0.699, d_acc=59, aadv_acc=59\n",
            ">19, d_loss=1.301, d_R_loss=0.527, d_f_loss=0.774, real_acc=78, fake_acc=40, g_loss=0.664, d_acc=59, aadv_acc=59\n",
            ">20, d_loss=1.174, d_R_loss=0.575, d_f_loss=0.599, real_acc=77, fake_acc=41, g_loss=0.829, d_acc=59, aadv_acc=58\n",
            ">21, d_loss=1.257, d_R_loss=0.539, d_f_loss=0.718, real_acc=77, fake_acc=41, g_loss=0.702, d_acc=59, aadv_acc=58\n",
            ">22, d_loss=1.431, d_R_loss=0.596, d_f_loss=0.835, real_acc=77, fake_acc=41, g_loss=0.605, d_acc=59, aadv_acc=58\n",
            ">23, d_loss=1.263, d_R_loss=0.464, d_f_loss=0.799, real_acc=77, fake_acc=41, g_loss=0.676, d_acc=59, aadv_acc=58\n",
            ">24, d_loss=1.228, d_R_loss=0.600, d_f_loss=0.627, real_acc=77, fake_acc=42, g_loss=0.809, d_acc=59, aadv_acc=57\n",
            ">25, d_loss=1.387, d_R_loss=0.714, d_f_loss=0.673, real_acc=76, fake_acc=42, g_loss=0.774, d_acc=59, aadv_acc=57\n",
            ">26, d_loss=1.224, d_R_loss=0.547, d_f_loss=0.677, real_acc=76, fake_acc=43, g_loss=0.791, d_acc=59, aadv_acc=56\n",
            ">27, d_loss=1.449, d_R_loss=0.782, d_f_loss=0.667, real_acc=76, fake_acc=43, g_loss=0.728, d_acc=59, aadv_acc=56\n",
            ">28, d_loss=1.282, d_R_loss=0.604, d_f_loss=0.677, real_acc=76, fake_acc=43, g_loss=0.758, d_acc=59, aadv_acc=56\n",
            ">29, d_loss=1.238, d_R_loss=0.489, d_f_loss=0.749, real_acc=76, fake_acc=43, g_loss=0.680, d_acc=59, aadv_acc=56\n",
            ">30, d_loss=1.231, d_R_loss=0.560, d_f_loss=0.671, real_acc=76, fake_acc=44, g_loss=0.747, d_acc=60, aadv_acc=55\n",
            ">31, d_loss=1.179, d_R_loss=0.605, d_f_loss=0.573, real_acc=76, fake_acc=44, g_loss=0.865, d_acc=60, aadv_acc=55\n",
            ">32, d_loss=1.390, d_R_loss=0.592, d_f_loss=0.798, real_acc=75, fake_acc=45, g_loss=0.717, d_acc=60, aadv_acc=54\n",
            ">33, d_loss=1.595, d_R_loss=0.814, d_f_loss=0.781, real_acc=75, fake_acc=45, g_loss=0.637, d_acc=60, aadv_acc=54\n",
            ">34, d_loss=1.175, d_R_loss=0.528, d_f_loss=0.647, real_acc=75, fake_acc=45, g_loss=0.768, d_acc=60, aadv_acc=54\n",
            ">35, d_loss=1.268, d_R_loss=0.634, d_f_loss=0.634, real_acc=75, fake_acc=46, g_loss=0.777, d_acc=60, aadv_acc=54\n",
            ">36, d_loss=1.256, d_R_loss=0.597, d_f_loss=0.659, real_acc=75, fake_acc=46, g_loss=0.782, d_acc=60, aadv_acc=53\n",
            ">37, d_loss=1.315, d_R_loss=0.720, d_f_loss=0.595, real_acc=75, fake_acc=46, g_loss=0.838, d_acc=60, aadv_acc=53\n",
            ">38, d_loss=1.171, d_R_loss=0.484, d_f_loss=0.687, real_acc=75, fake_acc=46, g_loss=0.780, d_acc=60, aadv_acc=53\n",
            ">39, d_loss=1.213, d_R_loss=0.563, d_f_loss=0.650, real_acc=74, fake_acc=47, g_loss=0.800, d_acc=61, aadv_acc=52\n",
            ">40, d_loss=1.379, d_R_loss=0.744, d_f_loss=0.634, real_acc=74, fake_acc=47, g_loss=0.773, d_acc=61, aadv_acc=52\n",
            ">41, d_loss=1.291, d_R_loss=0.657, d_f_loss=0.634, real_acc=74, fake_acc=47, g_loss=0.857, d_acc=61, aadv_acc=52\n",
            ">42, d_loss=1.217, d_R_loss=0.582, d_f_loss=0.634, real_acc=74, fake_acc=47, g_loss=0.844, d_acc=61, aadv_acc=52\n",
            ">43, d_loss=1.410, d_R_loss=0.755, d_f_loss=0.655, real_acc=74, fake_acc=47, g_loss=0.767, d_acc=61, aadv_acc=52\n",
            ">44, d_loss=1.466, d_R_loss=0.662, d_f_loss=0.803, real_acc=74, fake_acc=48, g_loss=0.604, d_acc=61, aadv_acc=51\n",
            ">45, d_loss=1.306, d_R_loss=0.683, d_f_loss=0.624, real_acc=74, fake_acc=48, g_loss=0.790, d_acc=61, aadv_acc=51\n",
            ">46, d_loss=1.344, d_R_loss=0.652, d_f_loss=0.692, real_acc=74, fake_acc=48, g_loss=0.802, d_acc=61, aadv_acc=51\n",
            ">47, d_loss=1.219, d_R_loss=0.489, d_f_loss=0.730, real_acc=74, fake_acc=48, g_loss=0.716, d_acc=61, aadv_acc=51\n",
            ">48, d_loss=1.310, d_R_loss=0.564, d_f_loss=0.746, real_acc=74, fake_acc=49, g_loss=0.685, d_acc=61, aadv_acc=50\n",
            ">49, d_loss=1.585, d_R_loss=0.849, d_f_loss=0.736, real_acc=74, fake_acc=49, g_loss=0.701, d_acc=61, aadv_acc=50\n",
            ">50, d_loss=1.395, d_R_loss=0.708, d_f_loss=0.687, real_acc=73, fake_acc=49, g_loss=0.725, d_acc=61, aadv_acc=50\n",
            ">51, d_loss=1.117, d_R_loss=0.507, d_f_loss=0.610, real_acc=73, fake_acc=49, g_loss=0.844, d_acc=61, aadv_acc=50\n",
            ">52, d_loss=1.269, d_R_loss=0.582, d_f_loss=0.687, real_acc=73, fake_acc=49, g_loss=0.779, d_acc=61, aadv_acc=50\n",
            ">53, d_loss=0.958, d_R_loss=0.347, d_f_loss=0.611, real_acc=73, fake_acc=49, g_loss=0.814, d_acc=61, aadv_acc=50\n",
            ">54, d_loss=1.340, d_R_loss=0.744, d_f_loss=0.595, real_acc=73, fake_acc=50, g_loss=0.829, d_acc=61, aadv_acc=49\n",
            ">55, d_loss=1.360, d_R_loss=0.685, d_f_loss=0.675, real_acc=73, fake_acc=50, g_loss=0.764, d_acc=61, aadv_acc=49\n",
            ">56, d_loss=1.142, d_R_loss=0.465, d_f_loss=0.677, real_acc=73, fake_acc=50, g_loss=0.762, d_acc=61, aadv_acc=49\n",
            ">57, d_loss=1.348, d_R_loss=0.683, d_f_loss=0.665, real_acc=73, fake_acc=50, g_loss=0.742, d_acc=61, aadv_acc=49\n",
            ">58, d_loss=1.075, d_R_loss=0.531, d_f_loss=0.545, real_acc=73, fake_acc=50, g_loss=0.955, d_acc=61, aadv_acc=49\n",
            ">59, d_loss=1.258, d_R_loss=0.535, d_f_loss=0.723, real_acc=73, fake_acc=50, g_loss=0.716, d_acc=62, aadv_acc=49\n",
            ">60, d_loss=1.349, d_R_loss=0.593, d_f_loss=0.756, real_acc=73, fake_acc=50, g_loss=0.704, d_acc=62, aadv_acc=49\n",
            ">61, d_loss=1.383, d_R_loss=0.624, d_f_loss=0.759, real_acc=73, fake_acc=50, g_loss=0.720, d_acc=62, aadv_acc=49\n",
            ">62, d_loss=1.385, d_R_loss=0.701, d_f_loss=0.685, real_acc=73, fake_acc=50, g_loss=0.770, d_acc=61, aadv_acc=49\n",
            ">63, d_loss=1.173, d_R_loss=0.649, d_f_loss=0.524, real_acc=73, fake_acc=51, g_loss=0.958, d_acc=62, aadv_acc=48\n",
            ">64, d_loss=1.192, d_R_loss=0.587, d_f_loss=0.604, real_acc=73, fake_acc=51, g_loss=0.848, d_acc=62, aadv_acc=48\n",
            ">65, d_loss=1.183, d_R_loss=0.507, d_f_loss=0.676, real_acc=73, fake_acc=51, g_loss=0.757, d_acc=62, aadv_acc=48\n",
            ">66, d_loss=1.381, d_R_loss=0.653, d_f_loss=0.728, real_acc=73, fake_acc=51, g_loss=0.705, d_acc=62, aadv_acc=48\n",
            ">67, d_loss=1.132, d_R_loss=0.524, d_f_loss=0.607, real_acc=72, fake_acc=51, g_loss=0.859, d_acc=62, aadv_acc=48\n",
            ">68, d_loss=1.307, d_R_loss=0.529, d_f_loss=0.778, real_acc=72, fake_acc=51, g_loss=0.655, d_acc=62, aadv_acc=48\n",
            ">69, d_loss=1.211, d_R_loss=0.585, d_f_loss=0.626, real_acc=72, fake_acc=51, g_loss=0.777, d_acc=62, aadv_acc=48\n",
            ">70, d_loss=1.087, d_R_loss=0.473, d_f_loss=0.614, real_acc=72, fake_acc=52, g_loss=0.862, d_acc=62, aadv_acc=47\n",
            ">71, d_loss=1.182, d_R_loss=0.587, d_f_loss=0.596, real_acc=72, fake_acc=52, g_loss=0.882, d_acc=62, aadv_acc=47\n",
            ">72, d_loss=1.278, d_R_loss=0.510, d_f_loss=0.767, real_acc=72, fake_acc=52, g_loss=0.676, d_acc=62, aadv_acc=47\n",
            ">73, d_loss=1.314, d_R_loss=0.641, d_f_loss=0.674, real_acc=72, fake_acc=52, g_loss=0.775, d_acc=62, aadv_acc=47\n",
            ">74, d_loss=1.192, d_R_loss=0.498, d_f_loss=0.694, real_acc=72, fake_acc=52, g_loss=0.741, d_acc=62, aadv_acc=47\n",
            ">75, d_loss=1.078, d_R_loss=0.540, d_f_loss=0.538, real_acc=72, fake_acc=52, g_loss=0.912, d_acc=62, aadv_acc=47\n",
            ">76, d_loss=1.286, d_R_loss=0.536, d_f_loss=0.750, real_acc=72, fake_acc=52, g_loss=0.694, d_acc=62, aadv_acc=47\n",
            ">77, d_loss=1.327, d_R_loss=0.707, d_f_loss=0.620, real_acc=72, fake_acc=52, g_loss=0.788, d_acc=62, aadv_acc=47\n",
            ">78, d_loss=1.352, d_R_loss=0.489, d_f_loss=0.863, real_acc=72, fake_acc=52, g_loss=0.576, d_acc=62, aadv_acc=47\n",
            ">79, d_loss=1.155, d_R_loss=0.541, d_f_loss=0.614, real_acc=72, fake_acc=52, g_loss=0.824, d_acc=62, aadv_acc=47\n",
            ">80, d_loss=1.284, d_R_loss=0.638, d_f_loss=0.645, real_acc=72, fake_acc=52, g_loss=0.786, d_acc=62, aadv_acc=47\n",
            ">81, d_loss=1.040, d_R_loss=0.468, d_f_loss=0.573, real_acc=72, fake_acc=52, g_loss=0.869, d_acc=62, aadv_acc=47\n",
            ">82, d_loss=1.270, d_R_loss=0.601, d_f_loss=0.668, real_acc=72, fake_acc=53, g_loss=0.762, d_acc=62, aadv_acc=46\n",
            ">83, d_loss=1.424, d_R_loss=0.667, d_f_loss=0.757, real_acc=72, fake_acc=53, g_loss=0.712, d_acc=62, aadv_acc=46\n",
            ">84, d_loss=1.198, d_R_loss=0.579, d_f_loss=0.619, real_acc=72, fake_acc=53, g_loss=0.866, d_acc=62, aadv_acc=46\n",
            ">85, d_loss=1.130, d_R_loss=0.514, d_f_loss=0.616, real_acc=72, fake_acc=53, g_loss=0.818, d_acc=62, aadv_acc=46\n",
            ">86, d_loss=1.176, d_R_loss=0.510, d_f_loss=0.666, real_acc=72, fake_acc=53, g_loss=0.797, d_acc=62, aadv_acc=46\n",
            ">87, d_loss=1.415, d_R_loss=0.699, d_f_loss=0.716, real_acc=72, fake_acc=53, g_loss=0.712, d_acc=62, aadv_acc=46\n",
            ">88, d_loss=1.238, d_R_loss=0.637, d_f_loss=0.602, real_acc=72, fake_acc=53, g_loss=0.886, d_acc=62, aadv_acc=46\n",
            ">89, d_loss=1.368, d_R_loss=0.582, d_f_loss=0.786, real_acc=72, fake_acc=53, g_loss=0.672, d_acc=62, aadv_acc=46\n",
            ">90, d_loss=1.167, d_R_loss=0.583, d_f_loss=0.585, real_acc=72, fake_acc=53, g_loss=0.843, d_acc=62, aadv_acc=46\n",
            ">91, d_loss=1.562, d_R_loss=0.805, d_f_loss=0.757, real_acc=72, fake_acc=53, g_loss=0.659, d_acc=63, aadv_acc=46\n",
            ">92, d_loss=1.265, d_R_loss=0.691, d_f_loss=0.574, real_acc=72, fake_acc=53, g_loss=0.895, d_acc=63, aadv_acc=46\n",
            ">93, d_loss=1.518, d_R_loss=0.804, d_f_loss=0.714, real_acc=72, fake_acc=53, g_loss=0.755, d_acc=63, aadv_acc=46\n",
            ">94, d_loss=1.211, d_R_loss=0.673, d_f_loss=0.537, real_acc=72, fake_acc=54, g_loss=1.011, d_acc=63, aadv_acc=45\n",
            ">95, d_loss=1.258, d_R_loss=0.505, d_f_loss=0.753, real_acc=72, fake_acc=54, g_loss=0.659, d_acc=63, aadv_acc=45\n",
            ">96, d_loss=1.389, d_R_loss=0.727, d_f_loss=0.662, real_acc=72, fake_acc=54, g_loss=0.804, d_acc=63, aadv_acc=45\n",
            ">97, d_loss=1.164, d_R_loss=0.524, d_f_loss=0.640, real_acc=72, fake_acc=54, g_loss=0.826, d_acc=63, aadv_acc=45\n",
            ">98, d_loss=1.048, d_R_loss=0.523, d_f_loss=0.525, real_acc=72, fake_acc=54, g_loss=0.935, d_acc=63, aadv_acc=45\n",
            ">99, d_loss=1.176, d_R_loss=0.564, d_f_loss=0.611, real_acc=72, fake_acc=54, g_loss=0.828, d_acc=63, aadv_acc=45\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMP_WYDJSFai"
      },
      "source": [
        "### Validate Performance German Credit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yko1Dmppyivq"
      },
      "source": [
        "#### Evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sggq7RpSFai",
        "outputId": "36679545-296f-482a-c5ea-55462f4ab616"
      },
      "source": [
        "# evaluate real data accuracy\n",
        "prediction = tf.math.round(discriminator.predict(train_data_german))\n",
        "real_acc = tf.keras.metrics.Accuracy()\n",
        "real_acc.update_state(tf.ones_like(prediction), prediction)\n",
        "result_real = real_acc.result()\n",
        "print(f\"Real data accuracy: {result_real.numpy()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Real data accuracy: 0.7979999780654907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_IGXZetSFaj",
        "outputId": "ae7f53e6-bac5-43f5-fc4a-90613ba8ae1f"
      },
      "source": [
        "# evaluate fake data accuracy\n",
        "num_examples_to_generate = 100\n",
        "seed = tf.random.normal([num_examples_to_generate, latent_dim])\n",
        "prediction_gen = generator(seed, training=False)\n",
        "prediction_disc_1 = discriminator.predict(prediction_gen)\n",
        "prediction_disc = tf.math.round(prediction_disc_1)\n",
        "fake_acc = tf.keras.metrics.Accuracy()\n",
        "fake_acc.update_state(tf.zeros_like(prediction_disc), prediction_disc)\n",
        "result_fake = fake_acc.result()\n",
        "print(f\"Fake data accuracy: {result_fake.numpy()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fake data accuracy: 0.7099999785423279\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qyKa1msSFak"
      },
      "source": [
        "# inverse to real values\n",
        "predictions_inverse = mms_german.inverse_transform(prediction_gen)\n",
        "real_data = mms_german.inverse_transform(data_german_credit_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NETsKLzmy44T",
        "outputId": "2bfeb6b5-bcff-4033-d1eb-58792a5d4e7d"
      },
      "source": [
        "# samples which fooled the detector\n",
        "indices_fooled = [i for i in range(100) if prediction_disc[i] == 1]\n",
        "chosen_samples = random.sample(indices_fooled, 5)\n",
        "for i in chosen_samples:\n",
        "  distances_scale = []\n",
        "  for item in data_german_credit_encoded:\n",
        "    distances_scale.append(np.linalg.norm(prediction_gen[i] - item))\n",
        "  closest_index = np.argmin(distances_scale)\n",
        "  print(f\"sample number {i}:\")\n",
        "  print(f\"Fake sample : {predictions_inverse[i].astype(int)}\")\n",
        "  print(f\"Closest real : {real_data[closest_index].astype(int)}\")\n",
        "  print(f\"Euclidean Distance: {distances_scale[closest_index]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample number 12:\n",
            "Fake sample : [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   3   3  10 530   3   3  30   1   1]\n",
            "Closest real : [  0   0   1   0   0   0   0   0   0   1   0   0   0   0   0   0   0   1\n",
            "   0   1   0   0   1   0   0   0   0   0   1   0   1   0   0   1   0   0\n",
            "   1   0   1   0   1   2   2  11 766   4   3  66   1   1]\n",
            "Euclidean Distance: 3.033763885498047\n",
            "sample number 29:\n",
            "Fake sample : [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    2\n",
            "    0   16 2789    1    1   24    1    1]\n",
            "Closest real : [   0    0    1    0    0    0    0    0    0    1    0    0    0    0\n",
            "    0    0    0    0    1    1    0    0    1    0    0    0    0    0\n",
            "    1    0    1    0    0    0    1    0    0    1    1    0    1    4\n",
            "    2   11 1483    2    1   25    1    1]\n",
            "Euclidean Distance: 2.953584671020508\n",
            "sample number 64:\n",
            "Fake sample : [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    2    0\n",
            "    0   21 3301    1    1   31    1    1]\n",
            "Closest real : [   0    0    1    0    0    1    0    0    0    0    0    0    0    0\n",
            "    0    0    1    0    0    1    0    0    0    1    0    0    0    0\n",
            "    1    0    1    0    0    0    1    0    1    0    1    0    3    2\n",
            "    2   11 1386    2    2   26    1    1]\n",
            "Euclidean Distance: 4.189874172210693\n",
            "sample number 72:\n",
            "Fake sample : [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    1    9 2011    3    1   26    1    1]\n",
            "Closest real : [   0    0    1    0    0    0    0    0    1    0    0    0    0    0\n",
            "    0    0    0    1    0    1    0    0    0    0    1    0    0    0\n",
            "    1    0    1    0    1    0    0    0    1    0    1    0    0    0\n",
            "    0   18 2472    4    1   25    1    1]\n",
            "Euclidean Distance: 4.0378828048706055\n",
            "sample number 83:\n",
            "Fake sample : [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    2   20 2310    3    1   32    1    1]\n",
            "Closest real : [  0   0   1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   1\n",
            "   0   1   0   0   0   0   1   0   0   0   1   0   1   0   0   1   0   0\n",
            "   1   0   1   0   0   0   2   8 653   4   3  28   1   1]\n",
            "Euclidean Distance: 3.6353302001953125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YzFStVWzZb9",
        "outputId": "c0a683f8-01e4-46f2-ca78-dbf908a3ecef"
      },
      "source": [
        "# samples which did not fool the detector\n",
        "indices_not_fooled = [i for i in range(100) if prediction_disc[i] == 0]\n",
        "chosen_samples = random.sample(indices_not_fooled, 5)\n",
        "for i in chosen_samples:\n",
        "  distances_scale = []\n",
        "  for item in data_german_credit_encoded:\n",
        "    distances_scale.append(np.linalg.norm(prediction_gen[i] - item))\n",
        "  closest_index = np.argmin(distances_scale)\n",
        "  print(f\"sample number {i}:\")\n",
        "  print(f\"Fake sample : {predictions_inverse[i].astype(int)}\")\n",
        "  print(f\"Closest real : {real_data[closest_index].astype(int)}\")\n",
        "  print(f\"Euclidean Distance: {distances_scale[closest_index]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample number 17:\n",
            "Fake sample : [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    1    0\n",
            "    3   23 1808    2    3   39    3    1]\n",
            "Closest real : [   0    0    0    0    1    0    0    0    0    1    0    0    0    0\n",
            "    0    0    0    1    0    1    0    0    0    0    1    0    0    0\n",
            "    1    0    1    0    0    0    1    0    0    1    1    0    3    0\n",
            "    4   18 6070    3    4   33    2    1]\n",
            "Euclidean Distance: 3.698437213897705\n",
            "sample number 45:\n",
            "Fake sample : [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    2    3\n",
            "    3   10 1018    2    3   30    2    1]\n",
            "Closest real : [   0    0    0    0    1    0    0    0    0    1    0    0    0    0\n",
            "    0    0    0    1    0    1    0    0    0    1    0    0    0    0\n",
            "    1    0    1    0    0    0    1    0    1    0    1    0    3    4\n",
            "    3   30 6742    2    3   36    2    1]\n",
            "Euclidean Distance: 3.2288851737976074\n",
            "sample number 55:\n",
            "Fake sample : [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   2   0   3  20 920   3   3  32   2   1]\n",
            "Closest real : [   0    0    0    0    1    0    0    0    0    1    0    0    0    0\n",
            "    0    0    0    1    0    1    0    0    0    0    1    0    0    0\n",
            "    1    0    1    0    0    0    1    0    1    0    1    0    3    0\n",
            "    4   28 2743    4    2   28    2    1]\n",
            "Euclidean Distance: 2.87572979927063\n",
            "sample number 2:\n",
            "Fake sample : [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    1    2\n",
            "    3    7 1785    3    3   38    1    1]\n",
            "Closest real : [   0    0    1    0    0    0    0    0    1    0    0    0    0    0\n",
            "    0    0    0    1    0    1    0    0    0    1    0    0    0    0\n",
            "    1    0    1    0    0    1    0    0    1    0    1    0    1    0\n",
            "    2   11 1921    4    2   37    1    1]\n",
            "Euclidean Distance: 2.5849335193634033\n",
            "sample number 97:\n",
            "Fake sample : [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   3   6 704   3   3  28   1   1]\n",
            "Closest real : [   0    0    1    0    0    0    0    0    1    0    0    0    0    0\n",
            "    0    0    0    1    0    1    0    0    0    1    0    0    0    0\n",
            "    1    0    1    0    0    1    0    0    1    0    1    0    1    0\n",
            "    2   11 1921    4    2   37    1    1]\n",
            "Euclidean Distance: 3.365809917449951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2LamDd-zhZy",
        "outputId": "ca7b9c9e-b705-41a3-a157-53ca9ddef6d1"
      },
      "source": [
        "# fooled successfully samples - mean distance\n",
        "indices_fooled = [i for i in range(100) if prediction_disc[i] == 1]\n",
        "closest_distances = []\n",
        "for i in indices_fooled:\n",
        "  distances_scale = []\n",
        "  for item in data_german_credit_encoded:\n",
        "    distances_scale.append(np.linalg.norm(prediction_gen[i] - item))\n",
        "  closest_distances.append(min(distances_scale))\n",
        "print(f\"The mean distance of samples which fooled the detector: {np.mean(closest_distances)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean distance of samples which fooled the detector: 3.4326775074005127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LL9g4hQzjxK",
        "outputId": "ac22f224-1fa4-46b7-9804-aad77fbc6c62"
      },
      "source": [
        "# Samples which did not fooled - mean distance\n",
        "indices_not_fooled = [i for i in range(100) if prediction_disc[i] == 0]\n",
        "closest_distances = []\n",
        "for i in indices_not_fooled:\n",
        "  distances_scale = []\n",
        "  for item in data_german_credit_encoded:\n",
        "    distances_scale.append(np.linalg.norm(prediction_gen[i] - item))\n",
        "  closest_distances.append(min(distances_scale))\n",
        "print(f\"The mean distance of samples which did not fooled the detector: {np.mean(closest_distances)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean distance of samples which did not fooled the detector: 3.432727098464966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIiSmEIuSFam"
      },
      "source": [
        "#### Dimensionality Reudction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFb44z8SSFam"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(real_data)\n",
        "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
        "principalDf['target'] = 'real'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "fzjmibpmz_Fq",
        "outputId": "eb0ded27-5987-42ea-b737-d10830b88bf0"
      },
      "source": [
        "# all fake data\n",
        "a = pca.transform(predictions_inverse)\n",
        "b = pd.DataFrame(a, columns=['principal component 1', 'principal component 2'])\n",
        "b['target'] = 'Fooled'\n",
        "principalDf = principalDf.append(b, ignore_index=True)\n",
        "principalDf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>principal component 1</th>\n",
              "      <th>principal component 2</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-2102.286066</td>\n",
              "      <td>32.968887</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2679.802872</td>\n",
              "      <td>-17.318299</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1175.275817</td>\n",
              "      <td>14.446452</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4610.783135</td>\n",
              "      <td>7.120072</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1598.746901</td>\n",
              "      <td>17.170536</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>-2641.715750</td>\n",
              "      <td>-11.843569</td>\n",
              "      <td>Fooled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>-46.781646</td>\n",
              "      <td>-6.063050</td>\n",
              "      <td>Fooled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>-2026.094821</td>\n",
              "      <td>-4.721135</td>\n",
              "      <td>Fooled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>-2229.254998</td>\n",
              "      <td>-6.827738</td>\n",
              "      <td>Fooled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>2191.949884</td>\n",
              "      <td>-7.920166</td>\n",
              "      <td>Fooled</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      principal component 1  principal component 2  target\n",
              "0              -2102.286066              32.968887    real\n",
              "1               2679.802872             -17.318299    real\n",
              "2              -1175.275817              14.446452    real\n",
              "3               4610.783135               7.120072    real\n",
              "4               1598.746901              17.170536    real\n",
              "...                     ...                    ...     ...\n",
              "1995           -2641.715750             -11.843569  Fooled\n",
              "1996             -46.781646              -6.063050  Fooled\n",
              "1997           -2026.094821              -4.721135  Fooled\n",
              "1998           -2229.254998              -6.827738  Fooled\n",
              "1999            2191.949884              -7.920166  Fooled\n",
              "\n",
              "[2000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7WwQ8MT0GNJ"
      },
      "source": [
        "# fake data which did not fool the detector\n",
        "indices_not_fooled = [i for i in range(100) if prediction_disc[i] == 0]\n",
        "index_0 = len(real_data)\n",
        "for i in indices_not_fooled:\n",
        "  principalDf[\"target\"].iloc[i+index_0] = 'NotFooled'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "i5PKx--JSFan",
        "outputId": "6b4f6394-a5ec-4503-849e-238cdae2942f"
      },
      "source": [
        "# plot real and fake data\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = ['real','NotFooled', 'Fooled']\n",
        "colors = ['r', 'black', 'b']\n",
        "for target, color in zip(targets, colors):\n",
        "    if target ==\"NotFooled\":\n",
        "      s = 60\n",
        "    else:\n",
        "      s=50\n",
        "    indicesToKeep = principalDf[\"target\"] == target\n",
        "    ax.scatter(principalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , principalDf.loc[indicesToKeep, 'principal component 2']\n",
        "               , c = color\n",
        "               , s = s)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "ik2Fi0hjkCw2",
        "outputId": "2bf0f36e-e3df-4d5d-ec5f-8deac14c72e7"
      },
      "source": [
        "# plot real data only\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = ['real']\n",
        "colors = ['r']\n",
        "for target, color in zip(targets, colors):\n",
        "    indicesToKeep = principalDf[\"target\"] == target\n",
        "    ax.scatter(principalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , principalDf.loc[indicesToKeep, 'principal component 2']\n",
        "               , c = color\n",
        "               , s = s)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oajgk_4-VE8"
      },
      "source": [
        "#Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM4w2f7NrM79"
      },
      "source": [
        "##Global Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBJTarNPrPdW"
      },
      "source": [
        "def plot_graphs(a_list, b_list, a_name, b_name, title, x_axis_name, y_axis_name):\n",
        "  plt.plot(a_list, 'b')\n",
        "  plt.plot(b_list, 'r')\n",
        "  plt.legend([a_name, b_name])\n",
        "  plt.title(title)\n",
        "  plt.xlabel(x_axis_name)\n",
        "  plt.ylabel(y_axis_name)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xak94f2hsMLR"
      },
      "source": [
        "def encode_cat_features(df, cat_features_inds):\n",
        "  # encodes categorical features into a number between 0 and 1, depends on the number of labels in the feature.\n",
        "  # returns two dictionaries: one for the bins (the intervals of the values) and one for the labels (group_names) of the encoded value\n",
        "  bins_dict = {}\n",
        "  group_names_dict = {}\n",
        "  df = df.copy()\n",
        "  for cat_feature_ind in cat_features_inds:\n",
        "    cat_feature = df.iloc[:, cat_feature_ind]\n",
        "    unique_vals = cat_feature.unique()\n",
        "    num_unique_vals = len(unique_vals)\n",
        "    i=0.5\n",
        "    hops = 1 / num_unique_vals\n",
        "    bins_dict[cat_feature_ind] = [0]\n",
        "    group_names_dict[cat_feature_ind] = []\n",
        "    for val in unique_vals:\n",
        "      gen_val = round(i * hops, 3)\n",
        "      cat_feature[cat_feature == val] = gen_val\n",
        "      bins_dict[cat_feature_ind].append(round((i+0.5)*hops, 3))\n",
        "      group_names_dict[cat_feature_ind].append(val)\n",
        "      i+=1\n",
        "    df.iloc[:, cat_feature_ind] = cat_feature\n",
        "  \n",
        "  return df, bins_dict, group_names_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJkGoXSlvWVv"
      },
      "source": [
        "def decode_cat_features(df, bins_dict, group_names_dict):\n",
        "  df = df.copy()\n",
        "  for key, val in bins_dict.items():\n",
        "    df.iloc[:, key] = pd.cut(df.iloc[:, key], bins=bins_dict[key], labels=group_names_dict[key])\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oe6U4MXEj2K"
      },
      "source": [
        "##Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJtYpTuQA3_p",
        "outputId": "f6e52ed2-d530-41a3-aa10-c833efcaac35"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9LDyPz4w06v"
      },
      "source": [
        "###Diabetes Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "o3JaiGmBuMnl",
        "outputId": "0cc54f4e-2b46-4ebc-d010-f6ef8e0c6381"
      },
      "source": [
        "\n",
        "home_path = \"drive/MyDrive/Deep Learning Assignment 4\"\n",
        "\n",
        "diabetes_dataset = loadarff(home_path + \"/diabetes.arff\")\n",
        "diabetes_data = pd.DataFrame(diabetes_dataset[0])\n",
        "diabetes_data.loc[diabetes_data[\"class\"] == b'tested_negative', \"class\"] = 0\n",
        "diabetes_data.loc[diabetes_data[\"class\"] == b'tested_positive', \"class\"] = 1\n",
        "\n",
        "diabetes_class =diabetes_data[\"class\"]\n",
        "diabetes_data = diabetes_data.drop(columns=\"class\")\n",
        "\n",
        "display(diabetes_data)\n",
        "display(diabetes_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preg</th>\n",
              "      <th>plas</th>\n",
              "      <th>pres</th>\n",
              "      <th>skin</th>\n",
              "      <th>insu</th>\n",
              "      <th>mass</th>\n",
              "      <th>pedi</th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     preg   plas  pres  skin   insu  mass   pedi   age\n",
              "0     6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0\n",
              "1     1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0\n",
              "2     8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0\n",
              "3     1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0\n",
              "4     0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0\n",
              "..    ...    ...   ...   ...    ...   ...    ...   ...\n",
              "763  10.0  101.0  76.0  48.0  180.0  32.9  0.171  63.0\n",
              "764   2.0  122.0  70.0  27.0    0.0  36.8  0.340  27.0\n",
              "765   5.0  121.0  72.0  23.0  112.0  26.2  0.245  30.0\n",
              "766   1.0  126.0  60.0   0.0    0.0  30.1  0.349  47.0\n",
              "767   1.0   93.0  70.0  31.0    0.0  30.4  0.315  23.0\n",
              "\n",
              "[768 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      0\n",
              "2      1\n",
              "3      0\n",
              "4      1\n",
              "      ..\n",
              "763    0\n",
              "764    0\n",
              "765    0\n",
              "766    1\n",
              "767    0\n",
              "Name: class, Length: 768, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "VZuUsUCnw5vj",
        "outputId": "7bf883a4-b57a-4828-b24a-9e35745f10f4"
      },
      "source": [
        "# scale the data between 0 and 1\n",
        "\n",
        "diabetes_scaler = MinMaxScaler()\n",
        "scaled_diabetes_data = pd.DataFrame(diabetes_scaler.fit_transform(diabetes_data))\n",
        "display(scaled_diabetes_data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.743719</td>\n",
              "      <td>0.590164</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500745</td>\n",
              "      <td>0.234415</td>\n",
              "      <td>0.483333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.427136</td>\n",
              "      <td>0.540984</td>\n",
              "      <td>0.292929</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.396423</td>\n",
              "      <td>0.116567</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.919598</td>\n",
              "      <td>0.524590</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.347243</td>\n",
              "      <td>0.253629</td>\n",
              "      <td>0.183333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.447236</td>\n",
              "      <td>0.540984</td>\n",
              "      <td>0.232323</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.418778</td>\n",
              "      <td>0.038002</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.688442</td>\n",
              "      <td>0.327869</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.198582</td>\n",
              "      <td>0.642325</td>\n",
              "      <td>0.943638</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.507538</td>\n",
              "      <td>0.622951</td>\n",
              "      <td>0.484848</td>\n",
              "      <td>0.212766</td>\n",
              "      <td>0.490313</td>\n",
              "      <td>0.039710</td>\n",
              "      <td>0.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.613065</td>\n",
              "      <td>0.573770</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.548435</td>\n",
              "      <td>0.111870</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.608040</td>\n",
              "      <td>0.590164</td>\n",
              "      <td>0.232323</td>\n",
              "      <td>0.132388</td>\n",
              "      <td>0.390462</td>\n",
              "      <td>0.071307</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.633166</td>\n",
              "      <td>0.491803</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.448584</td>\n",
              "      <td>0.115713</td>\n",
              "      <td>0.433333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.467337</td>\n",
              "      <td>0.573770</td>\n",
              "      <td>0.313131</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.453055</td>\n",
              "      <td>0.101196</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2  ...         5         6         7\n",
              "0    0.352941  0.743719  0.590164  ...  0.500745  0.234415  0.483333\n",
              "1    0.058824  0.427136  0.540984  ...  0.396423  0.116567  0.166667\n",
              "2    0.470588  0.919598  0.524590  ...  0.347243  0.253629  0.183333\n",
              "3    0.058824  0.447236  0.540984  ...  0.418778  0.038002  0.000000\n",
              "4    0.000000  0.688442  0.327869  ...  0.642325  0.943638  0.200000\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "763  0.588235  0.507538  0.622951  ...  0.490313  0.039710  0.700000\n",
              "764  0.117647  0.613065  0.573770  ...  0.548435  0.111870  0.100000\n",
              "765  0.294118  0.608040  0.590164  ...  0.390462  0.071307  0.150000\n",
              "766  0.058824  0.633166  0.491803  ...  0.448584  0.115713  0.433333\n",
              "767  0.058824  0.467337  0.573770  ...  0.453055  0.101196  0.033333\n",
              "\n",
              "[768 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBsN6AJ8w24_"
      },
      "source": [
        "###German Credit Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbE8Wjv0gCk1",
        "outputId": "2446f6fd-a2b8-4933-ce50-9159f9260c84"
      },
      "source": [
        "home_path = \"drive/MyDrive/Deep Learning Assignment 4\"\n",
        "\n",
        "german_credit_dataset = loadarff(home_path + \"/german_credit.arff\")\n",
        "german_credit_class = pd.DataFrame(german_credit_dataset[0]).iloc[:,-1]\n",
        "german_credit_data = pd.DataFrame(german_credit_dataset[0]).iloc[:, :-1]\n",
        "# german_credit_data.loc[german_credit_data[\"class\"] == b'tested_negative', \"class\"] = 0\n",
        "# german_credit_data.loc[german_credit_data[\"class\"] == b'tested_positive', \"class\"] = 1\n",
        "\n",
        "german_credit_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>6.0</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>1169.0</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>67.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b'A12'</td>\n",
              "      <td>48.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>5951.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A92'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>22.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b'A14'</td>\n",
              "      <td>12.0</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A46'</td>\n",
              "      <td>2096.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A74'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>3.0</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>49.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A172'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>42.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A42'</td>\n",
              "      <td>7882.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A74'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A103'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A122'</td>\n",
              "      <td>45.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A153'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>24.0</td>\n",
              "      <td>b'A33'</td>\n",
              "      <td>b'A40'</td>\n",
              "      <td>4870.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>3.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A124'</td>\n",
              "      <td>53.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A153'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>2.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>b'A14'</td>\n",
              "      <td>12.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A42'</td>\n",
              "      <td>1736.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A74'</td>\n",
              "      <td>3.0</td>\n",
              "      <td>b'A92'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>31.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A172'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>30.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A41'</td>\n",
              "      <td>3857.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A91'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A122'</td>\n",
              "      <td>40.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A174'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>b'A14'</td>\n",
              "      <td>12.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>804.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A123'</td>\n",
              "      <td>38.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>45.0</td>\n",
              "      <td>b'A32'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>1845.0</td>\n",
              "      <td>b'A61'</td>\n",
              "      <td>b'A73'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A124'</td>\n",
              "      <td>23.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A153'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>b'A12'</td>\n",
              "      <td>45.0</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A41'</td>\n",
              "      <td>4576.0</td>\n",
              "      <td>b'A62'</td>\n",
              "      <td>b'A71'</td>\n",
              "      <td>3.0</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>4.0</td>\n",
              "      <td>b'A123'</td>\n",
              "      <td>27.0</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.0</td>\n",
              "      <td>b'A191'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          1     2       3       4       5  ...   16       17   18       19       20\n",
              "0    b'A11'   6.0  b'A34'  b'A43'  1169.0  ...  2.0  b'A173'  1.0  b'A192'  b'A201'\n",
              "1    b'A12'  48.0  b'A32'  b'A43'  5951.0  ...  1.0  b'A173'  1.0  b'A191'  b'A201'\n",
              "2    b'A14'  12.0  b'A34'  b'A46'  2096.0  ...  1.0  b'A172'  2.0  b'A191'  b'A201'\n",
              "3    b'A11'  42.0  b'A32'  b'A42'  7882.0  ...  1.0  b'A173'  2.0  b'A191'  b'A201'\n",
              "4    b'A11'  24.0  b'A33'  b'A40'  4870.0  ...  2.0  b'A173'  2.0  b'A191'  b'A201'\n",
              "..      ...   ...     ...     ...     ...  ...  ...      ...  ...      ...      ...\n",
              "995  b'A14'  12.0  b'A32'  b'A42'  1736.0  ...  1.0  b'A172'  1.0  b'A191'  b'A201'\n",
              "996  b'A11'  30.0  b'A32'  b'A41'  3857.0  ...  1.0  b'A174'  1.0  b'A192'  b'A201'\n",
              "997  b'A14'  12.0  b'A32'  b'A43'   804.0  ...  1.0  b'A173'  1.0  b'A191'  b'A201'\n",
              "998  b'A11'  45.0  b'A32'  b'A43'  1845.0  ...  1.0  b'A173'  1.0  b'A192'  b'A201'\n",
              "999  b'A12'  45.0  b'A34'  b'A41'  4576.0  ...  1.0  b'A173'  1.0  b'A191'  b'A201'\n",
              "\n",
              "[1000 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI8MBmyZ8dUx",
        "outputId": "427c50c3-34a2-4372-e2de-1a460d8c54c4"
      },
      "source": [
        "cat_col_numbers=[0, 2, 3, 5, 6, 8, 9, 11, 13,14,16,18,19]\n",
        "german_data_encoded, bins_dict, group_names_dict = encode_cat_features(german_credit_data, cat_col_numbers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNFkKcjl_j-s",
        "outputId": "793b4cd7-3ca0-44a3-d57c-78d27effb8ba"
      },
      "source": [
        "german_data_encoded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.125</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1169.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.375</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>5951.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.167</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.625</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.15</td>\n",
              "      <td>2096.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.125</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.25</td>\n",
              "      <td>7882.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.125</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.35</td>\n",
              "      <td>4870.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>53.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>0.625</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1736.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0.125</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.45</td>\n",
              "      <td>3857.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0.625</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>804.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.875</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0.125</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1845.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0.375</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.45</td>\n",
              "      <td>4576.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.875</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1     2    3     4       5    6  ...     15   16     17   18    19    20\n",
              "0    0.125   6.0  0.1  0.05  1169.0  0.1  ...  0.167  2.0  0.125  1.0  0.25  0.25\n",
              "1    0.375  48.0  0.3  0.05  5951.0  0.3  ...  0.167  1.0  0.125  1.0  0.75  0.25\n",
              "2    0.625  12.0  0.1  0.15  2096.0  0.3  ...  0.167  1.0  0.375  2.0  0.75  0.25\n",
              "3    0.125  42.0  0.3  0.25  7882.0  0.3  ...    0.5  1.0  0.125  2.0  0.75  0.25\n",
              "4    0.125  24.0  0.5  0.35  4870.0  0.3  ...    0.5  2.0  0.125  2.0  0.75  0.25\n",
              "..     ...   ...  ...   ...     ...  ...  ...    ...  ...    ...  ...   ...   ...\n",
              "995  0.625  12.0  0.3  0.25  1736.0  0.3  ...  0.167  1.0  0.375  1.0  0.75  0.25\n",
              "996  0.125  30.0  0.3  0.45  3857.0  0.3  ...  0.167  1.0  0.625  1.0  0.25  0.25\n",
              "997  0.625  12.0  0.3  0.05   804.0  0.3  ...  0.167  1.0  0.125  1.0  0.75  0.25\n",
              "998  0.125  45.0  0.3  0.05  1845.0  0.3  ...    0.5  1.0  0.125  1.0  0.25  0.25\n",
              "999  0.375  45.0  0.1  0.45  4576.0  0.9  ...  0.167  1.0  0.125  1.0  0.75  0.25\n",
              "\n",
              "[1000 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SvaH5Gn4V61"
      },
      "source": [
        "scale_cols = [1, 4, 7, 10, 12, 15, 17]\n",
        "\n",
        "def scale_german_credit_data(german_data, scale_cols = scale_cols):\n",
        "  german_data_scaler = MinMaxScaler()\n",
        "  german_data = german_data.copy()\n",
        "  german_data.iloc[:, scale_cols] = german_data_scaler.fit_transform(german_data.iloc[:, scale_cols])\n",
        "\n",
        "  return german_data, german_data_scaler\n",
        "\n",
        "def descale_german_credit_data(german_data_scaled, german_data_scaler, scale_cols=scale_cols):\n",
        "  german_data_scaled = german_data_scaled.copy()\n",
        "  german_data_scaled.iloc[:, scale_cols] = german_data_scaler.inverse_transform(german_data_scaled.iloc[:, scale_cols])\n",
        "  return german_data_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwcSJGO_B_uS"
      },
      "source": [
        "german_data_scaled, german_data_scaler = scale_german_credit_data(german_data_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8la09FDyDVJE"
      },
      "source": [
        "german_data_descaled = descale_german_credit_data(german_data_scaled, german_data_scaler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3dOqF1DDc5w",
        "outputId": "5cdef967-be48-453d-e12b-b62de6af8380"
      },
      "source": [
        "german_data_descaled"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.125</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1169.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.375</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>5951.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.167</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.625</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.15</td>\n",
              "      <td>2096.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.125</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.25</td>\n",
              "      <td>7882.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.125</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.35</td>\n",
              "      <td>4870.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>53.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>0.625</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1736.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0.125</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.45</td>\n",
              "      <td>3857.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.375</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0.625</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>804.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.875</td>\n",
              "      <td>38.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0.125</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1845.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0.375</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.45</td>\n",
              "      <td>4576.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.875</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1     2    3     4       5    6  ...     15   16     17   18    19    20\n",
              "0    0.125   6.0  0.1  0.05  1169.0  0.1  ...  0.167  2.0  0.125  1.0  0.25  0.25\n",
              "1    0.375  48.0  0.3  0.05  5951.0  0.3  ...  0.167  1.0  0.125  1.0  0.75  0.25\n",
              "2    0.625  12.0  0.1  0.15  2096.0  0.3  ...  0.167  1.0  0.375  2.0  0.75  0.25\n",
              "3    0.125  42.0  0.3  0.25  7882.0  0.3  ...    0.5  1.0  0.125  2.0  0.75  0.25\n",
              "4    0.125  24.0  0.5  0.35  4870.0  0.3  ...    0.5  2.0  0.125  2.0  0.75  0.25\n",
              "..     ...   ...  ...   ...     ...  ...  ...    ...  ...    ...  ...   ...   ...\n",
              "995  0.625  12.0  0.3  0.25  1736.0  0.3  ...  0.167  1.0  0.375  1.0  0.75  0.25\n",
              "996  0.125  30.0  0.3  0.45  3857.0  0.3  ...  0.167  1.0  0.625  1.0  0.25  0.25\n",
              "997  0.625  12.0  0.3  0.05   804.0  0.3  ...  0.167  1.0  0.125  1.0  0.75  0.25\n",
              "998  0.125  45.0  0.3  0.05  1845.0  0.3  ...    0.5  1.0  0.125  1.0  0.25  0.25\n",
              "999  0.375  45.0  0.1  0.45  4576.0  0.9  ...  0.167  1.0  0.125  1.0  0.75  0.25\n",
              "\n",
              "[1000 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thTX8UfsCaXp",
        "outputId": "001a2749-9609-4cc5-d923-90e88fc347fa"
      },
      "source": [
        "german_data_scaled"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.029412</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.050567</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.375</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.313690</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.053571</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.625</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.101574</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.535714</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.419941</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.464286</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.254209</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.607143</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>0.625</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.081765</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.382353</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.198470</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0.625</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.030483</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.339286</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.602941</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.087763</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0.375</td>\n",
              "      <td>0.602941</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.238032</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1         2    3     4         5  ...        16     17   18    19    20\n",
              "0    0.125  0.029412  0.1  0.05  0.050567  ...  0.333333  0.125  0.0  0.25  0.25\n",
              "1    0.375  0.647059  0.3  0.05  0.313690  ...  0.000000  0.125  0.0  0.75  0.25\n",
              "2    0.625  0.117647  0.1  0.15  0.101574  ...  0.000000  0.375  1.0  0.75  0.25\n",
              "3    0.125  0.558824  0.3  0.25  0.419941  ...  0.000000  0.125  1.0  0.75  0.25\n",
              "4    0.125  0.294118  0.5  0.35  0.254209  ...  0.333333  0.125  1.0  0.75  0.25\n",
              "..     ...       ...  ...   ...       ...  ...       ...    ...  ...   ...   ...\n",
              "995  0.625  0.117647  0.3  0.25  0.081765  ...  0.000000  0.375  0.0  0.75  0.25\n",
              "996  0.125  0.382353  0.3  0.45  0.198470  ...  0.000000  0.625  0.0  0.25  0.25\n",
              "997  0.625  0.117647  0.3  0.05  0.030483  ...  0.000000  0.125  0.0  0.75  0.25\n",
              "998  0.125  0.602941  0.3  0.05  0.087763  ...  0.000000  0.125  0.0  0.25  0.25\n",
              "999  0.375  0.602941  0.1  0.45  0.238032  ...  0.000000  0.125  0.0  0.75  0.25\n",
              "\n",
              "[1000 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGqsD1vR69PO",
        "outputId": "1f736d59-d410-4d5d-ac76-7b4fcb3bdd80"
      },
      "source": [
        "german_data_class = german_credit_class\n",
        "german_data_class[german_data_class == b'1'] = 0\n",
        "german_data_class[german_data_class == b'2'] = 1\n",
        "german_data_class"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      1\n",
              "2      0\n",
              "3      0\n",
              "4      1\n",
              "      ..\n",
              "995    0\n",
              "996    0\n",
              "997    0\n",
              "998    1\n",
              "999    0\n",
              "Name: 21, Length: 1000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHk1YNpUdmRf",
        "outputId": "e4187d8c-ded2-4f7a-82ae-abb7f17ef219"
      },
      "source": [
        "german_data_class.sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8jFPq9AYAjK"
      },
      "source": [
        "##BB GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZAxVwEqMI3_"
      },
      "source": [
        "###BB GAN Components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwIMT8Z6MhpL"
      },
      "source": [
        "class BBGAN():\n",
        "  def __init__(self, data_dim, noise_dim):\n",
        "    self.data_dim = data_dim\n",
        "    self.D = None\n",
        "    self.G = None\n",
        "    self.AM = None\n",
        "    self.DM = None\n",
        "\n",
        "    self.noise_dim = noise_dim\n",
        "\n",
        "  def discriminator(self, dropout = 0.5):\n",
        "    # the discriminator here will be trained on a data dimension of data_dim + 2, where the y will be probability that the last vector component holds the true probability (from the BB model)\n",
        "    if self.D:\n",
        "      return self.D\n",
        "\n",
        "    sample_input = Input((self.data_dim,))\n",
        "    c_input = Input((1,))\n",
        "    y_input = Input((1,))\n",
        "\n",
        "    concat_layer = Concatenate()([sample_input, c_input, y_input])\n",
        "\n",
        "    layer_1 = Dense(256 , activation='relu')(concat_layer)\n",
        "    layer_1_dropout = Dropout(dropout)(layer_1)\n",
        "\n",
        "    layer_2 = Dense(256 , activation='relu')(layer_1_dropout)\n",
        "    layer_2_dropout = Dropout(dropout)(layer_2)\n",
        "\n",
        "    layer_3 = Dense(128 , activation='relu')(layer_2_dropout)\n",
        "    layer_3_dropout = Dropout(dropout)(layer_3)\n",
        "\n",
        "    output = Dense(1, activation = 'sigmoid')(layer_3_dropout)\n",
        "\n",
        "    self.D = Model(inputs = [sample_input, c_input, y_input], outputs = output)\n",
        "\n",
        "    print(\"Discriminator summary\")\n",
        "    self.D.summary()\n",
        "\n",
        "    return self.D\n",
        "\n",
        "\n",
        "  def generator(self, dropout=0.5):\n",
        "    # gets the y input but does nothing with it (just ouputs it)\n",
        "\n",
        "    if self.G:\n",
        "      return self.G\n",
        "\n",
        "    z_input = Input((self.noise_dim,))\n",
        "    c_input = Input((1,))\n",
        "\n",
        "    concat_layer = Concatenate()([z_input, c_input])\n",
        "\n",
        "    layer_1 = Dense(128 , activation='relu')(concat_layer)\n",
        "    layer_1_dropout = Dropout(dropout)(layer_1)\n",
        "\n",
        "    layer_2 = Dense(256 , activation='relu')(layer_1_dropout)\n",
        "    layer_2_dropout = Dropout(dropout)(layer_2)\n",
        "\n",
        "    layer_3 = Dense(256 , activation='relu')(layer_2_dropout)\n",
        "    layer_3_dropout = Dropout(dropout)(layer_3)\n",
        "\n",
        "    output = Dense(self.data_dim, activation = 'sigmoid', name=\"samples_generator_layer\")(layer_3_dropout)\n",
        "\n",
        "    self.G = Model(inputs=[z_input, c_input], outputs=output)\n",
        "\n",
        "    print(\"Generator summary\")\n",
        "    self.G.summary()\n",
        "\n",
        "    return self.G\n",
        "\n",
        "  def discriminator_model(self):\n",
        "    if self.DM:\n",
        "      return self.DM\n",
        "\n",
        "    optimizer = Adam()\n",
        "    self.DM = self.discriminator()\n",
        "    self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
        "                    metrics=['accuracy'])\n",
        "    return self.DM\n",
        "\n",
        "\n",
        "  def adversarial_model(self):\n",
        "    if self.AM:\n",
        "      return self.AM\n",
        "\n",
        "    optimizer = Adam()\n",
        "\n",
        "    # Adverserial Model\n",
        "    z_input = Input((self.noise_dim,))\n",
        "    c_input = Input((1,))\n",
        "    y_input = Input((1,))\n",
        "    \n",
        "    # Generator\n",
        "    generator = self.generator()([z_input, c_input])\n",
        "    discriminator = self.discriminator()([generator, c_input, y_input])\n",
        "\n",
        "    self.AM = Model(inputs = [z_input, c_input, y_input], outputs = discriminator)\n",
        "\n",
        "    print(\"Adversarial model summary:\")\n",
        "    self.AM.summary()\n",
        "\n",
        "    self.AM.compile(loss='binary_crossentropy', optimizer = optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "    return self.AM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezNAECdwP60H"
      },
      "source": [
        "class IMPLEMENTED_BBGAN(object):\n",
        "  def __init__(self, bbmodel, data_dim, noise_dim = 2):\n",
        "    self.data_dim = data_dim\n",
        "\n",
        "    self.noise_dim = noise_dim\n",
        "\n",
        "    self.BBGAN = BBGAN(data_dim=self.data_dim, noise_dim = self.noise_dim)\n",
        "    self.discriminator =  self.BBGAN.discriminator_model()\n",
        "    self.adversarial = self.BBGAN.adversarial_model()\n",
        "    self.generator = self.BBGAN.generator()\n",
        "\n",
        "    self.bbmodel = bbmodel\n",
        "\n",
        "\n",
        "  def train(self, train_steps=300, batch_size=256):\n",
        "\n",
        "    discriminator_accuracies = []\n",
        "    adversarial_accuracies = []\n",
        "\n",
        "    discriminator_losses = []\n",
        "    adversarial_losses = []\n",
        "\n",
        "    for i in range(train_steps):\n",
        "      z = np.random.uniform(-1.0, 1.0, size=[batch_size, self.noise_dim])\n",
        "      c = np.random.uniform(0.0, 1.0, size=[batch_size, 1]) #generated fake confidence\n",
        "\n",
        "      samples_fake = self.generator([z, c]).numpy()\n",
        "\n",
        "      y = self.bbmodel.predict_proba(samples_fake)[:,1]\n",
        "\n",
        "      y_hat = np.ones([batch_size, 1]) # 1 if y has the real value, 0 otherwise\n",
        "\n",
        "      for ind in range(y_hat.shape[0]):\n",
        "        rand = random.random()\n",
        "        # in a probability of 50% - shuffle the components\n",
        "        if rand >= 0.5:\n",
        "          temp = c[ind]\n",
        "          c[ind] = y[ind]\n",
        "          y[ind] = temp\n",
        "          y_hat[ind] = 0\n",
        "\n",
        "      d_loss = self.discriminator.train_on_batch([samples_fake, c, y], y_hat)\n",
        "\n",
        "      # we need to fool the discriminator by the samples that are generated by the generator, thus we will switch the labels on y_hat (that they will be the opposite)\n",
        "      for ind_fool in range(len(y_hat)):\n",
        "        if y_hat[ind_fool] == 0:\n",
        "          y_hat[ind_fool] = 1\n",
        "        else:\n",
        "          y_hat[ind_fool] = 0\n",
        "\n",
        "      # freeze the discriminator\n",
        "      self.discriminator.trainable = False\n",
        "      # train the generator on the fooling data\n",
        "      a_loss = self.adversarial.train_on_batch([z, c, y], y_hat)\n",
        "      # un-freeze the discriminator\n",
        "      self.discriminator.trainable = True\n",
        "\n",
        "      log_mesg = \"%d: [Discriminator loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
        "      log_mesg = \"%s  [Adversarial loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
        "      print(log_mesg)\n",
        "\n",
        "      discriminator_acc = d_loss[1]\n",
        "      adversarial_acc = a_loss[1]\n",
        "      discriminator_loss = d_loss[0]\n",
        "      adversarial_loss = a_loss[0]\n",
        "\n",
        "      discriminator_accuracies.append(discriminator_acc)\n",
        "      adversarial_accuracies.append(adversarial_acc)\n",
        "      discriminator_losses.append(discriminator_loss)\n",
        "      adversarial_losses.append(adversarial_loss)\n",
        "      \n",
        "    return discriminator_losses, discriminator_accuracies, adversarial_losses, adversarial_accuracies          \n",
        "\n",
        "  def generate_samples(self, num_samples = 100):\n",
        "    noise = np.random.uniform(-1.0, 1.0, size=[num_samples, self.BBGAN.noise_dim])\n",
        "    c = np.random.uniform(0.0, 1.0, size=[num_samples, 1]) #generated fake confidence\n",
        "    generated_samples = self.generator.predict([noise,c])\n",
        "    return generated_samples, c\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9xmDRn59fMC"
      },
      "source": [
        "##BB Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fok-gp11rlCC"
      },
      "source": [
        "###Diabetes Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBTaJXz3rwYq"
      },
      "source": [
        "####Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "D_ZvqOd99iTT",
        "outputId": "3ac8ac3c-4002-4aba-a398-77083831e84f"
      },
      "source": [
        "diabetes_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preg</th>\n",
              "      <th>plas</th>\n",
              "      <th>pres</th>\n",
              "      <th>skin</th>\n",
              "      <th>insu</th>\n",
              "      <th>mass</th>\n",
              "      <th>pedi</th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     preg   plas  pres  skin   insu  mass   pedi   age\n",
              "0     6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0\n",
              "1     1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0\n",
              "2     8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0\n",
              "3     1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0\n",
              "4     0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0\n",
              "..    ...    ...   ...   ...    ...   ...    ...   ...\n",
              "763  10.0  101.0  76.0  48.0  180.0  32.9  0.171  63.0\n",
              "764   2.0  122.0  70.0  27.0    0.0  36.8  0.340  27.0\n",
              "765   5.0  121.0  72.0  23.0  112.0  26.2  0.245  30.0\n",
              "766   1.0  126.0  60.0   0.0    0.0  30.1  0.349  47.0\n",
              "767   1.0   93.0  70.0  31.0    0.0  30.4  0.315  23.0\n",
              "\n",
              "[768 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcWh6Y9X9qOI"
      },
      "source": [
        "X = diabetes_data\n",
        "y = diabetes_class.astype('int')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHQxbeC5-QIq"
      },
      "source": [
        "####Diabetes Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdWIYYya-M01",
        "outputId": "7f8fb168-039b-4034-af37-00fb6cfa929d"
      },
      "source": [
        "diabetes_rf_clf = RandomForestClassifier()\n",
        "diabetes_rf_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVN-JAM3_dJy",
        "outputId": "c68ad1bb-1dad-4f10-f790-6d908c989cfd"
      },
      "source": [
        "diabetes_confidence = diabetes_rf_clf.predict_proba(X_test)[:, 1]\n",
        "print(diabetes_confidence)\n",
        "print(f\"max confidence score: {max(diabetes_confidence)}\")\n",
        "print(f\"min confidence score: {min(diabetes_confidence)}\")\n",
        "print(f\"avg confidence score: {np.mean(diabetes_confidence)}\")\n",
        "diabetes_pred = diabetes_rf_clf.predict(X_test)\n",
        "print(f\"diabetes rf classifier accuracy: {accuracy_score(y_test, diabetes_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.51 0.2  0.1  0.2  0.55 0.58 0.01 0.7  0.59 0.68 0.33 0.79 0.26 0.42\n",
            " 0.   0.41 0.06 0.03 0.66 0.4  0.49 0.11 0.32 0.02 0.51 0.87 0.03 0.02\n",
            " 0.23 0.21 0.77 0.8  0.79 0.82 0.44 0.81 0.78 0.46 0.18 0.74 0.06 0.32\n",
            " 0.66 0.48 0.05 0.71 0.52 0.1  0.14 0.83 0.   0.74 0.75 0.3  0.12 0.03\n",
            " 0.71 0.01 0.16 0.82 0.71 0.2  0.27 0.33 0.1  0.7  0.02 0.54 0.05 0.79\n",
            " 0.73 0.2  0.13 0.05 0.1  0.39 0.23 0.1  0.14 0.19 0.82 0.14 0.19 0.55\n",
            " 0.19 0.97 0.7  0.38 0.37 0.03 0.01 0.18 0.01 0.62 0.4  0.56 0.65 0.05\n",
            " 0.74 0.12 0.72 0.1  0.46 0.58 0.86 0.21 0.26 0.89 0.15 0.72 0.01 0.49\n",
            " 0.14 0.85 0.32 0.36 0.65 0.28 0.02 0.45 0.   0.21 0.38 0.1  0.25 0.5\n",
            " 0.23 0.83 0.8  0.64 0.67 0.8  0.03 0.44 0.74 0.47 0.32 0.65 0.83 0.\n",
            " 0.   0.01 0.25 0.56 0.1  0.47 0.3  0.01 0.41 0.8  0.13 0.3  0.44 0.29\n",
            " 0.02 0.5  0.26 0.45 0.57 0.09 0.41 0.65 0.18 0.   0.07 0.86 0.03 0.33\n",
            " 0.8  0.72 0.69 0.04 0.63 0.45 0.81 0.13 0.42 0.51 0.4  0.64 0.27 0.42\n",
            " 0.48 0.77 0.63 0.03 0.08 0.08 0.56 0.57 0.03 0.06 0.74 0.28 0.21 0.04\n",
            " 0.   0.05 0.12 0.8  0.3  0.05 0.36 0.19 0.81 0.1  0.05 0.29 0.79 0.36\n",
            " 0.49 0.16 0.07 0.12 0.67 0.04 0.67 0.7  0.39 0.88 0.58 0.28 0.02 0.14\n",
            " 0.07 0.84 0.27 0.21 0.36 0.33 0.2 ]\n",
            "max confidence score: 0.97\n",
            "min confidence score: 0.0\n",
            "avg confidence score: 0.3774458874458874\n",
            "diabetes rf classifier accuracy: 0.7445887445887446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "j0ailK52-yQF",
        "outputId": "270024dd-5925-4a87-dced-c512fe673b56"
      },
      "source": [
        "plt.hist(diabetes_confidence, rwidth=0.8)\n",
        "plt.xlabel(\"Confidence\")\n",
        "plt.ylabel(\"Number of cases\")\n",
        "plt.title(\"Confidence scores distribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj913KM-P2la"
      },
      "source": [
        "###Diabetes BBModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Siimu4-UkyyP",
        "outputId": "07352b82-78ea-446b-8c97-6eec44861f2c"
      },
      "source": [
        "diabetes_bbgan = IMPLEMENTED_BBGAN(diabetes_rf_clf, diabetes_data.shape[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator summary\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 10)           0           input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          2816        concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          65792       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            129         dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 101,633\n",
            "Trainable params: 101,633\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Generator summary\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 2)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 3)            0           input_7[0][0]                    \n",
            "                                                                 input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          512         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 128)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 256)          33024       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 256)          0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 256)          65792       dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 256)          0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "samples_generator_layer (Dense) (None, 8)            2056        dropout_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 101,384\n",
            "Trainable params: 101,384\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Adversarial model summary:\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 2)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Functional)            (None, 8)            101384      input_4[0][0]                    \n",
            "                                                                 input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model (Functional)              (None, 1)            101633      model_1[0][0]                    \n",
            "                                                                 input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 203,017\n",
            "Trainable params: 203,017\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apfv2QAxo7Sl",
        "outputId": "9e9578ce-7069-42fe-cc16-77c7389445b7"
      },
      "source": [
        "discriminator_losses, discriminator_accuracies, adversarial_losses, adversarial_accuracies = diabetes_bbgan.train(train_steps=1500, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: [Discriminator loss: 0.704969, acc: 0.480469]  [Adversarial loss: 0.700817, acc: 0.468750]\n",
            "1: [Discriminator loss: 0.696598, acc: 0.460938]  [Adversarial loss: 0.704232, acc: 0.425781]\n",
            "2: [Discriminator loss: 0.701345, acc: 0.468750]  [Adversarial loss: 0.703388, acc: 0.460938]\n",
            "3: [Discriminator loss: 0.702642, acc: 0.437500]  [Adversarial loss: 0.695224, acc: 0.496094]\n",
            "4: [Discriminator loss: 0.691842, acc: 0.550781]  [Adversarial loss: 0.700650, acc: 0.492188]\n",
            "5: [Discriminator loss: 0.697925, acc: 0.496094]  [Adversarial loss: 0.703253, acc: 0.468750]\n",
            "6: [Discriminator loss: 0.697633, acc: 0.476562]  [Adversarial loss: 0.703825, acc: 0.437500]\n",
            "7: [Discriminator loss: 0.695754, acc: 0.484375]  [Adversarial loss: 0.698877, acc: 0.496094]\n",
            "8: [Discriminator loss: 0.688302, acc: 0.515625]  [Adversarial loss: 0.698617, acc: 0.468750]\n",
            "9: [Discriminator loss: 0.690349, acc: 0.503906]  [Adversarial loss: 0.702647, acc: 0.453125]\n",
            "10: [Discriminator loss: 0.692195, acc: 0.472656]  [Adversarial loss: 0.698581, acc: 0.480469]\n",
            "11: [Discriminator loss: 0.690288, acc: 0.535156]  [Adversarial loss: 0.702242, acc: 0.433594]\n",
            "12: [Discriminator loss: 0.695709, acc: 0.488281]  [Adversarial loss: 0.702724, acc: 0.425781]\n",
            "13: [Discriminator loss: 0.692676, acc: 0.503906]  [Adversarial loss: 0.703390, acc: 0.414062]\n",
            "14: [Discriminator loss: 0.691387, acc: 0.500000]  [Adversarial loss: 0.701783, acc: 0.460938]\n",
            "15: [Discriminator loss: 0.692478, acc: 0.527344]  [Adversarial loss: 0.698357, acc: 0.460938]\n",
            "16: [Discriminator loss: 0.691680, acc: 0.500000]  [Adversarial loss: 0.696878, acc: 0.488281]\n",
            "17: [Discriminator loss: 0.688948, acc: 0.539062]  [Adversarial loss: 0.698366, acc: 0.460938]\n",
            "18: [Discriminator loss: 0.696280, acc: 0.476562]  [Adversarial loss: 0.693199, acc: 0.511719]\n",
            "19: [Discriminator loss: 0.685428, acc: 0.562500]  [Adversarial loss: 0.700574, acc: 0.457031]\n",
            "20: [Discriminator loss: 0.694455, acc: 0.496094]  [Adversarial loss: 0.700820, acc: 0.429688]\n",
            "21: [Discriminator loss: 0.694832, acc: 0.492188]  [Adversarial loss: 0.698699, acc: 0.480469]\n",
            "22: [Discriminator loss: 0.690437, acc: 0.523438]  [Adversarial loss: 0.699290, acc: 0.492188]\n",
            "23: [Discriminator loss: 0.694651, acc: 0.480469]  [Adversarial loss: 0.702389, acc: 0.441406]\n",
            "24: [Discriminator loss: 0.692607, acc: 0.496094]  [Adversarial loss: 0.698214, acc: 0.453125]\n",
            "25: [Discriminator loss: 0.693523, acc: 0.496094]  [Adversarial loss: 0.695106, acc: 0.531250]\n",
            "26: [Discriminator loss: 0.695792, acc: 0.507812]  [Adversarial loss: 0.698442, acc: 0.488281]\n",
            "27: [Discriminator loss: 0.689399, acc: 0.546875]  [Adversarial loss: 0.695315, acc: 0.496094]\n",
            "28: [Discriminator loss: 0.692744, acc: 0.492188]  [Adversarial loss: 0.693937, acc: 0.476562]\n",
            "29: [Discriminator loss: 0.693118, acc: 0.511719]  [Adversarial loss: 0.692982, acc: 0.484375]\n",
            "30: [Discriminator loss: 0.692386, acc: 0.554688]  [Adversarial loss: 0.697997, acc: 0.457031]\n",
            "31: [Discriminator loss: 0.695457, acc: 0.460938]  [Adversarial loss: 0.697230, acc: 0.441406]\n",
            "32: [Discriminator loss: 0.695580, acc: 0.488281]  [Adversarial loss: 0.689066, acc: 0.527344]\n",
            "33: [Discriminator loss: 0.695115, acc: 0.476562]  [Adversarial loss: 0.696529, acc: 0.441406]\n",
            "34: [Discriminator loss: 0.695595, acc: 0.480469]  [Adversarial loss: 0.688723, acc: 0.550781]\n",
            "35: [Discriminator loss: 0.689912, acc: 0.554688]  [Adversarial loss: 0.695062, acc: 0.468750]\n",
            "36: [Discriminator loss: 0.695674, acc: 0.488281]  [Adversarial loss: 0.687427, acc: 0.535156]\n",
            "37: [Discriminator loss: 0.691981, acc: 0.523438]  [Adversarial loss: 0.685775, acc: 0.566406]\n",
            "38: [Discriminator loss: 0.696173, acc: 0.445312]  [Adversarial loss: 0.686519, acc: 0.566406]\n",
            "39: [Discriminator loss: 0.693452, acc: 0.480469]  [Adversarial loss: 0.685624, acc: 0.558594]\n",
            "40: [Discriminator loss: 0.693429, acc: 0.511719]  [Adversarial loss: 0.683536, acc: 0.617188]\n",
            "41: [Discriminator loss: 0.692906, acc: 0.511719]  [Adversarial loss: 0.682021, acc: 0.613281]\n",
            "42: [Discriminator loss: 0.686691, acc: 0.562500]  [Adversarial loss: 0.673107, acc: 0.675781]\n",
            "43: [Discriminator loss: 0.688433, acc: 0.539062]  [Adversarial loss: 0.670671, acc: 0.679688]\n",
            "44: [Discriminator loss: 0.697272, acc: 0.480469]  [Adversarial loss: 0.668017, acc: 0.687500]\n",
            "45: [Discriminator loss: 0.689384, acc: 0.503906]  [Adversarial loss: 0.651455, acc: 0.730469]\n",
            "46: [Discriminator loss: 0.702228, acc: 0.519531]  [Adversarial loss: 0.647483, acc: 0.714844]\n",
            "47: [Discriminator loss: 0.706282, acc: 0.492188]  [Adversarial loss: 0.634974, acc: 0.812500]\n",
            "48: [Discriminator loss: 0.685919, acc: 0.562500]  [Adversarial loss: 0.630925, acc: 0.808594]\n",
            "49: [Discriminator loss: 0.689366, acc: 0.507812]  [Adversarial loss: 0.611901, acc: 0.843750]\n",
            "50: [Discriminator loss: 0.703550, acc: 0.457031]  [Adversarial loss: 0.597660, acc: 0.792969]\n",
            "51: [Discriminator loss: 0.700727, acc: 0.492188]  [Adversarial loss: 0.569184, acc: 0.835938]\n",
            "52: [Discriminator loss: 0.720756, acc: 0.472656]  [Adversarial loss: 0.559951, acc: 0.863281]\n",
            "53: [Discriminator loss: 0.749866, acc: 0.464844]  [Adversarial loss: 0.536386, acc: 0.882812]\n",
            "54: [Discriminator loss: 0.675914, acc: 0.570312]  [Adversarial loss: 0.502152, acc: 0.902344]\n",
            "55: [Discriminator loss: 0.712284, acc: 0.507812]  [Adversarial loss: 0.522072, acc: 0.855469]\n",
            "56: [Discriminator loss: 0.766013, acc: 0.472656]  [Adversarial loss: 0.452322, acc: 0.910156]\n",
            "57: [Discriminator loss: 0.693899, acc: 0.574219]  [Adversarial loss: 0.443738, acc: 0.890625]\n",
            "58: [Discriminator loss: 0.790203, acc: 0.449219]  [Adversarial loss: 0.418518, acc: 0.921875]\n",
            "59: [Discriminator loss: 0.828340, acc: 0.437500]  [Adversarial loss: 0.416771, acc: 0.921875]\n",
            "60: [Discriminator loss: 0.711695, acc: 0.570312]  [Adversarial loss: 0.392974, acc: 0.925781]\n",
            "61: [Discriminator loss: 0.840936, acc: 0.453125]  [Adversarial loss: 0.409654, acc: 0.925781]\n",
            "62: [Discriminator loss: 0.818086, acc: 0.484375]  [Adversarial loss: 0.395094, acc: 0.925781]\n",
            "63: [Discriminator loss: 0.827998, acc: 0.464844]  [Adversarial loss: 0.427566, acc: 0.902344]\n",
            "64: [Discriminator loss: 0.808074, acc: 0.500000]  [Adversarial loss: 0.391587, acc: 0.925781]\n",
            "65: [Discriminator loss: 0.751723, acc: 0.539062]  [Adversarial loss: 0.375835, acc: 0.933594]\n",
            "66: [Discriminator loss: 0.805932, acc: 0.468750]  [Adversarial loss: 0.385838, acc: 0.929688]\n",
            "67: [Discriminator loss: 0.785166, acc: 0.503906]  [Adversarial loss: 0.363800, acc: 0.941406]\n",
            "68: [Discriminator loss: 0.755226, acc: 0.515625]  [Adversarial loss: 0.417713, acc: 0.894531]\n",
            "69: [Discriminator loss: 0.785488, acc: 0.492188]  [Adversarial loss: 0.413194, acc: 0.902344]\n",
            "70: [Discriminator loss: 0.802766, acc: 0.472656]  [Adversarial loss: 0.412793, acc: 0.917969]\n",
            "71: [Discriminator loss: 0.781724, acc: 0.507812]  [Adversarial loss: 0.434806, acc: 0.886719]\n",
            "72: [Discriminator loss: 0.749267, acc: 0.546875]  [Adversarial loss: 0.403029, acc: 0.917969]\n",
            "73: [Discriminator loss: 0.809792, acc: 0.472656]  [Adversarial loss: 0.383325, acc: 0.929688]\n",
            "74: [Discriminator loss: 0.746179, acc: 0.542969]  [Adversarial loss: 0.405829, acc: 0.906250]\n",
            "75: [Discriminator loss: 0.818222, acc: 0.484375]  [Adversarial loss: 0.467140, acc: 0.886719]\n",
            "76: [Discriminator loss: 0.829085, acc: 0.464844]  [Adversarial loss: 0.414461, acc: 0.882812]\n",
            "77: [Discriminator loss: 0.707403, acc: 0.539062]  [Adversarial loss: 0.396695, acc: 0.917969]\n",
            "78: [Discriminator loss: 0.754833, acc: 0.496094]  [Adversarial loss: 0.423490, acc: 0.925781]\n",
            "79: [Discriminator loss: 0.716067, acc: 0.523438]  [Adversarial loss: 0.382406, acc: 0.910156]\n",
            "80: [Discriminator loss: 0.681082, acc: 0.566406]  [Adversarial loss: 0.420958, acc: 0.910156]\n",
            "81: [Discriminator loss: 0.716985, acc: 0.535156]  [Adversarial loss: 0.412835, acc: 0.902344]\n",
            "82: [Discriminator loss: 0.699842, acc: 0.531250]  [Adversarial loss: 0.427711, acc: 0.906250]\n",
            "83: [Discriminator loss: 0.765059, acc: 0.472656]  [Adversarial loss: 0.386244, acc: 0.937500]\n",
            "84: [Discriminator loss: 0.747668, acc: 0.492188]  [Adversarial loss: 0.435030, acc: 0.882812]\n",
            "85: [Discriminator loss: 0.762229, acc: 0.496094]  [Adversarial loss: 0.429200, acc: 0.917969]\n",
            "86: [Discriminator loss: 0.703377, acc: 0.546875]  [Adversarial loss: 0.387295, acc: 0.906250]\n",
            "87: [Discriminator loss: 0.721442, acc: 0.492188]  [Adversarial loss: 0.470291, acc: 0.882812]\n",
            "88: [Discriminator loss: 0.738189, acc: 0.496094]  [Adversarial loss: 0.413599, acc: 0.894531]\n",
            "89: [Discriminator loss: 0.743780, acc: 0.488281]  [Adversarial loss: 0.444437, acc: 0.886719]\n",
            "90: [Discriminator loss: 0.744132, acc: 0.472656]  [Adversarial loss: 0.439777, acc: 0.898438]\n",
            "91: [Discriminator loss: 0.720097, acc: 0.519531]  [Adversarial loss: 0.433262, acc: 0.894531]\n",
            "92: [Discriminator loss: 0.754568, acc: 0.492188]  [Adversarial loss: 0.426470, acc: 0.890625]\n",
            "93: [Discriminator loss: 0.783496, acc: 0.433594]  [Adversarial loss: 0.411178, acc: 0.914062]\n",
            "94: [Discriminator loss: 0.652295, acc: 0.546875]  [Adversarial loss: 0.400720, acc: 0.910156]\n",
            "95: [Discriminator loss: 0.712475, acc: 0.500000]  [Adversarial loss: 0.422625, acc: 0.902344]\n",
            "96: [Discriminator loss: 0.735226, acc: 0.476562]  [Adversarial loss: 0.387919, acc: 0.894531]\n",
            "97: [Discriminator loss: 0.665103, acc: 0.539062]  [Adversarial loss: 0.398282, acc: 0.875000]\n",
            "98: [Discriminator loss: 0.756831, acc: 0.488281]  [Adversarial loss: 0.464734, acc: 0.871094]\n",
            "99: [Discriminator loss: 0.625637, acc: 0.562500]  [Adversarial loss: 0.388611, acc: 0.890625]\n",
            "100: [Discriminator loss: 0.744067, acc: 0.468750]  [Adversarial loss: 0.385919, acc: 0.882812]\n",
            "101: [Discriminator loss: 0.690648, acc: 0.515625]  [Adversarial loss: 0.407814, acc: 0.890625]\n",
            "102: [Discriminator loss: 0.737354, acc: 0.515625]  [Adversarial loss: 0.372022, acc: 0.914062]\n",
            "103: [Discriminator loss: 0.709621, acc: 0.523438]  [Adversarial loss: 0.409356, acc: 0.867188]\n",
            "104: [Discriminator loss: 0.749219, acc: 0.488281]  [Adversarial loss: 0.417231, acc: 0.875000]\n",
            "105: [Discriminator loss: 0.689752, acc: 0.519531]  [Adversarial loss: 0.361741, acc: 0.890625]\n",
            "106: [Discriminator loss: 0.762337, acc: 0.464844]  [Adversarial loss: 0.367527, acc: 0.898438]\n",
            "107: [Discriminator loss: 0.651878, acc: 0.542969]  [Adversarial loss: 0.356680, acc: 0.902344]\n",
            "108: [Discriminator loss: 0.625818, acc: 0.570312]  [Adversarial loss: 0.349912, acc: 0.906250]\n",
            "109: [Discriminator loss: 0.711401, acc: 0.496094]  [Adversarial loss: 0.362736, acc: 0.914062]\n",
            "110: [Discriminator loss: 0.769722, acc: 0.500000]  [Adversarial loss: 0.412785, acc: 0.894531]\n",
            "111: [Discriminator loss: 0.764040, acc: 0.472656]  [Adversarial loss: 0.429817, acc: 0.886719]\n",
            "112: [Discriminator loss: 0.759468, acc: 0.472656]  [Adversarial loss: 0.356667, acc: 0.941406]\n",
            "113: [Discriminator loss: 0.759996, acc: 0.488281]  [Adversarial loss: 0.393333, acc: 0.906250]\n",
            "114: [Discriminator loss: 0.782579, acc: 0.488281]  [Adversarial loss: 0.392353, acc: 0.921875]\n",
            "115: [Discriminator loss: 0.799142, acc: 0.527344]  [Adversarial loss: 0.328866, acc: 0.949219]\n",
            "116: [Discriminator loss: 0.710960, acc: 0.546875]  [Adversarial loss: 0.376195, acc: 0.917969]\n",
            "117: [Discriminator loss: 0.780644, acc: 0.488281]  [Adversarial loss: 0.358236, acc: 0.917969]\n",
            "118: [Discriminator loss: 0.803677, acc: 0.480469]  [Adversarial loss: 0.353805, acc: 0.949219]\n",
            "119: [Discriminator loss: 0.729243, acc: 0.515625]  [Adversarial loss: 0.365114, acc: 0.929688]\n",
            "120: [Discriminator loss: 0.740760, acc: 0.500000]  [Adversarial loss: 0.433448, acc: 0.898438]\n",
            "121: [Discriminator loss: 0.717937, acc: 0.539062]  [Adversarial loss: 0.365533, acc: 0.925781]\n",
            "122: [Discriminator loss: 0.690953, acc: 0.539062]  [Adversarial loss: 0.372885, acc: 0.937500]\n",
            "123: [Discriminator loss: 0.607245, acc: 0.585938]  [Adversarial loss: 0.398045, acc: 0.917969]\n",
            "124: [Discriminator loss: 0.663389, acc: 0.542969]  [Adversarial loss: 0.344691, acc: 0.937500]\n",
            "125: [Discriminator loss: 0.786908, acc: 0.453125]  [Adversarial loss: 0.372098, acc: 0.925781]\n",
            "126: [Discriminator loss: 0.692086, acc: 0.527344]  [Adversarial loss: 0.402593, acc: 0.910156]\n",
            "127: [Discriminator loss: 0.610659, acc: 0.593750]  [Adversarial loss: 0.402598, acc: 0.882812]\n",
            "128: [Discriminator loss: 0.739945, acc: 0.507812]  [Adversarial loss: 0.392287, acc: 0.910156]\n",
            "129: [Discriminator loss: 0.722096, acc: 0.492188]  [Adversarial loss: 0.337962, acc: 0.945312]\n",
            "130: [Discriminator loss: 0.678088, acc: 0.535156]  [Adversarial loss: 0.379240, acc: 0.906250]\n",
            "131: [Discriminator loss: 0.685099, acc: 0.507812]  [Adversarial loss: 0.359500, acc: 0.929688]\n",
            "132: [Discriminator loss: 0.702958, acc: 0.523438]  [Adversarial loss: 0.347986, acc: 0.929688]\n",
            "133: [Discriminator loss: 0.739967, acc: 0.500000]  [Adversarial loss: 0.388097, acc: 0.882812]\n",
            "134: [Discriminator loss: 0.710683, acc: 0.488281]  [Adversarial loss: 0.390451, acc: 0.914062]\n",
            "135: [Discriminator loss: 0.724048, acc: 0.507812]  [Adversarial loss: 0.341258, acc: 0.941406]\n",
            "136: [Discriminator loss: 0.674618, acc: 0.539062]  [Adversarial loss: 0.373973, acc: 0.886719]\n",
            "137: [Discriminator loss: 0.697733, acc: 0.527344]  [Adversarial loss: 0.356165, acc: 0.917969]\n",
            "138: [Discriminator loss: 0.728253, acc: 0.507812]  [Adversarial loss: 0.356583, acc: 0.914062]\n",
            "139: [Discriminator loss: 0.656416, acc: 0.523438]  [Adversarial loss: 0.331776, acc: 0.945312]\n",
            "140: [Discriminator loss: 0.682396, acc: 0.539062]  [Adversarial loss: 0.349064, acc: 0.921875]\n",
            "141: [Discriminator loss: 0.715635, acc: 0.503906]  [Adversarial loss: 0.360744, acc: 0.917969]\n",
            "142: [Discriminator loss: 0.772587, acc: 0.496094]  [Adversarial loss: 0.359508, acc: 0.902344]\n",
            "143: [Discriminator loss: 0.676337, acc: 0.539062]  [Adversarial loss: 0.405137, acc: 0.882812]\n",
            "144: [Discriminator loss: 0.719434, acc: 0.472656]  [Adversarial loss: 0.359905, acc: 0.910156]\n",
            "145: [Discriminator loss: 0.692931, acc: 0.511719]  [Adversarial loss: 0.335091, acc: 0.902344]\n",
            "146: [Discriminator loss: 0.704338, acc: 0.535156]  [Adversarial loss: 0.353484, acc: 0.906250]\n",
            "147: [Discriminator loss: 0.693884, acc: 0.468750]  [Adversarial loss: 0.365048, acc: 0.925781]\n",
            "148: [Discriminator loss: 0.672925, acc: 0.539062]  [Adversarial loss: 0.347629, acc: 0.933594]\n",
            "149: [Discriminator loss: 0.695712, acc: 0.519531]  [Adversarial loss: 0.363237, acc: 0.937500]\n",
            "150: [Discriminator loss: 0.696925, acc: 0.527344]  [Adversarial loss: 0.333705, acc: 0.921875]\n",
            "151: [Discriminator loss: 0.737780, acc: 0.480469]  [Adversarial loss: 0.362079, acc: 0.906250]\n",
            "152: [Discriminator loss: 0.679130, acc: 0.515625]  [Adversarial loss: 0.396308, acc: 0.882812]\n",
            "153: [Discriminator loss: 0.742195, acc: 0.476562]  [Adversarial loss: 0.393986, acc: 0.914062]\n",
            "154: [Discriminator loss: 0.682118, acc: 0.503906]  [Adversarial loss: 0.382078, acc: 0.894531]\n",
            "155: [Discriminator loss: 0.759702, acc: 0.476562]  [Adversarial loss: 0.405011, acc: 0.910156]\n",
            "156: [Discriminator loss: 0.708020, acc: 0.500000]  [Adversarial loss: 0.392202, acc: 0.914062]\n",
            "157: [Discriminator loss: 0.723649, acc: 0.457031]  [Adversarial loss: 0.397146, acc: 0.902344]\n",
            "158: [Discriminator loss: 0.705224, acc: 0.472656]  [Adversarial loss: 0.385710, acc: 0.917969]\n",
            "159: [Discriminator loss: 0.683068, acc: 0.480469]  [Adversarial loss: 0.403033, acc: 0.898438]\n",
            "160: [Discriminator loss: 0.681747, acc: 0.503906]  [Adversarial loss: 0.381438, acc: 0.902344]\n",
            "161: [Discriminator loss: 0.770901, acc: 0.449219]  [Adversarial loss: 0.380495, acc: 0.925781]\n",
            "162: [Discriminator loss: 0.613474, acc: 0.527344]  [Adversarial loss: 0.384070, acc: 0.921875]\n",
            "163: [Discriminator loss: 0.605558, acc: 0.546875]  [Adversarial loss: 0.327827, acc: 0.925781]\n",
            "164: [Discriminator loss: 0.780209, acc: 0.441406]  [Adversarial loss: 0.344530, acc: 0.925781]\n",
            "165: [Discriminator loss: 0.693974, acc: 0.496094]  [Adversarial loss: 0.391649, acc: 0.878906]\n",
            "166: [Discriminator loss: 0.660010, acc: 0.539062]  [Adversarial loss: 0.327120, acc: 0.917969]\n",
            "167: [Discriminator loss: 0.676853, acc: 0.488281]  [Adversarial loss: 0.373058, acc: 0.910156]\n",
            "168: [Discriminator loss: 0.687533, acc: 0.480469]  [Adversarial loss: 0.363079, acc: 0.910156]\n",
            "169: [Discriminator loss: 0.671421, acc: 0.507812]  [Adversarial loss: 0.342757, acc: 0.914062]\n",
            "170: [Discriminator loss: 0.715269, acc: 0.468750]  [Adversarial loss: 0.391473, acc: 0.890625]\n",
            "171: [Discriminator loss: 0.685291, acc: 0.503906]  [Adversarial loss: 0.381125, acc: 0.898438]\n",
            "172: [Discriminator loss: 0.665838, acc: 0.523438]  [Adversarial loss: 0.347368, acc: 0.925781]\n",
            "173: [Discriminator loss: 0.588510, acc: 0.546875]  [Adversarial loss: 0.338647, acc: 0.933594]\n",
            "174: [Discriminator loss: 0.654125, acc: 0.542969]  [Adversarial loss: 0.369582, acc: 0.906250]\n",
            "175: [Discriminator loss: 0.657078, acc: 0.539062]  [Adversarial loss: 0.334564, acc: 0.925781]\n",
            "176: [Discriminator loss: 0.693615, acc: 0.488281]  [Adversarial loss: 0.353707, acc: 0.902344]\n",
            "177: [Discriminator loss: 0.625923, acc: 0.515625]  [Adversarial loss: 0.320966, acc: 0.953125]\n",
            "178: [Discriminator loss: 0.618720, acc: 0.539062]  [Adversarial loss: 0.340974, acc: 0.898438]\n",
            "179: [Discriminator loss: 0.676259, acc: 0.476562]  [Adversarial loss: 0.360655, acc: 0.902344]\n",
            "180: [Discriminator loss: 0.725806, acc: 0.503906]  [Adversarial loss: 0.376930, acc: 0.886719]\n",
            "181: [Discriminator loss: 0.669161, acc: 0.468750]  [Adversarial loss: 0.367914, acc: 0.898438]\n",
            "182: [Discriminator loss: 0.682036, acc: 0.492188]  [Adversarial loss: 0.381918, acc: 0.894531]\n",
            "183: [Discriminator loss: 0.653516, acc: 0.511719]  [Adversarial loss: 0.364539, acc: 0.894531]\n",
            "184: [Discriminator loss: 0.600258, acc: 0.550781]  [Adversarial loss: 0.360861, acc: 0.910156]\n",
            "185: [Discriminator loss: 0.649688, acc: 0.496094]  [Adversarial loss: 0.356764, acc: 0.914062]\n",
            "186: [Discriminator loss: 0.609415, acc: 0.515625]  [Adversarial loss: 0.348642, acc: 0.906250]\n",
            "187: [Discriminator loss: 0.632581, acc: 0.503906]  [Adversarial loss: 0.368067, acc: 0.914062]\n",
            "188: [Discriminator loss: 0.612447, acc: 0.503906]  [Adversarial loss: 0.361571, acc: 0.906250]\n",
            "189: [Discriminator loss: 0.671454, acc: 0.480469]  [Adversarial loss: 0.299938, acc: 0.960938]\n",
            "190: [Discriminator loss: 0.644233, acc: 0.492188]  [Adversarial loss: 0.314989, acc: 0.945312]\n",
            "191: [Discriminator loss: 0.675120, acc: 0.480469]  [Adversarial loss: 0.370839, acc: 0.917969]\n",
            "192: [Discriminator loss: 0.640228, acc: 0.488281]  [Adversarial loss: 0.341931, acc: 0.894531]\n",
            "193: [Discriminator loss: 0.693500, acc: 0.468750]  [Adversarial loss: 0.341319, acc: 0.933594]\n",
            "194: [Discriminator loss: 0.654376, acc: 0.500000]  [Adversarial loss: 0.317245, acc: 0.925781]\n",
            "195: [Discriminator loss: 0.655120, acc: 0.453125]  [Adversarial loss: 0.363668, acc: 0.925781]\n",
            "196: [Discriminator loss: 0.570897, acc: 0.539062]  [Adversarial loss: 0.351900, acc: 0.910156]\n",
            "197: [Discriminator loss: 0.667081, acc: 0.464844]  [Adversarial loss: 0.370033, acc: 0.894531]\n",
            "198: [Discriminator loss: 0.593997, acc: 0.535156]  [Adversarial loss: 0.334965, acc: 0.917969]\n",
            "199: [Discriminator loss: 0.620715, acc: 0.523438]  [Adversarial loss: 0.295710, acc: 0.945312]\n",
            "200: [Discriminator loss: 0.615264, acc: 0.507812]  [Adversarial loss: 0.360607, acc: 0.914062]\n",
            "201: [Discriminator loss: 0.655199, acc: 0.484375]  [Adversarial loss: 0.389803, acc: 0.902344]\n",
            "202: [Discriminator loss: 0.595843, acc: 0.539062]  [Adversarial loss: 0.310109, acc: 0.914062]\n",
            "203: [Discriminator loss: 0.568722, acc: 0.550781]  [Adversarial loss: 0.328260, acc: 0.882812]\n",
            "204: [Discriminator loss: 0.651420, acc: 0.464844]  [Adversarial loss: 0.343467, acc: 0.917969]\n",
            "205: [Discriminator loss: 0.569330, acc: 0.519531]  [Adversarial loss: 0.336974, acc: 0.902344]\n",
            "206: [Discriminator loss: 0.616768, acc: 0.496094]  [Adversarial loss: 0.348892, acc: 0.894531]\n",
            "207: [Discriminator loss: 0.646704, acc: 0.488281]  [Adversarial loss: 0.344156, acc: 0.894531]\n",
            "208: [Discriminator loss: 0.572357, acc: 0.500000]  [Adversarial loss: 0.328269, acc: 0.917969]\n",
            "209: [Discriminator loss: 0.577768, acc: 0.542969]  [Adversarial loss: 0.307292, acc: 0.949219]\n",
            "210: [Discriminator loss: 0.556556, acc: 0.535156]  [Adversarial loss: 0.306563, acc: 0.917969]\n",
            "211: [Discriminator loss: 0.597193, acc: 0.503906]  [Adversarial loss: 0.327469, acc: 0.894531]\n",
            "212: [Discriminator loss: 0.637729, acc: 0.492188]  [Adversarial loss: 0.344603, acc: 0.902344]\n",
            "213: [Discriminator loss: 0.545774, acc: 0.566406]  [Adversarial loss: 0.265122, acc: 0.949219]\n",
            "214: [Discriminator loss: 0.587872, acc: 0.527344]  [Adversarial loss: 0.312995, acc: 0.917969]\n",
            "215: [Discriminator loss: 0.594150, acc: 0.492188]  [Adversarial loss: 0.337302, acc: 0.921875]\n",
            "216: [Discriminator loss: 0.639283, acc: 0.453125]  [Adversarial loss: 0.322045, acc: 0.937500]\n",
            "217: [Discriminator loss: 0.531591, acc: 0.566406]  [Adversarial loss: 0.279778, acc: 0.929688]\n",
            "218: [Discriminator loss: 0.623162, acc: 0.511719]  [Adversarial loss: 0.269815, acc: 0.960938]\n",
            "219: [Discriminator loss: 0.623394, acc: 0.488281]  [Adversarial loss: 0.306801, acc: 0.957031]\n",
            "220: [Discriminator loss: 0.613100, acc: 0.503906]  [Adversarial loss: 0.321973, acc: 0.933594]\n",
            "221: [Discriminator loss: 0.585591, acc: 0.496094]  [Adversarial loss: 0.308239, acc: 0.933594]\n",
            "222: [Discriminator loss: 0.573787, acc: 0.492188]  [Adversarial loss: 0.353720, acc: 0.890625]\n",
            "223: [Discriminator loss: 0.625850, acc: 0.519531]  [Adversarial loss: 0.295343, acc: 0.925781]\n",
            "224: [Discriminator loss: 0.603549, acc: 0.492188]  [Adversarial loss: 0.337894, acc: 0.917969]\n",
            "225: [Discriminator loss: 0.558940, acc: 0.531250]  [Adversarial loss: 0.291660, acc: 0.937500]\n",
            "226: [Discriminator loss: 0.597551, acc: 0.519531]  [Adversarial loss: 0.301413, acc: 0.917969]\n",
            "227: [Discriminator loss: 0.601914, acc: 0.492188]  [Adversarial loss: 0.330876, acc: 0.921875]\n",
            "228: [Discriminator loss: 0.474689, acc: 0.597656]  [Adversarial loss: 0.287957, acc: 0.917969]\n",
            "229: [Discriminator loss: 0.647642, acc: 0.476562]  [Adversarial loss: 0.326212, acc: 0.914062]\n",
            "230: [Discriminator loss: 0.564262, acc: 0.535156]  [Adversarial loss: 0.293244, acc: 0.949219]\n",
            "231: [Discriminator loss: 0.617550, acc: 0.527344]  [Adversarial loss: 0.286874, acc: 0.933594]\n",
            "232: [Discriminator loss: 0.501680, acc: 0.566406]  [Adversarial loss: 0.264926, acc: 0.929688]\n",
            "233: [Discriminator loss: 0.572104, acc: 0.503906]  [Adversarial loss: 0.300557, acc: 0.921875]\n",
            "234: [Discriminator loss: 0.642065, acc: 0.457031]  [Adversarial loss: 0.316539, acc: 0.917969]\n",
            "235: [Discriminator loss: 0.569332, acc: 0.539062]  [Adversarial loss: 0.275780, acc: 0.937500]\n",
            "236: [Discriminator loss: 0.605172, acc: 0.515625]  [Adversarial loss: 0.308687, acc: 0.937500]\n",
            "237: [Discriminator loss: 0.579435, acc: 0.503906]  [Adversarial loss: 0.302979, acc: 0.953125]\n",
            "238: [Discriminator loss: 0.505646, acc: 0.589844]  [Adversarial loss: 0.277517, acc: 0.925781]\n",
            "239: [Discriminator loss: 0.564670, acc: 0.511719]  [Adversarial loss: 0.290489, acc: 0.945312]\n",
            "240: [Discriminator loss: 0.557221, acc: 0.558594]  [Adversarial loss: 0.267130, acc: 0.953125]\n",
            "241: [Discriminator loss: 0.571206, acc: 0.472656]  [Adversarial loss: 0.307316, acc: 0.937500]\n",
            "242: [Discriminator loss: 0.547505, acc: 0.523438]  [Adversarial loss: 0.287327, acc: 0.945312]\n",
            "243: [Discriminator loss: 0.577137, acc: 0.539062]  [Adversarial loss: 0.281219, acc: 0.953125]\n",
            "244: [Discriminator loss: 0.575958, acc: 0.507812]  [Adversarial loss: 0.297822, acc: 0.945312]\n",
            "245: [Discriminator loss: 0.555558, acc: 0.519531]  [Adversarial loss: 0.300925, acc: 0.949219]\n",
            "246: [Discriminator loss: 0.540392, acc: 0.546875]  [Adversarial loss: 0.280981, acc: 0.933594]\n",
            "247: [Discriminator loss: 0.592075, acc: 0.523438]  [Adversarial loss: 0.302217, acc: 0.925781]\n",
            "248: [Discriminator loss: 0.551801, acc: 0.546875]  [Adversarial loss: 0.291210, acc: 0.949219]\n",
            "249: [Discriminator loss: 0.586169, acc: 0.484375]  [Adversarial loss: 0.316414, acc: 0.953125]\n",
            "250: [Discriminator loss: 0.564902, acc: 0.492188]  [Adversarial loss: 0.319650, acc: 0.933594]\n",
            "251: [Discriminator loss: 0.516637, acc: 0.546875]  [Adversarial loss: 0.287332, acc: 0.945312]\n",
            "252: [Discriminator loss: 0.522155, acc: 0.574219]  [Adversarial loss: 0.285420, acc: 0.945312]\n",
            "253: [Discriminator loss: 0.621100, acc: 0.496094]  [Adversarial loss: 0.302913, acc: 0.929688]\n",
            "254: [Discriminator loss: 0.576083, acc: 0.492188]  [Adversarial loss: 0.321869, acc: 0.917969]\n",
            "255: [Discriminator loss: 0.581479, acc: 0.503906]  [Adversarial loss: 0.307389, acc: 0.937500]\n",
            "256: [Discriminator loss: 0.611923, acc: 0.523438]  [Adversarial loss: 0.298302, acc: 0.949219]\n",
            "257: [Discriminator loss: 0.587104, acc: 0.511719]  [Adversarial loss: 0.311743, acc: 0.933594]\n",
            "258: [Discriminator loss: 0.560991, acc: 0.535156]  [Adversarial loss: 0.294175, acc: 0.929688]\n",
            "259: [Discriminator loss: 0.588180, acc: 0.492188]  [Adversarial loss: 0.325522, acc: 0.929688]\n",
            "260: [Discriminator loss: 0.659354, acc: 0.441406]  [Adversarial loss: 0.329609, acc: 0.937500]\n",
            "261: [Discriminator loss: 0.532872, acc: 0.535156]  [Adversarial loss: 0.309511, acc: 0.945312]\n",
            "262: [Discriminator loss: 0.553607, acc: 0.511719]  [Adversarial loss: 0.302610, acc: 0.960938]\n",
            "263: [Discriminator loss: 0.560156, acc: 0.496094]  [Adversarial loss: 0.316927, acc: 0.933594]\n",
            "264: [Discriminator loss: 0.526322, acc: 0.546875]  [Adversarial loss: 0.294647, acc: 0.957031]\n",
            "265: [Discriminator loss: 0.575368, acc: 0.515625]  [Adversarial loss: 0.298488, acc: 0.945312]\n",
            "266: [Discriminator loss: 0.665970, acc: 0.445312]  [Adversarial loss: 0.342339, acc: 0.933594]\n",
            "267: [Discriminator loss: 0.554086, acc: 0.511719]  [Adversarial loss: 0.293519, acc: 0.941406]\n",
            "268: [Discriminator loss: 0.527670, acc: 0.527344]  [Adversarial loss: 0.301293, acc: 0.953125]\n",
            "269: [Discriminator loss: 0.570723, acc: 0.539062]  [Adversarial loss: 0.288446, acc: 0.933594]\n",
            "270: [Discriminator loss: 0.566168, acc: 0.519531]  [Adversarial loss: 0.262629, acc: 0.972656]\n",
            "271: [Discriminator loss: 0.609562, acc: 0.468750]  [Adversarial loss: 0.307355, acc: 0.929688]\n",
            "272: [Discriminator loss: 0.587877, acc: 0.511719]  [Adversarial loss: 0.287259, acc: 0.957031]\n",
            "273: [Discriminator loss: 0.604031, acc: 0.484375]  [Adversarial loss: 0.314915, acc: 0.933594]\n",
            "274: [Discriminator loss: 0.543385, acc: 0.500000]  [Adversarial loss: 0.307491, acc: 0.953125]\n",
            "275: [Discriminator loss: 0.507102, acc: 0.570312]  [Adversarial loss: 0.250810, acc: 0.957031]\n",
            "276: [Discriminator loss: 0.570664, acc: 0.523438]  [Adversarial loss: 0.315783, acc: 0.945312]\n",
            "277: [Discriminator loss: 0.516087, acc: 0.539062]  [Adversarial loss: 0.273456, acc: 0.957031]\n",
            "278: [Discriminator loss: 0.540030, acc: 0.558594]  [Adversarial loss: 0.289087, acc: 0.945312]\n",
            "279: [Discriminator loss: 0.527932, acc: 0.542969]  [Adversarial loss: 0.296680, acc: 0.933594]\n",
            "280: [Discriminator loss: 0.569274, acc: 0.523438]  [Adversarial loss: 0.292747, acc: 0.945312]\n",
            "281: [Discriminator loss: 0.548532, acc: 0.527344]  [Adversarial loss: 0.301058, acc: 0.949219]\n",
            "282: [Discriminator loss: 0.533443, acc: 0.523438]  [Adversarial loss: 0.277785, acc: 0.980469]\n",
            "283: [Discriminator loss: 0.579223, acc: 0.472656]  [Adversarial loss: 0.304044, acc: 0.972656]\n",
            "284: [Discriminator loss: 0.538824, acc: 0.542969]  [Adversarial loss: 0.272112, acc: 0.972656]\n",
            "285: [Discriminator loss: 0.567377, acc: 0.507812]  [Adversarial loss: 0.300167, acc: 0.949219]\n",
            "286: [Discriminator loss: 0.598496, acc: 0.476562]  [Adversarial loss: 0.318945, acc: 0.949219]\n",
            "287: [Discriminator loss: 0.586382, acc: 0.484375]  [Adversarial loss: 0.290779, acc: 0.964844]\n",
            "288: [Discriminator loss: 0.577288, acc: 0.511719]  [Adversarial loss: 0.302377, acc: 0.937500]\n",
            "289: [Discriminator loss: 0.537492, acc: 0.535156]  [Adversarial loss: 0.285705, acc: 0.949219]\n",
            "290: [Discriminator loss: 0.522622, acc: 0.550781]  [Adversarial loss: 0.276091, acc: 0.964844]\n",
            "291: [Discriminator loss: 0.548213, acc: 0.492188]  [Adversarial loss: 0.302535, acc: 0.953125]\n",
            "292: [Discriminator loss: 0.573341, acc: 0.531250]  [Adversarial loss: 0.276561, acc: 0.953125]\n",
            "293: [Discriminator loss: 0.598775, acc: 0.464844]  [Adversarial loss: 0.323364, acc: 0.953125]\n",
            "294: [Discriminator loss: 0.581860, acc: 0.492188]  [Adversarial loss: 0.303117, acc: 0.949219]\n",
            "295: [Discriminator loss: 0.536926, acc: 0.503906]  [Adversarial loss: 0.300278, acc: 0.957031]\n",
            "296: [Discriminator loss: 0.595459, acc: 0.460938]  [Adversarial loss: 0.317983, acc: 0.960938]\n",
            "297: [Discriminator loss: 0.555604, acc: 0.503906]  [Adversarial loss: 0.295321, acc: 0.953125]\n",
            "298: [Discriminator loss: 0.561814, acc: 0.496094]  [Adversarial loss: 0.305387, acc: 0.968750]\n",
            "299: [Discriminator loss: 0.554207, acc: 0.496094]  [Adversarial loss: 0.289510, acc: 0.972656]\n",
            "300: [Discriminator loss: 0.538010, acc: 0.515625]  [Adversarial loss: 0.278992, acc: 0.972656]\n",
            "301: [Discriminator loss: 0.530279, acc: 0.523438]  [Adversarial loss: 0.307588, acc: 0.957031]\n",
            "302: [Discriminator loss: 0.526532, acc: 0.531250]  [Adversarial loss: 0.290459, acc: 0.964844]\n",
            "303: [Discriminator loss: 0.523510, acc: 0.527344]  [Adversarial loss: 0.273713, acc: 0.976562]\n",
            "304: [Discriminator loss: 0.527539, acc: 0.535156]  [Adversarial loss: 0.268828, acc: 0.976562]\n",
            "305: [Discriminator loss: 0.536505, acc: 0.496094]  [Adversarial loss: 0.293080, acc: 0.980469]\n",
            "306: [Discriminator loss: 0.528344, acc: 0.519531]  [Adversarial loss: 0.299120, acc: 0.964844]\n",
            "307: [Discriminator loss: 0.596891, acc: 0.449219]  [Adversarial loss: 0.322238, acc: 0.957031]\n",
            "308: [Discriminator loss: 0.525782, acc: 0.515625]  [Adversarial loss: 0.286081, acc: 0.945312]\n",
            "309: [Discriminator loss: 0.586040, acc: 0.457031]  [Adversarial loss: 0.334883, acc: 0.933594]\n",
            "310: [Discriminator loss: 0.506271, acc: 0.554688]  [Adversarial loss: 0.273204, acc: 0.953125]\n",
            "311: [Discriminator loss: 0.576135, acc: 0.507812]  [Adversarial loss: 0.283288, acc: 0.972656]\n",
            "312: [Discriminator loss: 0.578282, acc: 0.480469]  [Adversarial loss: 0.308719, acc: 0.960938]\n",
            "313: [Discriminator loss: 0.528471, acc: 0.546875]  [Adversarial loss: 0.292112, acc: 0.937500]\n",
            "314: [Discriminator loss: 0.561644, acc: 0.484375]  [Adversarial loss: 0.321206, acc: 0.953125]\n",
            "315: [Discriminator loss: 0.560603, acc: 0.519531]  [Adversarial loss: 0.294922, acc: 0.953125]\n",
            "316: [Discriminator loss: 0.583243, acc: 0.457031]  [Adversarial loss: 0.318183, acc: 0.953125]\n",
            "317: [Discriminator loss: 0.590869, acc: 0.429688]  [Adversarial loss: 0.330978, acc: 0.964844]\n",
            "318: [Discriminator loss: 0.525293, acc: 0.527344]  [Adversarial loss: 0.286208, acc: 0.937500]\n",
            "319: [Discriminator loss: 0.502452, acc: 0.535156]  [Adversarial loss: 0.285321, acc: 0.929688]\n",
            "320: [Discriminator loss: 0.538905, acc: 0.496094]  [Adversarial loss: 0.301308, acc: 0.960938]\n",
            "321: [Discriminator loss: 0.534686, acc: 0.488281]  [Adversarial loss: 0.297282, acc: 0.960938]\n",
            "322: [Discriminator loss: 0.527774, acc: 0.519531]  [Adversarial loss: 0.279985, acc: 0.960938]\n",
            "323: [Discriminator loss: 0.508549, acc: 0.539062]  [Adversarial loss: 0.264620, acc: 0.949219]\n",
            "324: [Discriminator loss: 0.512486, acc: 0.546875]  [Adversarial loss: 0.254320, acc: 0.960938]\n",
            "325: [Discriminator loss: 0.593741, acc: 0.492188]  [Adversarial loss: 0.309667, acc: 0.949219]\n",
            "326: [Discriminator loss: 0.561787, acc: 0.511719]  [Adversarial loss: 0.277205, acc: 0.949219]\n",
            "327: [Discriminator loss: 0.505441, acc: 0.535156]  [Adversarial loss: 0.274507, acc: 0.976562]\n",
            "328: [Discriminator loss: 0.550136, acc: 0.500000]  [Adversarial loss: 0.284461, acc: 0.960938]\n",
            "329: [Discriminator loss: 0.480052, acc: 0.578125]  [Adversarial loss: 0.258501, acc: 0.980469]\n",
            "330: [Discriminator loss: 0.593450, acc: 0.492188]  [Adversarial loss: 0.296958, acc: 0.964844]\n",
            "331: [Discriminator loss: 0.485913, acc: 0.566406]  [Adversarial loss: 0.266525, acc: 0.964844]\n",
            "332: [Discriminator loss: 0.571971, acc: 0.480469]  [Adversarial loss: 0.323788, acc: 0.937500]\n",
            "333: [Discriminator loss: 0.557823, acc: 0.488281]  [Adversarial loss: 0.293982, acc: 0.968750]\n",
            "334: [Discriminator loss: 0.520483, acc: 0.511719]  [Adversarial loss: 0.288406, acc: 0.957031]\n",
            "335: [Discriminator loss: 0.550353, acc: 0.464844]  [Adversarial loss: 0.290239, acc: 0.972656]\n",
            "336: [Discriminator loss: 0.531253, acc: 0.515625]  [Adversarial loss: 0.292179, acc: 0.960938]\n",
            "337: [Discriminator loss: 0.523407, acc: 0.515625]  [Adversarial loss: 0.309386, acc: 0.968750]\n",
            "338: [Discriminator loss: 0.513210, acc: 0.523438]  [Adversarial loss: 0.279098, acc: 0.941406]\n",
            "339: [Discriminator loss: 0.494651, acc: 0.527344]  [Adversarial loss: 0.272412, acc: 0.972656]\n",
            "340: [Discriminator loss: 0.556414, acc: 0.472656]  [Adversarial loss: 0.312686, acc: 0.976562]\n",
            "341: [Discriminator loss: 0.509772, acc: 0.554688]  [Adversarial loss: 0.302228, acc: 0.972656]\n",
            "342: [Discriminator loss: 0.624651, acc: 0.437500]  [Adversarial loss: 0.340917, acc: 0.960938]\n",
            "343: [Discriminator loss: 0.514789, acc: 0.503906]  [Adversarial loss: 0.288582, acc: 0.980469]\n",
            "344: [Discriminator loss: 0.597091, acc: 0.472656]  [Adversarial loss: 0.301182, acc: 0.980469]\n",
            "345: [Discriminator loss: 0.575537, acc: 0.492188]  [Adversarial loss: 0.309356, acc: 0.964844]\n",
            "346: [Discriminator loss: 0.523355, acc: 0.531250]  [Adversarial loss: 0.270748, acc: 0.984375]\n",
            "347: [Discriminator loss: 0.543155, acc: 0.515625]  [Adversarial loss: 0.285674, acc: 0.964844]\n",
            "348: [Discriminator loss: 0.567991, acc: 0.488281]  [Adversarial loss: 0.296192, acc: 0.972656]\n",
            "349: [Discriminator loss: 0.534116, acc: 0.531250]  [Adversarial loss: 0.309165, acc: 0.960938]\n",
            "350: [Discriminator loss: 0.557581, acc: 0.492188]  [Adversarial loss: 0.327199, acc: 0.937500]\n",
            "351: [Discriminator loss: 0.485366, acc: 0.523438]  [Adversarial loss: 0.281967, acc: 0.964844]\n",
            "352: [Discriminator loss: 0.574737, acc: 0.464844]  [Adversarial loss: 0.316822, acc: 0.964844]\n",
            "353: [Discriminator loss: 0.545138, acc: 0.457031]  [Adversarial loss: 0.346217, acc: 0.964844]\n",
            "354: [Discriminator loss: 0.553211, acc: 0.519531]  [Adversarial loss: 0.323885, acc: 0.941406]\n",
            "355: [Discriminator loss: 0.459773, acc: 0.570312]  [Adversarial loss: 0.297569, acc: 0.960938]\n",
            "356: [Discriminator loss: 0.475165, acc: 0.558594]  [Adversarial loss: 0.256198, acc: 0.988281]\n",
            "357: [Discriminator loss: 0.562132, acc: 0.472656]  [Adversarial loss: 0.318701, acc: 0.933594]\n",
            "358: [Discriminator loss: 0.611528, acc: 0.476562]  [Adversarial loss: 0.314386, acc: 0.937500]\n",
            "359: [Discriminator loss: 0.488489, acc: 0.527344]  [Adversarial loss: 0.289241, acc: 0.945312]\n",
            "360: [Discriminator loss: 0.548864, acc: 0.488281]  [Adversarial loss: 0.290852, acc: 0.972656]\n",
            "361: [Discriminator loss: 0.539408, acc: 0.527344]  [Adversarial loss: 0.270890, acc: 0.949219]\n",
            "362: [Discriminator loss: 0.573008, acc: 0.457031]  [Adversarial loss: 0.310338, acc: 0.972656]\n",
            "363: [Discriminator loss: 0.590439, acc: 0.472656]  [Adversarial loss: 0.295842, acc: 0.964844]\n",
            "364: [Discriminator loss: 0.493958, acc: 0.527344]  [Adversarial loss: 0.294926, acc: 0.937500]\n",
            "365: [Discriminator loss: 0.521622, acc: 0.496094]  [Adversarial loss: 0.309379, acc: 0.957031]\n",
            "366: [Discriminator loss: 0.517482, acc: 0.535156]  [Adversarial loss: 0.298588, acc: 0.953125]\n",
            "367: [Discriminator loss: 0.561702, acc: 0.476562]  [Adversarial loss: 0.297182, acc: 0.957031]\n",
            "368: [Discriminator loss: 0.487043, acc: 0.535156]  [Adversarial loss: 0.261191, acc: 0.980469]\n",
            "369: [Discriminator loss: 0.509098, acc: 0.496094]  [Adversarial loss: 0.294382, acc: 0.960938]\n",
            "370: [Discriminator loss: 0.530034, acc: 0.484375]  [Adversarial loss: 0.293456, acc: 0.964844]\n",
            "371: [Discriminator loss: 0.551905, acc: 0.484375]  [Adversarial loss: 0.291677, acc: 0.957031]\n",
            "372: [Discriminator loss: 0.557648, acc: 0.464844]  [Adversarial loss: 0.307915, acc: 0.980469]\n",
            "373: [Discriminator loss: 0.512874, acc: 0.511719]  [Adversarial loss: 0.281255, acc: 0.964844]\n",
            "374: [Discriminator loss: 0.560122, acc: 0.464844]  [Adversarial loss: 0.318017, acc: 0.957031]\n",
            "375: [Discriminator loss: 0.529582, acc: 0.511719]  [Adversarial loss: 0.284892, acc: 0.949219]\n",
            "376: [Discriminator loss: 0.560811, acc: 0.472656]  [Adversarial loss: 0.305583, acc: 0.953125]\n",
            "377: [Discriminator loss: 0.526821, acc: 0.500000]  [Adversarial loss: 0.293244, acc: 0.960938]\n",
            "378: [Discriminator loss: 0.575262, acc: 0.480469]  [Adversarial loss: 0.287155, acc: 0.984375]\n",
            "379: [Discriminator loss: 0.617117, acc: 0.449219]  [Adversarial loss: 0.318817, acc: 0.941406]\n",
            "380: [Discriminator loss: 0.546486, acc: 0.492188]  [Adversarial loss: 0.289798, acc: 0.968750]\n",
            "381: [Discriminator loss: 0.497754, acc: 0.554688]  [Adversarial loss: 0.270879, acc: 0.976562]\n",
            "382: [Discriminator loss: 0.532739, acc: 0.519531]  [Adversarial loss: 0.277287, acc: 0.968750]\n",
            "383: [Discriminator loss: 0.566788, acc: 0.457031]  [Adversarial loss: 0.310281, acc: 0.957031]\n",
            "384: [Discriminator loss: 0.457280, acc: 0.539062]  [Adversarial loss: 0.255502, acc: 0.980469]\n",
            "385: [Discriminator loss: 0.540934, acc: 0.511719]  [Adversarial loss: 0.282126, acc: 0.964844]\n",
            "386: [Discriminator loss: 0.515489, acc: 0.523438]  [Adversarial loss: 0.291991, acc: 0.945312]\n",
            "387: [Discriminator loss: 0.440446, acc: 0.585938]  [Adversarial loss: 0.235312, acc: 0.960938]\n",
            "388: [Discriminator loss: 0.525245, acc: 0.492188]  [Adversarial loss: 0.291535, acc: 0.968750]\n",
            "389: [Discriminator loss: 0.579409, acc: 0.468750]  [Adversarial loss: 0.330936, acc: 0.949219]\n",
            "390: [Discriminator loss: 0.512043, acc: 0.539062]  [Adversarial loss: 0.245466, acc: 0.980469]\n",
            "391: [Discriminator loss: 0.586084, acc: 0.457031]  [Adversarial loss: 0.310262, acc: 0.953125]\n",
            "392: [Discriminator loss: 0.566604, acc: 0.496094]  [Adversarial loss: 0.308410, acc: 0.964844]\n",
            "393: [Discriminator loss: 0.554650, acc: 0.457031]  [Adversarial loss: 0.301693, acc: 0.976562]\n",
            "394: [Discriminator loss: 0.482381, acc: 0.558594]  [Adversarial loss: 0.260046, acc: 0.964844]\n",
            "395: [Discriminator loss: 0.481914, acc: 0.550781]  [Adversarial loss: 0.272412, acc: 0.953125]\n",
            "396: [Discriminator loss: 0.552142, acc: 0.480469]  [Adversarial loss: 0.295780, acc: 0.980469]\n",
            "397: [Discriminator loss: 0.558668, acc: 0.500000]  [Adversarial loss: 0.294213, acc: 0.964844]\n",
            "398: [Discriminator loss: 0.562211, acc: 0.488281]  [Adversarial loss: 0.320759, acc: 0.960938]\n",
            "399: [Discriminator loss: 0.568846, acc: 0.464844]  [Adversarial loss: 0.296117, acc: 0.968750]\n",
            "400: [Discriminator loss: 0.532008, acc: 0.507812]  [Adversarial loss: 0.289703, acc: 0.972656]\n",
            "401: [Discriminator loss: 0.582913, acc: 0.484375]  [Adversarial loss: 0.288752, acc: 0.968750]\n",
            "402: [Discriminator loss: 0.592799, acc: 0.460938]  [Adversarial loss: 0.296953, acc: 0.976562]\n",
            "403: [Discriminator loss: 0.540961, acc: 0.488281]  [Adversarial loss: 0.293004, acc: 0.972656]\n",
            "404: [Discriminator loss: 0.526770, acc: 0.500000]  [Adversarial loss: 0.276068, acc: 0.988281]\n",
            "405: [Discriminator loss: 0.554892, acc: 0.519531]  [Adversarial loss: 0.257851, acc: 0.988281]\n",
            "406: [Discriminator loss: 0.562338, acc: 0.511719]  [Adversarial loss: 0.273388, acc: 0.949219]\n",
            "407: [Discriminator loss: 0.571993, acc: 0.449219]  [Adversarial loss: 0.306246, acc: 0.984375]\n",
            "408: [Discriminator loss: 0.592525, acc: 0.433594]  [Adversarial loss: 0.326481, acc: 0.960938]\n",
            "409: [Discriminator loss: 0.547772, acc: 0.507812]  [Adversarial loss: 0.295430, acc: 0.964844]\n",
            "410: [Discriminator loss: 0.510856, acc: 0.531250]  [Adversarial loss: 0.273413, acc: 0.957031]\n",
            "411: [Discriminator loss: 0.536361, acc: 0.527344]  [Adversarial loss: 0.273726, acc: 0.976562]\n",
            "412: [Discriminator loss: 0.541157, acc: 0.500000]  [Adversarial loss: 0.286835, acc: 0.968750]\n",
            "413: [Discriminator loss: 0.574325, acc: 0.503906]  [Adversarial loss: 0.289684, acc: 0.960938]\n",
            "414: [Discriminator loss: 0.574166, acc: 0.503906]  [Adversarial loss: 0.309910, acc: 0.945312]\n",
            "415: [Discriminator loss: 0.593906, acc: 0.433594]  [Adversarial loss: 0.334869, acc: 0.960938]\n",
            "416: [Discriminator loss: 0.526005, acc: 0.519531]  [Adversarial loss: 0.270990, acc: 0.976562]\n",
            "417: [Discriminator loss: 0.560014, acc: 0.500000]  [Adversarial loss: 0.297748, acc: 0.972656]\n",
            "418: [Discriminator loss: 0.535598, acc: 0.503906]  [Adversarial loss: 0.271206, acc: 0.984375]\n",
            "419: [Discriminator loss: 0.533428, acc: 0.507812]  [Adversarial loss: 0.278188, acc: 0.984375]\n",
            "420: [Discriminator loss: 0.479813, acc: 0.574219]  [Adversarial loss: 0.245515, acc: 0.984375]\n",
            "421: [Discriminator loss: 0.541078, acc: 0.507812]  [Adversarial loss: 0.269008, acc: 0.968750]\n",
            "422: [Discriminator loss: 0.574352, acc: 0.472656]  [Adversarial loss: 0.282685, acc: 0.972656]\n",
            "423: [Discriminator loss: 0.567869, acc: 0.496094]  [Adversarial loss: 0.277808, acc: 0.976562]\n",
            "424: [Discriminator loss: 0.535422, acc: 0.492188]  [Adversarial loss: 0.290830, acc: 0.984375]\n",
            "425: [Discriminator loss: 0.464972, acc: 0.566406]  [Adversarial loss: 0.260342, acc: 0.953125]\n",
            "426: [Discriminator loss: 0.553835, acc: 0.507812]  [Adversarial loss: 0.290426, acc: 0.980469]\n",
            "427: [Discriminator loss: 0.492973, acc: 0.546875]  [Adversarial loss: 0.238200, acc: 0.984375]\n",
            "428: [Discriminator loss: 0.605036, acc: 0.437500]  [Adversarial loss: 0.313239, acc: 0.984375]\n",
            "429: [Discriminator loss: 0.566867, acc: 0.480469]  [Adversarial loss: 0.278495, acc: 0.964844]\n",
            "430: [Discriminator loss: 0.533196, acc: 0.523438]  [Adversarial loss: 0.257991, acc: 0.984375]\n",
            "431: [Discriminator loss: 0.532285, acc: 0.550781]  [Adversarial loss: 0.254549, acc: 0.960938]\n",
            "432: [Discriminator loss: 0.603988, acc: 0.468750]  [Adversarial loss: 0.292500, acc: 0.984375]\n",
            "433: [Discriminator loss: 0.523467, acc: 0.531250]  [Adversarial loss: 0.262214, acc: 0.972656]\n",
            "434: [Discriminator loss: 0.546634, acc: 0.515625]  [Adversarial loss: 0.285906, acc: 0.976562]\n",
            "435: [Discriminator loss: 0.547809, acc: 0.562500]  [Adversarial loss: 0.238402, acc: 0.992188]\n",
            "436: [Discriminator loss: 0.538109, acc: 0.496094]  [Adversarial loss: 0.304062, acc: 0.968750]\n",
            "437: [Discriminator loss: 0.528133, acc: 0.500000]  [Adversarial loss: 0.294115, acc: 0.968750]\n",
            "438: [Discriminator loss: 0.561977, acc: 0.492188]  [Adversarial loss: 0.287866, acc: 0.976562]\n",
            "439: [Discriminator loss: 0.534557, acc: 0.515625]  [Adversarial loss: 0.276663, acc: 0.968750]\n",
            "440: [Discriminator loss: 0.561688, acc: 0.472656]  [Adversarial loss: 0.279879, acc: 0.988281]\n",
            "441: [Discriminator loss: 0.523135, acc: 0.507812]  [Adversarial loss: 0.301718, acc: 0.945312]\n",
            "442: [Discriminator loss: 0.524497, acc: 0.527344]  [Adversarial loss: 0.268141, acc: 0.984375]\n",
            "443: [Discriminator loss: 0.531931, acc: 0.515625]  [Adversarial loss: 0.273475, acc: 0.960938]\n",
            "444: [Discriminator loss: 0.528448, acc: 0.500000]  [Adversarial loss: 0.271601, acc: 0.988281]\n",
            "445: [Discriminator loss: 0.513631, acc: 0.542969]  [Adversarial loss: 0.247999, acc: 0.976562]\n",
            "446: [Discriminator loss: 0.529868, acc: 0.523438]  [Adversarial loss: 0.276547, acc: 0.960938]\n",
            "447: [Discriminator loss: 0.551998, acc: 0.539062]  [Adversarial loss: 0.254820, acc: 0.980469]\n",
            "448: [Discriminator loss: 0.605367, acc: 0.488281]  [Adversarial loss: 0.289757, acc: 0.972656]\n",
            "449: [Discriminator loss: 0.553459, acc: 0.500000]  [Adversarial loss: 0.293329, acc: 0.984375]\n",
            "450: [Discriminator loss: 0.527361, acc: 0.531250]  [Adversarial loss: 0.264798, acc: 0.972656]\n",
            "451: [Discriminator loss: 0.559301, acc: 0.480469]  [Adversarial loss: 0.290331, acc: 0.984375]\n",
            "452: [Discriminator loss: 0.559323, acc: 0.492188]  [Adversarial loss: 0.271430, acc: 0.992188]\n",
            "453: [Discriminator loss: 0.519364, acc: 0.531250]  [Adversarial loss: 0.269785, acc: 0.972656]\n",
            "454: [Discriminator loss: 0.623128, acc: 0.484375]  [Adversarial loss: 0.300496, acc: 0.941406]\n",
            "455: [Discriminator loss: 0.588358, acc: 0.523438]  [Adversarial loss: 0.269058, acc: 0.964844]\n",
            "456: [Discriminator loss: 0.500728, acc: 0.531250]  [Adversarial loss: 0.264221, acc: 0.980469]\n",
            "457: [Discriminator loss: 0.596946, acc: 0.449219]  [Adversarial loss: 0.319047, acc: 0.972656]\n",
            "458: [Discriminator loss: 0.525130, acc: 0.542969]  [Adversarial loss: 0.270322, acc: 0.964844]\n",
            "459: [Discriminator loss: 0.517173, acc: 0.527344]  [Adversarial loss: 0.263392, acc: 0.980469]\n",
            "460: [Discriminator loss: 0.533065, acc: 0.519531]  [Adversarial loss: 0.274485, acc: 0.957031]\n",
            "461: [Discriminator loss: 0.536492, acc: 0.500000]  [Adversarial loss: 0.294886, acc: 0.976562]\n",
            "462: [Discriminator loss: 0.524002, acc: 0.507812]  [Adversarial loss: 0.271283, acc: 0.972656]\n",
            "463: [Discriminator loss: 0.555118, acc: 0.492188]  [Adversarial loss: 0.316620, acc: 0.964844]\n",
            "464: [Discriminator loss: 0.555561, acc: 0.457031]  [Adversarial loss: 0.301853, acc: 0.976562]\n",
            "465: [Discriminator loss: 0.501119, acc: 0.542969]  [Adversarial loss: 0.296996, acc: 0.953125]\n",
            "466: [Discriminator loss: 0.562080, acc: 0.535156]  [Adversarial loss: 0.274092, acc: 0.972656]\n",
            "467: [Discriminator loss: 0.625623, acc: 0.476562]  [Adversarial loss: 0.309532, acc: 0.976562]\n",
            "468: [Discriminator loss: 0.586863, acc: 0.500000]  [Adversarial loss: 0.294452, acc: 0.976562]\n",
            "469: [Discriminator loss: 0.577062, acc: 0.460938]  [Adversarial loss: 0.335878, acc: 0.988281]\n",
            "470: [Discriminator loss: 0.490293, acc: 0.519531]  [Adversarial loss: 0.280684, acc: 0.984375]\n",
            "471: [Discriminator loss: 0.581740, acc: 0.519531]  [Adversarial loss: 0.253321, acc: 0.984375]\n",
            "472: [Discriminator loss: 0.516189, acc: 0.546875]  [Adversarial loss: 0.255142, acc: 0.964844]\n",
            "473: [Discriminator loss: 0.508486, acc: 0.546875]  [Adversarial loss: 0.259587, acc: 0.976562]\n",
            "474: [Discriminator loss: 0.541497, acc: 0.515625]  [Adversarial loss: 0.281790, acc: 0.980469]\n",
            "475: [Discriminator loss: 0.542607, acc: 0.511719]  [Adversarial loss: 0.290841, acc: 0.937500]\n",
            "476: [Discriminator loss: 0.537529, acc: 0.507812]  [Adversarial loss: 0.300667, acc: 0.957031]\n",
            "477: [Discriminator loss: 0.537489, acc: 0.472656]  [Adversarial loss: 0.313205, acc: 0.953125]\n",
            "478: [Discriminator loss: 0.593945, acc: 0.433594]  [Adversarial loss: 0.340846, acc: 0.964844]\n",
            "479: [Discriminator loss: 0.567976, acc: 0.480469]  [Adversarial loss: 0.310210, acc: 0.964844]\n",
            "480: [Discriminator loss: 0.527160, acc: 0.546875]  [Adversarial loss: 0.263907, acc: 0.964844]\n",
            "481: [Discriminator loss: 0.580225, acc: 0.484375]  [Adversarial loss: 0.318620, acc: 0.960938]\n",
            "482: [Discriminator loss: 0.608777, acc: 0.433594]  [Adversarial loss: 0.331496, acc: 0.976562]\n",
            "483: [Discriminator loss: 0.553009, acc: 0.519531]  [Adversarial loss: 0.292269, acc: 0.941406]\n",
            "484: [Discriminator loss: 0.523571, acc: 0.507812]  [Adversarial loss: 0.279505, acc: 0.960938]\n",
            "485: [Discriminator loss: 0.531028, acc: 0.500000]  [Adversarial loss: 0.290616, acc: 0.964844]\n",
            "486: [Discriminator loss: 0.545961, acc: 0.503906]  [Adversarial loss: 0.309914, acc: 0.972656]\n",
            "487: [Discriminator loss: 0.561530, acc: 0.500000]  [Adversarial loss: 0.312562, acc: 0.957031]\n",
            "488: [Discriminator loss: 0.469214, acc: 0.578125]  [Adversarial loss: 0.241761, acc: 0.968750]\n",
            "489: [Discriminator loss: 0.529380, acc: 0.507812]  [Adversarial loss: 0.289873, acc: 0.957031]\n",
            "490: [Discriminator loss: 0.506087, acc: 0.519531]  [Adversarial loss: 0.282088, acc: 0.960938]\n",
            "491: [Discriminator loss: 0.543665, acc: 0.527344]  [Adversarial loss: 0.264580, acc: 0.980469]\n",
            "492: [Discriminator loss: 0.531651, acc: 0.535156]  [Adversarial loss: 0.274854, acc: 0.972656]\n",
            "493: [Discriminator loss: 0.568992, acc: 0.480469]  [Adversarial loss: 0.294331, acc: 0.968750]\n",
            "494: [Discriminator loss: 0.560475, acc: 0.511719]  [Adversarial loss: 0.262080, acc: 0.988281]\n",
            "495: [Discriminator loss: 0.518244, acc: 0.507812]  [Adversarial loss: 0.302356, acc: 0.968750]\n",
            "496: [Discriminator loss: 0.541613, acc: 0.488281]  [Adversarial loss: 0.306842, acc: 0.964844]\n",
            "497: [Discriminator loss: 0.590521, acc: 0.519531]  [Adversarial loss: 0.270312, acc: 0.972656]\n",
            "498: [Discriminator loss: 0.602964, acc: 0.445312]  [Adversarial loss: 0.308345, acc: 0.972656]\n",
            "499: [Discriminator loss: 0.540825, acc: 0.484375]  [Adversarial loss: 0.288214, acc: 0.976562]\n",
            "500: [Discriminator loss: 0.474647, acc: 0.539062]  [Adversarial loss: 0.269942, acc: 0.964844]\n",
            "501: [Discriminator loss: 0.476442, acc: 0.550781]  [Adversarial loss: 0.264480, acc: 0.960938]\n",
            "502: [Discriminator loss: 0.544385, acc: 0.511719]  [Adversarial loss: 0.289313, acc: 0.960938]\n",
            "503: [Discriminator loss: 0.589341, acc: 0.445312]  [Adversarial loss: 0.316917, acc: 0.953125]\n",
            "504: [Discriminator loss: 0.517819, acc: 0.500000]  [Adversarial loss: 0.269168, acc: 0.984375]\n",
            "505: [Discriminator loss: 0.448455, acc: 0.613281]  [Adversarial loss: 0.214526, acc: 0.972656]\n",
            "506: [Discriminator loss: 0.520117, acc: 0.519531]  [Adversarial loss: 0.261021, acc: 0.957031]\n",
            "507: [Discriminator loss: 0.497153, acc: 0.535156]  [Adversarial loss: 0.260424, acc: 0.976562]\n",
            "508: [Discriminator loss: 0.596574, acc: 0.460938]  [Adversarial loss: 0.287346, acc: 0.968750]\n",
            "509: [Discriminator loss: 0.505251, acc: 0.531250]  [Adversarial loss: 0.251298, acc: 0.980469]\n",
            "510: [Discriminator loss: 0.532318, acc: 0.511719]  [Adversarial loss: 0.272563, acc: 0.968750]\n",
            "511: [Discriminator loss: 0.521718, acc: 0.503906]  [Adversarial loss: 0.292556, acc: 0.968750]\n",
            "512: [Discriminator loss: 0.594411, acc: 0.480469]  [Adversarial loss: 0.295806, acc: 0.968750]\n",
            "513: [Discriminator loss: 0.577468, acc: 0.453125]  [Adversarial loss: 0.305940, acc: 0.968750]\n",
            "514: [Discriminator loss: 0.577145, acc: 0.511719]  [Adversarial loss: 0.265109, acc: 0.976562]\n",
            "515: [Discriminator loss: 0.552634, acc: 0.507812]  [Adversarial loss: 0.270939, acc: 0.964844]\n",
            "516: [Discriminator loss: 0.565826, acc: 0.480469]  [Adversarial loss: 0.289779, acc: 0.980469]\n",
            "517: [Discriminator loss: 0.493353, acc: 0.546875]  [Adversarial loss: 0.258708, acc: 0.960938]\n",
            "518: [Discriminator loss: 0.585064, acc: 0.441406]  [Adversarial loss: 0.317131, acc: 0.980469]\n",
            "519: [Discriminator loss: 0.547465, acc: 0.484375]  [Adversarial loss: 0.301261, acc: 0.976562]\n",
            "520: [Discriminator loss: 0.560085, acc: 0.515625]  [Adversarial loss: 0.270694, acc: 0.968750]\n",
            "521: [Discriminator loss: 0.610506, acc: 0.464844]  [Adversarial loss: 0.284047, acc: 0.960938]\n",
            "522: [Discriminator loss: 0.620528, acc: 0.488281]  [Adversarial loss: 0.280928, acc: 0.976562]\n",
            "523: [Discriminator loss: 0.529742, acc: 0.515625]  [Adversarial loss: 0.272826, acc: 0.984375]\n",
            "524: [Discriminator loss: 0.579667, acc: 0.472656]  [Adversarial loss: 0.310744, acc: 0.980469]\n",
            "525: [Discriminator loss: 0.553807, acc: 0.480469]  [Adversarial loss: 0.271846, acc: 0.992188]\n",
            "526: [Discriminator loss: 0.537316, acc: 0.519531]  [Adversarial loss: 0.261604, acc: 0.968750]\n",
            "527: [Discriminator loss: 0.565013, acc: 0.484375]  [Adversarial loss: 0.281509, acc: 0.984375]\n",
            "528: [Discriminator loss: 0.574722, acc: 0.484375]  [Adversarial loss: 0.332104, acc: 0.953125]\n",
            "529: [Discriminator loss: 0.527949, acc: 0.539062]  [Adversarial loss: 0.290215, acc: 0.980469]\n",
            "530: [Discriminator loss: 0.584269, acc: 0.496094]  [Adversarial loss: 0.282477, acc: 0.972656]\n",
            "531: [Discriminator loss: 0.569306, acc: 0.507812]  [Adversarial loss: 0.285070, acc: 0.980469]\n",
            "532: [Discriminator loss: 0.531754, acc: 0.519531]  [Adversarial loss: 0.279015, acc: 0.968750]\n",
            "533: [Discriminator loss: 0.587585, acc: 0.460938]  [Adversarial loss: 0.300235, acc: 0.984375]\n",
            "534: [Discriminator loss: 0.561062, acc: 0.507812]  [Adversarial loss: 0.269696, acc: 0.972656]\n",
            "535: [Discriminator loss: 0.606706, acc: 0.472656]  [Adversarial loss: 0.284472, acc: 0.980469]\n",
            "536: [Discriminator loss: 0.579026, acc: 0.449219]  [Adversarial loss: 0.308350, acc: 0.968750]\n",
            "537: [Discriminator loss: 0.517256, acc: 0.492188]  [Adversarial loss: 0.284246, acc: 0.968750]\n",
            "538: [Discriminator loss: 0.510282, acc: 0.519531]  [Adversarial loss: 0.330824, acc: 0.964844]\n",
            "539: [Discriminator loss: 0.522583, acc: 0.507812]  [Adversarial loss: 0.282086, acc: 0.976562]\n",
            "540: [Discriminator loss: 0.601650, acc: 0.457031]  [Adversarial loss: 0.318937, acc: 0.976562]\n",
            "541: [Discriminator loss: 0.529004, acc: 0.503906]  [Adversarial loss: 0.274610, acc: 0.976562]\n",
            "542: [Discriminator loss: 0.519064, acc: 0.519531]  [Adversarial loss: 0.274103, acc: 0.980469]\n",
            "543: [Discriminator loss: 0.577265, acc: 0.511719]  [Adversarial loss: 0.278278, acc: 0.972656]\n",
            "544: [Discriminator loss: 0.549147, acc: 0.507812]  [Adversarial loss: 0.275904, acc: 0.980469]\n",
            "545: [Discriminator loss: 0.550788, acc: 0.511719]  [Adversarial loss: 0.257379, acc: 0.988281]\n",
            "546: [Discriminator loss: 0.564233, acc: 0.480469]  [Adversarial loss: 0.289772, acc: 0.988281]\n",
            "547: [Discriminator loss: 0.559151, acc: 0.472656]  [Adversarial loss: 0.295338, acc: 0.980469]\n",
            "548: [Discriminator loss: 0.475705, acc: 0.535156]  [Adversarial loss: 0.278445, acc: 0.984375]\n",
            "549: [Discriminator loss: 0.493101, acc: 0.539062]  [Adversarial loss: 0.280290, acc: 0.968750]\n",
            "550: [Discriminator loss: 0.573856, acc: 0.480469]  [Adversarial loss: 0.290494, acc: 0.980469]\n",
            "551: [Discriminator loss: 0.529839, acc: 0.527344]  [Adversarial loss: 0.269043, acc: 0.968750]\n",
            "552: [Discriminator loss: 0.514683, acc: 0.492188]  [Adversarial loss: 0.303488, acc: 0.968750]\n",
            "553: [Discriminator loss: 0.538205, acc: 0.511719]  [Adversarial loss: 0.278301, acc: 0.968750]\n",
            "554: [Discriminator loss: 0.521957, acc: 0.531250]  [Adversarial loss: 0.275849, acc: 0.972656]\n",
            "555: [Discriminator loss: 0.549160, acc: 0.492188]  [Adversarial loss: 0.287040, acc: 0.972656]\n",
            "556: [Discriminator loss: 0.556005, acc: 0.503906]  [Adversarial loss: 0.269601, acc: 0.988281]\n",
            "557: [Discriminator loss: 0.582565, acc: 0.429688]  [Adversarial loss: 0.322738, acc: 0.992188]\n",
            "558: [Discriminator loss: 0.506136, acc: 0.542969]  [Adversarial loss: 0.268566, acc: 0.968750]\n",
            "559: [Discriminator loss: 0.591267, acc: 0.484375]  [Adversarial loss: 0.327658, acc: 0.964844]\n",
            "560: [Discriminator loss: 0.473111, acc: 0.554688]  [Adversarial loss: 0.277437, acc: 0.964844]\n",
            "561: [Discriminator loss: 0.559045, acc: 0.492188]  [Adversarial loss: 0.310044, acc: 0.957031]\n",
            "562: [Discriminator loss: 0.559689, acc: 0.488281]  [Adversarial loss: 0.272898, acc: 0.980469]\n",
            "563: [Discriminator loss: 0.575339, acc: 0.488281]  [Adversarial loss: 0.290065, acc: 0.957031]\n",
            "564: [Discriminator loss: 0.505235, acc: 0.511719]  [Adversarial loss: 0.306578, acc: 0.976562]\n",
            "565: [Discriminator loss: 0.501399, acc: 0.527344]  [Adversarial loss: 0.252648, acc: 0.984375]\n",
            "566: [Discriminator loss: 0.522566, acc: 0.523438]  [Adversarial loss: 0.275021, acc: 0.957031]\n",
            "567: [Discriminator loss: 0.559337, acc: 0.484375]  [Adversarial loss: 0.295100, acc: 0.980469]\n",
            "568: [Discriminator loss: 0.523988, acc: 0.496094]  [Adversarial loss: 0.290124, acc: 0.968750]\n",
            "569: [Discriminator loss: 0.505439, acc: 0.515625]  [Adversarial loss: 0.270599, acc: 0.968750]\n",
            "570: [Discriminator loss: 0.574023, acc: 0.503906]  [Adversarial loss: 0.279120, acc: 0.988281]\n",
            "571: [Discriminator loss: 0.600782, acc: 0.460938]  [Adversarial loss: 0.297325, acc: 0.976562]\n",
            "572: [Discriminator loss: 0.508468, acc: 0.511719]  [Adversarial loss: 0.278701, acc: 0.968750]\n",
            "573: [Discriminator loss: 0.561300, acc: 0.496094]  [Adversarial loss: 0.288859, acc: 0.968750]\n",
            "574: [Discriminator loss: 0.574931, acc: 0.464844]  [Adversarial loss: 0.308712, acc: 0.960938]\n",
            "575: [Discriminator loss: 0.543659, acc: 0.546875]  [Adversarial loss: 0.255369, acc: 0.964844]\n",
            "576: [Discriminator loss: 0.463917, acc: 0.535156]  [Adversarial loss: 0.273081, acc: 0.984375]\n",
            "577: [Discriminator loss: 0.510669, acc: 0.527344]  [Adversarial loss: 0.261456, acc: 0.972656]\n",
            "578: [Discriminator loss: 0.435535, acc: 0.585938]  [Adversarial loss: 0.245176, acc: 0.984375]\n",
            "579: [Discriminator loss: 0.570027, acc: 0.511719]  [Adversarial loss: 0.294691, acc: 0.960938]\n",
            "580: [Discriminator loss: 0.510009, acc: 0.550781]  [Adversarial loss: 0.246214, acc: 0.960938]\n",
            "581: [Discriminator loss: 0.555359, acc: 0.503906]  [Adversarial loss: 0.288354, acc: 0.964844]\n",
            "582: [Discriminator loss: 0.586221, acc: 0.464844]  [Adversarial loss: 0.294451, acc: 0.980469]\n",
            "583: [Discriminator loss: 0.622171, acc: 0.441406]  [Adversarial loss: 0.292621, acc: 0.984375]\n",
            "584: [Discriminator loss: 0.533695, acc: 0.507812]  [Adversarial loss: 0.274718, acc: 0.972656]\n",
            "585: [Discriminator loss: 0.547913, acc: 0.472656]  [Adversarial loss: 0.305665, acc: 0.968750]\n",
            "586: [Discriminator loss: 0.531557, acc: 0.539062]  [Adversarial loss: 0.254869, acc: 0.980469]\n",
            "587: [Discriminator loss: 0.517418, acc: 0.507812]  [Adversarial loss: 0.293779, acc: 0.957031]\n",
            "588: [Discriminator loss: 0.532739, acc: 0.492188]  [Adversarial loss: 0.293827, acc: 0.968750]\n",
            "589: [Discriminator loss: 0.511379, acc: 0.527344]  [Adversarial loss: 0.283080, acc: 0.964844]\n",
            "590: [Discriminator loss: 0.530929, acc: 0.515625]  [Adversarial loss: 0.265504, acc: 0.984375]\n",
            "591: [Discriminator loss: 0.504602, acc: 0.511719]  [Adversarial loss: 0.284127, acc: 0.968750]\n",
            "592: [Discriminator loss: 0.481538, acc: 0.550781]  [Adversarial loss: 0.238074, acc: 0.988281]\n",
            "593: [Discriminator loss: 0.470698, acc: 0.554688]  [Adversarial loss: 0.226214, acc: 0.984375]\n",
            "594: [Discriminator loss: 0.490520, acc: 0.550781]  [Adversarial loss: 0.246379, acc: 0.992188]\n",
            "595: [Discriminator loss: 0.618294, acc: 0.464844]  [Adversarial loss: 0.284804, acc: 0.992188]\n",
            "596: [Discriminator loss: 0.588681, acc: 0.484375]  [Adversarial loss: 0.288047, acc: 0.988281]\n",
            "597: [Discriminator loss: 0.487939, acc: 0.554688]  [Adversarial loss: 0.259576, acc: 0.972656]\n",
            "598: [Discriminator loss: 0.510846, acc: 0.539062]  [Adversarial loss: 0.238614, acc: 0.988281]\n",
            "599: [Discriminator loss: 0.518106, acc: 0.515625]  [Adversarial loss: 0.265936, acc: 0.972656]\n",
            "600: [Discriminator loss: 0.611363, acc: 0.472656]  [Adversarial loss: 0.269156, acc: 0.976562]\n",
            "601: [Discriminator loss: 0.544331, acc: 0.519531]  [Adversarial loss: 0.265168, acc: 0.984375]\n",
            "602: [Discriminator loss: 0.555774, acc: 0.484375]  [Adversarial loss: 0.288162, acc: 0.980469]\n",
            "603: [Discriminator loss: 0.504200, acc: 0.542969]  [Adversarial loss: 0.256274, acc: 0.980469]\n",
            "604: [Discriminator loss: 0.568601, acc: 0.500000]  [Adversarial loss: 0.273582, acc: 0.972656]\n",
            "605: [Discriminator loss: 0.515227, acc: 0.523438]  [Adversarial loss: 0.265380, acc: 0.976562]\n",
            "606: [Discriminator loss: 0.574825, acc: 0.500000]  [Adversarial loss: 0.280586, acc: 0.960938]\n",
            "607: [Discriminator loss: 0.534657, acc: 0.515625]  [Adversarial loss: 0.262544, acc: 0.972656]\n",
            "608: [Discriminator loss: 0.524869, acc: 0.531250]  [Adversarial loss: 0.257495, acc: 0.984375]\n",
            "609: [Discriminator loss: 0.509109, acc: 0.519531]  [Adversarial loss: 0.271089, acc: 0.968750]\n",
            "610: [Discriminator loss: 0.589686, acc: 0.460938]  [Adversarial loss: 0.293620, acc: 0.980469]\n",
            "611: [Discriminator loss: 0.606841, acc: 0.480469]  [Adversarial loss: 0.289010, acc: 0.984375]\n",
            "612: [Discriminator loss: 0.509988, acc: 0.566406]  [Adversarial loss: 0.239996, acc: 0.976562]\n",
            "613: [Discriminator loss: 0.578460, acc: 0.500000]  [Adversarial loss: 0.283739, acc: 0.964844]\n",
            "614: [Discriminator loss: 0.523468, acc: 0.546875]  [Adversarial loss: 0.256190, acc: 0.980469]\n",
            "615: [Discriminator loss: 0.559775, acc: 0.527344]  [Adversarial loss: 0.257587, acc: 0.980469]\n",
            "616: [Discriminator loss: 0.501819, acc: 0.539062]  [Adversarial loss: 0.256252, acc: 0.980469]\n",
            "617: [Discriminator loss: 0.555410, acc: 0.492188]  [Adversarial loss: 0.282021, acc: 0.972656]\n",
            "618: [Discriminator loss: 0.553361, acc: 0.507812]  [Adversarial loss: 0.267934, acc: 0.984375]\n",
            "619: [Discriminator loss: 0.488708, acc: 0.539062]  [Adversarial loss: 0.256490, acc: 0.968750]\n",
            "620: [Discriminator loss: 0.514894, acc: 0.500000]  [Adversarial loss: 0.265680, acc: 0.988281]\n",
            "621: [Discriminator loss: 0.526816, acc: 0.488281]  [Adversarial loss: 0.298247, acc: 0.953125]\n",
            "622: [Discriminator loss: 0.562645, acc: 0.492188]  [Adversarial loss: 0.301831, acc: 0.953125]\n",
            "623: [Discriminator loss: 0.599387, acc: 0.492188]  [Adversarial loss: 0.276154, acc: 0.988281]\n",
            "624: [Discriminator loss: 0.561703, acc: 0.468750]  [Adversarial loss: 0.292541, acc: 0.972656]\n",
            "625: [Discriminator loss: 0.518708, acc: 0.515625]  [Adversarial loss: 0.276230, acc: 0.957031]\n",
            "626: [Discriminator loss: 0.578408, acc: 0.464844]  [Adversarial loss: 0.289339, acc: 0.980469]\n",
            "627: [Discriminator loss: 0.470344, acc: 0.550781]  [Adversarial loss: 0.251927, acc: 0.972656]\n",
            "628: [Discriminator loss: 0.488053, acc: 0.558594]  [Adversarial loss: 0.249328, acc: 0.968750]\n",
            "629: [Discriminator loss: 0.524381, acc: 0.492188]  [Adversarial loss: 0.276769, acc: 0.984375]\n",
            "630: [Discriminator loss: 0.555939, acc: 0.500000]  [Adversarial loss: 0.301310, acc: 0.964844]\n",
            "631: [Discriminator loss: 0.589277, acc: 0.503906]  [Adversarial loss: 0.288498, acc: 0.953125]\n",
            "632: [Discriminator loss: 0.482855, acc: 0.554688]  [Adversarial loss: 0.248486, acc: 0.980469]\n",
            "633: [Discriminator loss: 0.538638, acc: 0.492188]  [Adversarial loss: 0.285581, acc: 0.957031]\n",
            "634: [Discriminator loss: 0.550591, acc: 0.503906]  [Adversarial loss: 0.268705, acc: 0.976562]\n",
            "635: [Discriminator loss: 0.582360, acc: 0.472656]  [Adversarial loss: 0.297619, acc: 0.980469]\n",
            "636: [Discriminator loss: 0.525533, acc: 0.484375]  [Adversarial loss: 0.278431, acc: 0.972656]\n",
            "637: [Discriminator loss: 0.501781, acc: 0.515625]  [Adversarial loss: 0.280136, acc: 0.972656]\n",
            "638: [Discriminator loss: 0.526247, acc: 0.503906]  [Adversarial loss: 0.283283, acc: 0.964844]\n",
            "639: [Discriminator loss: 0.455087, acc: 0.597656]  [Adversarial loss: 0.226077, acc: 0.984375]\n",
            "640: [Discriminator loss: 0.618295, acc: 0.476562]  [Adversarial loss: 0.299514, acc: 0.964844]\n",
            "641: [Discriminator loss: 0.526736, acc: 0.515625]  [Adversarial loss: 0.259540, acc: 0.984375]\n",
            "642: [Discriminator loss: 0.536787, acc: 0.523438]  [Adversarial loss: 0.270834, acc: 0.988281]\n",
            "643: [Discriminator loss: 0.610805, acc: 0.488281]  [Adversarial loss: 0.292828, acc: 0.968750]\n",
            "644: [Discriminator loss: 0.617278, acc: 0.484375]  [Adversarial loss: 0.284991, acc: 0.976562]\n",
            "645: [Discriminator loss: 0.582257, acc: 0.457031]  [Adversarial loss: 0.308381, acc: 0.972656]\n",
            "646: [Discriminator loss: 0.558187, acc: 0.480469]  [Adversarial loss: 0.277955, acc: 0.968750]\n",
            "647: [Discriminator loss: 0.498636, acc: 0.527344]  [Adversarial loss: 0.279200, acc: 0.960938]\n",
            "648: [Discriminator loss: 0.516354, acc: 0.500000]  [Adversarial loss: 0.300355, acc: 0.968750]\n",
            "649: [Discriminator loss: 0.536620, acc: 0.484375]  [Adversarial loss: 0.271805, acc: 0.976562]\n",
            "650: [Discriminator loss: 0.548092, acc: 0.539062]  [Adversarial loss: 0.265143, acc: 0.964844]\n",
            "651: [Discriminator loss: 0.481933, acc: 0.539062]  [Adversarial loss: 0.258604, acc: 0.988281]\n",
            "652: [Discriminator loss: 0.470399, acc: 0.570312]  [Adversarial loss: 0.258872, acc: 0.972656]\n",
            "653: [Discriminator loss: 0.490578, acc: 0.539062]  [Adversarial loss: 0.253140, acc: 0.980469]\n",
            "654: [Discriminator loss: 0.504000, acc: 0.546875]  [Adversarial loss: 0.259781, acc: 0.968750]\n",
            "655: [Discriminator loss: 0.510903, acc: 0.554688]  [Adversarial loss: 0.255162, acc: 0.972656]\n",
            "656: [Discriminator loss: 0.496298, acc: 0.523438]  [Adversarial loss: 0.265490, acc: 0.980469]\n",
            "657: [Discriminator loss: 0.550719, acc: 0.480469]  [Adversarial loss: 0.283919, acc: 0.984375]\n",
            "658: [Discriminator loss: 0.570504, acc: 0.496094]  [Adversarial loss: 0.298378, acc: 0.960938]\n",
            "659: [Discriminator loss: 0.528320, acc: 0.523438]  [Adversarial loss: 0.254672, acc: 0.980469]\n",
            "660: [Discriminator loss: 0.559459, acc: 0.496094]  [Adversarial loss: 0.290269, acc: 0.984375]\n",
            "661: [Discriminator loss: 0.503768, acc: 0.542969]  [Adversarial loss: 0.254900, acc: 0.972656]\n",
            "662: [Discriminator loss: 0.524973, acc: 0.531250]  [Adversarial loss: 0.273687, acc: 0.960938]\n",
            "663: [Discriminator loss: 0.578401, acc: 0.515625]  [Adversarial loss: 0.287196, acc: 0.957031]\n",
            "664: [Discriminator loss: 0.487456, acc: 0.519531]  [Adversarial loss: 0.274782, acc: 0.976562]\n",
            "665: [Discriminator loss: 0.473594, acc: 0.566406]  [Adversarial loss: 0.257499, acc: 0.976562]\n",
            "666: [Discriminator loss: 0.484223, acc: 0.585938]  [Adversarial loss: 0.243841, acc: 0.964844]\n",
            "667: [Discriminator loss: 0.489864, acc: 0.597656]  [Adversarial loss: 0.223551, acc: 0.980469]\n",
            "668: [Discriminator loss: 0.609755, acc: 0.441406]  [Adversarial loss: 0.335474, acc: 0.960938]\n",
            "669: [Discriminator loss: 0.531961, acc: 0.507812]  [Adversarial loss: 0.277708, acc: 0.953125]\n",
            "670: [Discriminator loss: 0.494688, acc: 0.531250]  [Adversarial loss: 0.271214, acc: 0.972656]\n",
            "671: [Discriminator loss: 0.544148, acc: 0.515625]  [Adversarial loss: 0.278767, acc: 0.972656]\n",
            "672: [Discriminator loss: 0.536646, acc: 0.546875]  [Adversarial loss: 0.269718, acc: 0.964844]\n",
            "673: [Discriminator loss: 0.556814, acc: 0.476562]  [Adversarial loss: 0.291888, acc: 0.968750]\n",
            "674: [Discriminator loss: 0.584417, acc: 0.511719]  [Adversarial loss: 0.278852, acc: 0.964844]\n",
            "675: [Discriminator loss: 0.555822, acc: 0.527344]  [Adversarial loss: 0.284530, acc: 0.972656]\n",
            "676: [Discriminator loss: 0.501113, acc: 0.542969]  [Adversarial loss: 0.267115, acc: 0.960938]\n",
            "677: [Discriminator loss: 0.522413, acc: 0.507812]  [Adversarial loss: 0.271569, acc: 0.972656]\n",
            "678: [Discriminator loss: 0.611402, acc: 0.476562]  [Adversarial loss: 0.286116, acc: 0.988281]\n",
            "679: [Discriminator loss: 0.529231, acc: 0.527344]  [Adversarial loss: 0.276951, acc: 0.972656]\n",
            "680: [Discriminator loss: 0.574436, acc: 0.503906]  [Adversarial loss: 0.282144, acc: 0.960938]\n",
            "681: [Discriminator loss: 0.547278, acc: 0.542969]  [Adversarial loss: 0.264251, acc: 0.968750]\n",
            "682: [Discriminator loss: 0.530335, acc: 0.539062]  [Adversarial loss: 0.258281, acc: 0.957031]\n",
            "683: [Discriminator loss: 0.539181, acc: 0.519531]  [Adversarial loss: 0.266620, acc: 0.972656]\n",
            "684: [Discriminator loss: 0.563384, acc: 0.500000]  [Adversarial loss: 0.282279, acc: 0.964844]\n",
            "685: [Discriminator loss: 0.569684, acc: 0.453125]  [Adversarial loss: 0.311871, acc: 0.957031]\n",
            "686: [Discriminator loss: 0.507090, acc: 0.535156]  [Adversarial loss: 0.265278, acc: 0.960938]\n",
            "687: [Discriminator loss: 0.542815, acc: 0.488281]  [Adversarial loss: 0.283756, acc: 0.953125]\n",
            "688: [Discriminator loss: 0.560836, acc: 0.488281]  [Adversarial loss: 0.286900, acc: 0.972656]\n",
            "689: [Discriminator loss: 0.468585, acc: 0.585938]  [Adversarial loss: 0.249234, acc: 0.945312]\n",
            "690: [Discriminator loss: 0.506342, acc: 0.558594]  [Adversarial loss: 0.249080, acc: 0.960938]\n",
            "691: [Discriminator loss: 0.590999, acc: 0.460938]  [Adversarial loss: 0.309188, acc: 0.949219]\n",
            "692: [Discriminator loss: 0.530214, acc: 0.503906]  [Adversarial loss: 0.271811, acc: 0.957031]\n",
            "693: [Discriminator loss: 0.511443, acc: 0.523438]  [Adversarial loss: 0.289171, acc: 0.957031]\n",
            "694: [Discriminator loss: 0.576008, acc: 0.464844]  [Adversarial loss: 0.291201, acc: 0.984375]\n",
            "695: [Discriminator loss: 0.546994, acc: 0.484375]  [Adversarial loss: 0.294236, acc: 0.960938]\n",
            "696: [Discriminator loss: 0.526357, acc: 0.511719]  [Adversarial loss: 0.289211, acc: 0.949219]\n",
            "697: [Discriminator loss: 0.506415, acc: 0.562500]  [Adversarial loss: 0.246344, acc: 0.960938]\n",
            "698: [Discriminator loss: 0.519986, acc: 0.523438]  [Adversarial loss: 0.293443, acc: 0.957031]\n",
            "699: [Discriminator loss: 0.501662, acc: 0.558594]  [Adversarial loss: 0.254436, acc: 0.972656]\n",
            "700: [Discriminator loss: 0.534808, acc: 0.546875]  [Adversarial loss: 0.285006, acc: 0.960938]\n",
            "701: [Discriminator loss: 0.523558, acc: 0.542969]  [Adversarial loss: 0.267077, acc: 0.968750]\n",
            "702: [Discriminator loss: 0.586247, acc: 0.500000]  [Adversarial loss: 0.285538, acc: 0.964844]\n",
            "703: [Discriminator loss: 0.534248, acc: 0.496094]  [Adversarial loss: 0.286750, acc: 0.960938]\n",
            "704: [Discriminator loss: 0.471118, acc: 0.562500]  [Adversarial loss: 0.240107, acc: 0.976562]\n",
            "705: [Discriminator loss: 0.554659, acc: 0.468750]  [Adversarial loss: 0.296357, acc: 0.968750]\n",
            "706: [Discriminator loss: 0.522097, acc: 0.523438]  [Adversarial loss: 0.273849, acc: 0.968750]\n",
            "707: [Discriminator loss: 0.539940, acc: 0.496094]  [Adversarial loss: 0.279621, acc: 0.964844]\n",
            "708: [Discriminator loss: 0.571924, acc: 0.515625]  [Adversarial loss: 0.277848, acc: 0.949219]\n",
            "709: [Discriminator loss: 0.512387, acc: 0.527344]  [Adversarial loss: 0.267029, acc: 0.972656]\n",
            "710: [Discriminator loss: 0.544720, acc: 0.476562]  [Adversarial loss: 0.291500, acc: 0.957031]\n",
            "711: [Discriminator loss: 0.589440, acc: 0.441406]  [Adversarial loss: 0.301712, acc: 0.980469]\n",
            "712: [Discriminator loss: 0.501605, acc: 0.531250]  [Adversarial loss: 0.253536, acc: 0.960938]\n",
            "713: [Discriminator loss: 0.576681, acc: 0.507812]  [Adversarial loss: 0.278958, acc: 0.949219]\n",
            "714: [Discriminator loss: 0.519053, acc: 0.507812]  [Adversarial loss: 0.278499, acc: 0.949219]\n",
            "715: [Discriminator loss: 0.536853, acc: 0.539062]  [Adversarial loss: 0.270484, acc: 0.949219]\n",
            "716: [Discriminator loss: 0.546640, acc: 0.527344]  [Adversarial loss: 0.247691, acc: 0.972656]\n",
            "717: [Discriminator loss: 0.526562, acc: 0.515625]  [Adversarial loss: 0.275342, acc: 0.953125]\n",
            "718: [Discriminator loss: 0.553406, acc: 0.496094]  [Adversarial loss: 0.271592, acc: 0.976562]\n",
            "719: [Discriminator loss: 0.608970, acc: 0.496094]  [Adversarial loss: 0.276119, acc: 0.992188]\n",
            "720: [Discriminator loss: 0.538725, acc: 0.523438]  [Adversarial loss: 0.294647, acc: 0.964844]\n",
            "721: [Discriminator loss: 0.614810, acc: 0.507812]  [Adversarial loss: 0.264795, acc: 0.964844]\n",
            "722: [Discriminator loss: 0.588314, acc: 0.460938]  [Adversarial loss: 0.298158, acc: 0.980469]\n",
            "723: [Discriminator loss: 0.555078, acc: 0.492188]  [Adversarial loss: 0.295939, acc: 0.957031]\n",
            "724: [Discriminator loss: 0.520878, acc: 0.515625]  [Adversarial loss: 0.281525, acc: 0.960938]\n",
            "725: [Discriminator loss: 0.540929, acc: 0.535156]  [Adversarial loss: 0.276019, acc: 0.972656]\n",
            "726: [Discriminator loss: 0.546364, acc: 0.515625]  [Adversarial loss: 0.287143, acc: 0.964844]\n",
            "727: [Discriminator loss: 0.524190, acc: 0.527344]  [Adversarial loss: 0.286965, acc: 0.957031]\n",
            "728: [Discriminator loss: 0.538740, acc: 0.523438]  [Adversarial loss: 0.273894, acc: 0.972656]\n",
            "729: [Discriminator loss: 0.527992, acc: 0.527344]  [Adversarial loss: 0.279228, acc: 0.957031]\n",
            "730: [Discriminator loss: 0.520831, acc: 0.527344]  [Adversarial loss: 0.270743, acc: 0.964844]\n",
            "731: [Discriminator loss: 0.557298, acc: 0.503906]  [Adversarial loss: 0.264941, acc: 0.972656]\n",
            "732: [Discriminator loss: 0.504190, acc: 0.535156]  [Adversarial loss: 0.281816, acc: 0.968750]\n",
            "733: [Discriminator loss: 0.519953, acc: 0.511719]  [Adversarial loss: 0.255682, acc: 0.984375]\n",
            "734: [Discriminator loss: 0.594428, acc: 0.535156]  [Adversarial loss: 0.268321, acc: 0.949219]\n",
            "735: [Discriminator loss: 0.544856, acc: 0.550781]  [Adversarial loss: 0.247184, acc: 0.964844]\n",
            "736: [Discriminator loss: 0.552540, acc: 0.542969]  [Adversarial loss: 0.283870, acc: 0.953125]\n",
            "737: [Discriminator loss: 0.497437, acc: 0.519531]  [Adversarial loss: 0.264560, acc: 0.964844]\n",
            "738: [Discriminator loss: 0.644261, acc: 0.507812]  [Adversarial loss: 0.297780, acc: 0.941406]\n",
            "739: [Discriminator loss: 0.577970, acc: 0.472656]  [Adversarial loss: 0.325143, acc: 0.941406]\n",
            "740: [Discriminator loss: 0.461309, acc: 0.593750]  [Adversarial loss: 0.240165, acc: 0.980469]\n",
            "741: [Discriminator loss: 0.526622, acc: 0.539062]  [Adversarial loss: 0.279429, acc: 0.980469]\n",
            "742: [Discriminator loss: 0.555958, acc: 0.496094]  [Adversarial loss: 0.299117, acc: 0.960938]\n",
            "743: [Discriminator loss: 0.510333, acc: 0.554688]  [Adversarial loss: 0.277936, acc: 0.964844]\n",
            "744: [Discriminator loss: 0.524313, acc: 0.539062]  [Adversarial loss: 0.271738, acc: 0.949219]\n",
            "745: [Discriminator loss: 0.531881, acc: 0.550781]  [Adversarial loss: 0.244983, acc: 0.964844]\n",
            "746: [Discriminator loss: 0.575101, acc: 0.488281]  [Adversarial loss: 0.283469, acc: 0.960938]\n",
            "747: [Discriminator loss: 0.606662, acc: 0.480469]  [Adversarial loss: 0.287890, acc: 0.968750]\n",
            "748: [Discriminator loss: 0.515581, acc: 0.542969]  [Adversarial loss: 0.252480, acc: 0.957031]\n",
            "749: [Discriminator loss: 0.559056, acc: 0.507812]  [Adversarial loss: 0.276961, acc: 0.972656]\n",
            "750: [Discriminator loss: 0.579868, acc: 0.515625]  [Adversarial loss: 0.321247, acc: 0.949219]\n",
            "751: [Discriminator loss: 0.573928, acc: 0.507812]  [Adversarial loss: 0.287156, acc: 0.957031]\n",
            "752: [Discriminator loss: 0.485110, acc: 0.566406]  [Adversarial loss: 0.250790, acc: 0.960938]\n",
            "753: [Discriminator loss: 0.519208, acc: 0.554688]  [Adversarial loss: 0.279756, acc: 0.953125]\n",
            "754: [Discriminator loss: 0.612524, acc: 0.511719]  [Adversarial loss: 0.293826, acc: 0.941406]\n",
            "755: [Discriminator loss: 0.552952, acc: 0.562500]  [Adversarial loss: 0.253692, acc: 0.972656]\n",
            "756: [Discriminator loss: 0.553831, acc: 0.511719]  [Adversarial loss: 0.269745, acc: 0.984375]\n",
            "757: [Discriminator loss: 0.548546, acc: 0.546875]  [Adversarial loss: 0.262874, acc: 0.968750]\n",
            "758: [Discriminator loss: 0.549706, acc: 0.480469]  [Adversarial loss: 0.297696, acc: 0.949219]\n",
            "759: [Discriminator loss: 0.511550, acc: 0.531250]  [Adversarial loss: 0.270735, acc: 0.976562]\n",
            "760: [Discriminator loss: 0.550457, acc: 0.468750]  [Adversarial loss: 0.299967, acc: 0.941406]\n",
            "761: [Discriminator loss: 0.573674, acc: 0.503906]  [Adversarial loss: 0.301834, acc: 0.945312]\n",
            "762: [Discriminator loss: 0.500081, acc: 0.503906]  [Adversarial loss: 0.283683, acc: 0.964844]\n",
            "763: [Discriminator loss: 0.571155, acc: 0.503906]  [Adversarial loss: 0.299001, acc: 0.945312]\n",
            "764: [Discriminator loss: 0.552860, acc: 0.492188]  [Adversarial loss: 0.275088, acc: 0.980469]\n",
            "765: [Discriminator loss: 0.484446, acc: 0.550781]  [Adversarial loss: 0.271123, acc: 0.960938]\n",
            "766: [Discriminator loss: 0.538690, acc: 0.511719]  [Adversarial loss: 0.301744, acc: 0.910156]\n",
            "767: [Discriminator loss: 0.524009, acc: 0.507812]  [Adversarial loss: 0.274573, acc: 0.941406]\n",
            "768: [Discriminator loss: 0.530337, acc: 0.507812]  [Adversarial loss: 0.269944, acc: 0.968750]\n",
            "769: [Discriminator loss: 0.601269, acc: 0.492188]  [Adversarial loss: 0.285143, acc: 0.964844]\n",
            "770: [Discriminator loss: 0.537868, acc: 0.554688]  [Adversarial loss: 0.268328, acc: 0.933594]\n",
            "771: [Discriminator loss: 0.550891, acc: 0.515625]  [Adversarial loss: 0.294066, acc: 0.960938]\n",
            "772: [Discriminator loss: 0.558971, acc: 0.519531]  [Adversarial loss: 0.275624, acc: 0.945312]\n",
            "773: [Discriminator loss: 0.560586, acc: 0.503906]  [Adversarial loss: 0.277902, acc: 0.972656]\n",
            "774: [Discriminator loss: 0.588345, acc: 0.484375]  [Adversarial loss: 0.295296, acc: 0.964844]\n",
            "775: [Discriminator loss: 0.553904, acc: 0.554688]  [Adversarial loss: 0.251705, acc: 0.941406]\n",
            "776: [Discriminator loss: 0.550992, acc: 0.578125]  [Adversarial loss: 0.248993, acc: 0.937500]\n",
            "777: [Discriminator loss: 0.594115, acc: 0.511719]  [Adversarial loss: 0.310655, acc: 0.941406]\n",
            "778: [Discriminator loss: 0.493974, acc: 0.542969]  [Adversarial loss: 0.265591, acc: 0.972656]\n",
            "779: [Discriminator loss: 0.500743, acc: 0.578125]  [Adversarial loss: 0.246198, acc: 0.960938]\n",
            "780: [Discriminator loss: 0.507271, acc: 0.539062]  [Adversarial loss: 0.257603, acc: 0.960938]\n",
            "781: [Discriminator loss: 0.490386, acc: 0.554688]  [Adversarial loss: 0.277583, acc: 0.945312]\n",
            "782: [Discriminator loss: 0.526541, acc: 0.550781]  [Adversarial loss: 0.270887, acc: 0.941406]\n",
            "783: [Discriminator loss: 0.451101, acc: 0.582031]  [Adversarial loss: 0.244943, acc: 0.960938]\n",
            "784: [Discriminator loss: 0.590819, acc: 0.472656]  [Adversarial loss: 0.278774, acc: 0.968750]\n",
            "785: [Discriminator loss: 0.484343, acc: 0.558594]  [Adversarial loss: 0.242588, acc: 0.972656]\n",
            "786: [Discriminator loss: 0.509033, acc: 0.570312]  [Adversarial loss: 0.292819, acc: 0.925781]\n",
            "787: [Discriminator loss: 0.527117, acc: 0.558594]  [Adversarial loss: 0.258438, acc: 0.968750]\n",
            "788: [Discriminator loss: 0.593075, acc: 0.511719]  [Adversarial loss: 0.302455, acc: 0.968750]\n",
            "789: [Discriminator loss: 0.636298, acc: 0.531250]  [Adversarial loss: 0.268907, acc: 0.964844]\n",
            "790: [Discriminator loss: 0.606329, acc: 0.511719]  [Adversarial loss: 0.286248, acc: 0.953125]\n",
            "791: [Discriminator loss: 0.522045, acc: 0.531250]  [Adversarial loss: 0.289333, acc: 0.960938]\n",
            "792: [Discriminator loss: 0.541626, acc: 0.503906]  [Adversarial loss: 0.295180, acc: 0.937500]\n",
            "793: [Discriminator loss: 0.572008, acc: 0.511719]  [Adversarial loss: 0.276837, acc: 0.972656]\n",
            "794: [Discriminator loss: 0.559408, acc: 0.515625]  [Adversarial loss: 0.284566, acc: 0.949219]\n",
            "795: [Discriminator loss: 0.558111, acc: 0.500000]  [Adversarial loss: 0.290754, acc: 0.960938]\n",
            "796: [Discriminator loss: 0.537050, acc: 0.515625]  [Adversarial loss: 0.261136, acc: 0.980469]\n",
            "797: [Discriminator loss: 0.527613, acc: 0.539062]  [Adversarial loss: 0.269308, acc: 0.937500]\n",
            "798: [Discriminator loss: 0.538416, acc: 0.535156]  [Adversarial loss: 0.269307, acc: 0.957031]\n",
            "799: [Discriminator loss: 0.554008, acc: 0.523438]  [Adversarial loss: 0.272302, acc: 0.945312]\n",
            "800: [Discriminator loss: 0.545256, acc: 0.535156]  [Adversarial loss: 0.251795, acc: 0.968750]\n",
            "801: [Discriminator loss: 0.563463, acc: 0.531250]  [Adversarial loss: 0.269171, acc: 0.953125]\n",
            "802: [Discriminator loss: 0.511677, acc: 0.539062]  [Adversarial loss: 0.246046, acc: 0.960938]\n",
            "803: [Discriminator loss: 0.591200, acc: 0.511719]  [Adversarial loss: 0.255865, acc: 0.972656]\n",
            "804: [Discriminator loss: 0.591473, acc: 0.515625]  [Adversarial loss: 0.258557, acc: 0.968750]\n",
            "805: [Discriminator loss: 0.572540, acc: 0.558594]  [Adversarial loss: 0.236144, acc: 0.964844]\n",
            "806: [Discriminator loss: 0.612483, acc: 0.496094]  [Adversarial loss: 0.284298, acc: 0.945312]\n",
            "807: [Discriminator loss: 0.499600, acc: 0.539062]  [Adversarial loss: 0.257855, acc: 0.949219]\n",
            "808: [Discriminator loss: 0.588135, acc: 0.511719]  [Adversarial loss: 0.276662, acc: 0.945312]\n",
            "809: [Discriminator loss: 0.549452, acc: 0.507812]  [Adversarial loss: 0.267967, acc: 0.968750]\n",
            "810: [Discriminator loss: 0.469209, acc: 0.597656]  [Adversarial loss: 0.225767, acc: 0.976562]\n",
            "811: [Discriminator loss: 0.504552, acc: 0.562500]  [Adversarial loss: 0.265657, acc: 0.949219]\n",
            "812: [Discriminator loss: 0.515793, acc: 0.539062]  [Adversarial loss: 0.254280, acc: 0.972656]\n",
            "813: [Discriminator loss: 0.572053, acc: 0.539062]  [Adversarial loss: 0.264647, acc: 0.960938]\n",
            "814: [Discriminator loss: 0.610796, acc: 0.496094]  [Adversarial loss: 0.282485, acc: 0.949219]\n",
            "815: [Discriminator loss: 0.507739, acc: 0.550781]  [Adversarial loss: 0.246237, acc: 0.949219]\n",
            "816: [Discriminator loss: 0.560850, acc: 0.527344]  [Adversarial loss: 0.259756, acc: 0.960938]\n",
            "817: [Discriminator loss: 0.547980, acc: 0.515625]  [Adversarial loss: 0.272094, acc: 0.960938]\n",
            "818: [Discriminator loss: 0.499655, acc: 0.585938]  [Adversarial loss: 0.233190, acc: 0.972656]\n",
            "819: [Discriminator loss: 0.617607, acc: 0.488281]  [Adversarial loss: 0.291920, acc: 0.949219]\n",
            "820: [Discriminator loss: 0.578029, acc: 0.507812]  [Adversarial loss: 0.279325, acc: 0.953125]\n",
            "821: [Discriminator loss: 0.543026, acc: 0.539062]  [Adversarial loss: 0.264592, acc: 0.957031]\n",
            "822: [Discriminator loss: 0.536215, acc: 0.546875]  [Adversarial loss: 0.268261, acc: 0.953125]\n",
            "823: [Discriminator loss: 0.511246, acc: 0.574219]  [Adversarial loss: 0.241887, acc: 0.960938]\n",
            "824: [Discriminator loss: 0.479240, acc: 0.523438]  [Adversarial loss: 0.270551, acc: 0.953125]\n",
            "825: [Discriminator loss: 0.581623, acc: 0.503906]  [Adversarial loss: 0.293074, acc: 0.949219]\n",
            "826: [Discriminator loss: 0.542417, acc: 0.519531]  [Adversarial loss: 0.286778, acc: 0.980469]\n",
            "827: [Discriminator loss: 0.509438, acc: 0.554688]  [Adversarial loss: 0.282115, acc: 0.929688]\n",
            "828: [Discriminator loss: 0.533219, acc: 0.546875]  [Adversarial loss: 0.276043, acc: 0.949219]\n",
            "829: [Discriminator loss: 0.629144, acc: 0.496094]  [Adversarial loss: 0.279063, acc: 0.964844]\n",
            "830: [Discriminator loss: 0.506084, acc: 0.527344]  [Adversarial loss: 0.284733, acc: 0.937500]\n",
            "831: [Discriminator loss: 0.493779, acc: 0.546875]  [Adversarial loss: 0.272246, acc: 0.957031]\n",
            "832: [Discriminator loss: 0.549668, acc: 0.527344]  [Adversarial loss: 0.275207, acc: 0.945312]\n",
            "833: [Discriminator loss: 0.476756, acc: 0.578125]  [Adversarial loss: 0.264140, acc: 0.937500]\n",
            "834: [Discriminator loss: 0.584761, acc: 0.527344]  [Adversarial loss: 0.273182, acc: 0.960938]\n",
            "835: [Discriminator loss: 0.496354, acc: 0.578125]  [Adversarial loss: 0.244626, acc: 0.968750]\n",
            "836: [Discriminator loss: 0.589792, acc: 0.488281]  [Adversarial loss: 0.299056, acc: 0.960938]\n",
            "837: [Discriminator loss: 0.570934, acc: 0.527344]  [Adversarial loss: 0.292852, acc: 0.937500]\n",
            "838: [Discriminator loss: 0.476627, acc: 0.605469]  [Adversarial loss: 0.241064, acc: 0.957031]\n",
            "839: [Discriminator loss: 0.521449, acc: 0.542969]  [Adversarial loss: 0.258864, acc: 0.960938]\n",
            "840: [Discriminator loss: 0.521891, acc: 0.542969]  [Adversarial loss: 0.252505, acc: 0.972656]\n",
            "841: [Discriminator loss: 0.542915, acc: 0.507812]  [Adversarial loss: 0.288787, acc: 0.949219]\n",
            "842: [Discriminator loss: 0.624993, acc: 0.464844]  [Adversarial loss: 0.313639, acc: 0.953125]\n",
            "843: [Discriminator loss: 0.534243, acc: 0.515625]  [Adversarial loss: 0.289076, acc: 0.957031]\n",
            "844: [Discriminator loss: 0.576257, acc: 0.484375]  [Adversarial loss: 0.286255, acc: 0.960938]\n",
            "845: [Discriminator loss: 0.539940, acc: 0.515625]  [Adversarial loss: 0.279304, acc: 0.968750]\n",
            "846: [Discriminator loss: 0.533965, acc: 0.523438]  [Adversarial loss: 0.264947, acc: 0.964844]\n",
            "847: [Discriminator loss: 0.562612, acc: 0.507812]  [Adversarial loss: 0.292263, acc: 0.949219]\n",
            "848: [Discriminator loss: 0.475954, acc: 0.574219]  [Adversarial loss: 0.253590, acc: 0.957031]\n",
            "849: [Discriminator loss: 0.549224, acc: 0.531250]  [Adversarial loss: 0.279037, acc: 0.941406]\n",
            "850: [Discriminator loss: 0.619364, acc: 0.492188]  [Adversarial loss: 0.273570, acc: 0.968750]\n",
            "851: [Discriminator loss: 0.560576, acc: 0.562500]  [Adversarial loss: 0.273738, acc: 0.953125]\n",
            "852: [Discriminator loss: 0.532987, acc: 0.542969]  [Adversarial loss: 0.269168, acc: 0.933594]\n",
            "853: [Discriminator loss: 0.546104, acc: 0.507812]  [Adversarial loss: 0.284492, acc: 0.933594]\n",
            "854: [Discriminator loss: 0.488733, acc: 0.562500]  [Adversarial loss: 0.249420, acc: 0.960938]\n",
            "855: [Discriminator loss: 0.572042, acc: 0.554688]  [Adversarial loss: 0.249911, acc: 0.937500]\n",
            "856: [Discriminator loss: 0.549115, acc: 0.519531]  [Adversarial loss: 0.289478, acc: 0.949219]\n",
            "857: [Discriminator loss: 0.538441, acc: 0.546875]  [Adversarial loss: 0.262930, acc: 0.953125]\n",
            "858: [Discriminator loss: 0.613705, acc: 0.527344]  [Adversarial loss: 0.269355, acc: 0.953125]\n",
            "859: [Discriminator loss: 0.579846, acc: 0.503906]  [Adversarial loss: 0.303949, acc: 0.937500]\n",
            "860: [Discriminator loss: 0.499849, acc: 0.574219]  [Adversarial loss: 0.257343, acc: 0.972656]\n",
            "861: [Discriminator loss: 0.518868, acc: 0.582031]  [Adversarial loss: 0.259085, acc: 0.953125]\n",
            "862: [Discriminator loss: 0.571003, acc: 0.460938]  [Adversarial loss: 0.323757, acc: 0.925781]\n",
            "863: [Discriminator loss: 0.575869, acc: 0.519531]  [Adversarial loss: 0.273864, acc: 0.964844]\n",
            "864: [Discriminator loss: 0.558668, acc: 0.500000]  [Adversarial loss: 0.286811, acc: 0.949219]\n",
            "865: [Discriminator loss: 0.557844, acc: 0.527344]  [Adversarial loss: 0.276872, acc: 0.949219]\n",
            "866: [Discriminator loss: 0.562112, acc: 0.527344]  [Adversarial loss: 0.273006, acc: 0.921875]\n",
            "867: [Discriminator loss: 0.558106, acc: 0.539062]  [Adversarial loss: 0.308179, acc: 0.941406]\n",
            "868: [Discriminator loss: 0.556161, acc: 0.566406]  [Adversarial loss: 0.236898, acc: 0.941406]\n",
            "869: [Discriminator loss: 0.478294, acc: 0.554688]  [Adversarial loss: 0.257097, acc: 0.937500]\n",
            "870: [Discriminator loss: 0.592626, acc: 0.558594]  [Adversarial loss: 0.260210, acc: 0.953125]\n",
            "871: [Discriminator loss: 0.554627, acc: 0.519531]  [Adversarial loss: 0.276681, acc: 0.953125]\n",
            "872: [Discriminator loss: 0.537562, acc: 0.515625]  [Adversarial loss: 0.300567, acc: 0.925781]\n",
            "873: [Discriminator loss: 0.572340, acc: 0.531250]  [Adversarial loss: 0.286279, acc: 0.960938]\n",
            "874: [Discriminator loss: 0.634503, acc: 0.484375]  [Adversarial loss: 0.301206, acc: 0.945312]\n",
            "875: [Discriminator loss: 0.515091, acc: 0.542969]  [Adversarial loss: 0.263686, acc: 0.960938]\n",
            "876: [Discriminator loss: 0.589831, acc: 0.500000]  [Adversarial loss: 0.316873, acc: 0.937500]\n",
            "877: [Discriminator loss: 0.548154, acc: 0.539062]  [Adversarial loss: 0.269482, acc: 0.953125]\n",
            "878: [Discriminator loss: 0.566185, acc: 0.500000]  [Adversarial loss: 0.282522, acc: 0.949219]\n",
            "879: [Discriminator loss: 0.544820, acc: 0.535156]  [Adversarial loss: 0.261768, acc: 0.972656]\n",
            "880: [Discriminator loss: 0.492014, acc: 0.562500]  [Adversarial loss: 0.259843, acc: 0.949219]\n",
            "881: [Discriminator loss: 0.583226, acc: 0.527344]  [Adversarial loss: 0.291538, acc: 0.941406]\n",
            "882: [Discriminator loss: 0.546151, acc: 0.527344]  [Adversarial loss: 0.272446, acc: 0.953125]\n",
            "883: [Discriminator loss: 0.641895, acc: 0.457031]  [Adversarial loss: 0.286621, acc: 0.945312]\n",
            "884: [Discriminator loss: 0.572249, acc: 0.562500]  [Adversarial loss: 0.268578, acc: 0.960938]\n",
            "885: [Discriminator loss: 0.579668, acc: 0.519531]  [Adversarial loss: 0.263875, acc: 0.972656]\n",
            "886: [Discriminator loss: 0.580396, acc: 0.550781]  [Adversarial loss: 0.257893, acc: 0.949219]\n",
            "887: [Discriminator loss: 0.584085, acc: 0.539062]  [Adversarial loss: 0.249581, acc: 0.968750]\n",
            "888: [Discriminator loss: 0.584328, acc: 0.535156]  [Adversarial loss: 0.259666, acc: 0.968750]\n",
            "889: [Discriminator loss: 0.528700, acc: 0.570312]  [Adversarial loss: 0.242107, acc: 0.972656]\n",
            "890: [Discriminator loss: 0.614333, acc: 0.464844]  [Adversarial loss: 0.286974, acc: 0.964844]\n",
            "891: [Discriminator loss: 0.492397, acc: 0.593750]  [Adversarial loss: 0.241197, acc: 0.937500]\n",
            "892: [Discriminator loss: 0.570816, acc: 0.496094]  [Adversarial loss: 0.286389, acc: 0.949219]\n",
            "893: [Discriminator loss: 0.514535, acc: 0.558594]  [Adversarial loss: 0.268387, acc: 0.933594]\n",
            "894: [Discriminator loss: 0.586292, acc: 0.484375]  [Adversarial loss: 0.287273, acc: 0.949219]\n",
            "895: [Discriminator loss: 0.504296, acc: 0.566406]  [Adversarial loss: 0.246877, acc: 0.945312]\n",
            "896: [Discriminator loss: 0.547157, acc: 0.531250]  [Adversarial loss: 0.269889, acc: 0.949219]\n",
            "897: [Discriminator loss: 0.556445, acc: 0.539062]  [Adversarial loss: 0.267956, acc: 0.949219]\n",
            "898: [Discriminator loss: 0.568773, acc: 0.519531]  [Adversarial loss: 0.275787, acc: 0.937500]\n",
            "899: [Discriminator loss: 0.533224, acc: 0.542969]  [Adversarial loss: 0.279374, acc: 0.957031]\n",
            "900: [Discriminator loss: 0.587513, acc: 0.550781]  [Adversarial loss: 0.262982, acc: 0.937500]\n",
            "901: [Discriminator loss: 0.580141, acc: 0.515625]  [Adversarial loss: 0.290847, acc: 0.960938]\n",
            "902: [Discriminator loss: 0.577457, acc: 0.558594]  [Adversarial loss: 0.265078, acc: 0.953125]\n",
            "903: [Discriminator loss: 0.583226, acc: 0.515625]  [Adversarial loss: 0.280571, acc: 0.957031]\n",
            "904: [Discriminator loss: 0.543407, acc: 0.496094]  [Adversarial loss: 0.294410, acc: 0.921875]\n",
            "905: [Discriminator loss: 0.586249, acc: 0.519531]  [Adversarial loss: 0.266982, acc: 0.968750]\n",
            "906: [Discriminator loss: 0.575008, acc: 0.558594]  [Adversarial loss: 0.265592, acc: 0.937500]\n",
            "907: [Discriminator loss: 0.625324, acc: 0.500000]  [Adversarial loss: 0.269656, acc: 0.953125]\n",
            "908: [Discriminator loss: 0.662432, acc: 0.503906]  [Adversarial loss: 0.285387, acc: 0.933594]\n",
            "909: [Discriminator loss: 0.562109, acc: 0.574219]  [Adversarial loss: 0.265053, acc: 0.917969]\n",
            "910: [Discriminator loss: 0.617000, acc: 0.503906]  [Adversarial loss: 0.286551, acc: 0.968750]\n",
            "911: [Discriminator loss: 0.506891, acc: 0.554688]  [Adversarial loss: 0.262925, acc: 0.941406]\n",
            "912: [Discriminator loss: 0.504477, acc: 0.566406]  [Adversarial loss: 0.247780, acc: 0.964844]\n",
            "913: [Discriminator loss: 0.520092, acc: 0.527344]  [Adversarial loss: 0.259080, acc: 0.941406]\n",
            "914: [Discriminator loss: 0.547095, acc: 0.539062]  [Adversarial loss: 0.285093, acc: 0.945312]\n",
            "915: [Discriminator loss: 0.516012, acc: 0.511719]  [Adversarial loss: 0.285815, acc: 0.949219]\n",
            "916: [Discriminator loss: 0.534569, acc: 0.531250]  [Adversarial loss: 0.279174, acc: 0.953125]\n",
            "917: [Discriminator loss: 0.536309, acc: 0.519531]  [Adversarial loss: 0.274218, acc: 0.968750]\n",
            "918: [Discriminator loss: 0.577418, acc: 0.515625]  [Adversarial loss: 0.271681, acc: 0.949219]\n",
            "919: [Discriminator loss: 0.531297, acc: 0.566406]  [Adversarial loss: 0.258316, acc: 0.980469]\n",
            "920: [Discriminator loss: 0.522634, acc: 0.539062]  [Adversarial loss: 0.276614, acc: 0.937500]\n",
            "921: [Discriminator loss: 0.605740, acc: 0.523438]  [Adversarial loss: 0.273041, acc: 0.953125]\n",
            "922: [Discriminator loss: 0.542260, acc: 0.519531]  [Adversarial loss: 0.302100, acc: 0.933594]\n",
            "923: [Discriminator loss: 0.529493, acc: 0.554688]  [Adversarial loss: 0.253399, acc: 0.960938]\n",
            "924: [Discriminator loss: 0.493190, acc: 0.554688]  [Adversarial loss: 0.279120, acc: 0.949219]\n",
            "925: [Discriminator loss: 0.588692, acc: 0.531250]  [Adversarial loss: 0.287376, acc: 0.945312]\n",
            "926: [Discriminator loss: 0.547419, acc: 0.558594]  [Adversarial loss: 0.255869, acc: 0.949219]\n",
            "927: [Discriminator loss: 0.618004, acc: 0.507812]  [Adversarial loss: 0.260134, acc: 0.964844]\n",
            "928: [Discriminator loss: 0.611219, acc: 0.554688]  [Adversarial loss: 0.292772, acc: 0.953125]\n",
            "929: [Discriminator loss: 0.549241, acc: 0.507812]  [Adversarial loss: 0.285337, acc: 0.949219]\n",
            "930: [Discriminator loss: 0.578156, acc: 0.464844]  [Adversarial loss: 0.304043, acc: 0.937500]\n",
            "931: [Discriminator loss: 0.600300, acc: 0.519531]  [Adversarial loss: 0.264362, acc: 0.937500]\n",
            "932: [Discriminator loss: 0.549690, acc: 0.562500]  [Adversarial loss: 0.277567, acc: 0.945312]\n",
            "933: [Discriminator loss: 0.581037, acc: 0.539062]  [Adversarial loss: 0.268465, acc: 0.949219]\n",
            "934: [Discriminator loss: 0.564435, acc: 0.535156]  [Adversarial loss: 0.272639, acc: 0.953125]\n",
            "935: [Discriminator loss: 0.573569, acc: 0.523438]  [Adversarial loss: 0.286279, acc: 0.937500]\n",
            "936: [Discriminator loss: 0.555446, acc: 0.546875]  [Adversarial loss: 0.268777, acc: 0.945312]\n",
            "937: [Discriminator loss: 0.528054, acc: 0.523438]  [Adversarial loss: 0.272045, acc: 0.953125]\n",
            "938: [Discriminator loss: 0.579386, acc: 0.468750]  [Adversarial loss: 0.298825, acc: 0.941406]\n",
            "939: [Discriminator loss: 0.513685, acc: 0.539062]  [Adversarial loss: 0.263972, acc: 0.921875]\n",
            "940: [Discriminator loss: 0.506536, acc: 0.554688]  [Adversarial loss: 0.264312, acc: 0.945312]\n",
            "941: [Discriminator loss: 0.539114, acc: 0.527344]  [Adversarial loss: 0.245742, acc: 0.960938]\n",
            "942: [Discriminator loss: 0.597220, acc: 0.519531]  [Adversarial loss: 0.262863, acc: 0.964844]\n",
            "943: [Discriminator loss: 0.607142, acc: 0.519531]  [Adversarial loss: 0.257876, acc: 0.964844]\n",
            "944: [Discriminator loss: 0.578003, acc: 0.546875]  [Adversarial loss: 0.255787, acc: 0.945312]\n",
            "945: [Discriminator loss: 0.545718, acc: 0.550781]  [Adversarial loss: 0.267839, acc: 0.953125]\n",
            "946: [Discriminator loss: 0.627788, acc: 0.539062]  [Adversarial loss: 0.238561, acc: 0.972656]\n",
            "947: [Discriminator loss: 0.576137, acc: 0.531250]  [Adversarial loss: 0.263997, acc: 0.949219]\n",
            "948: [Discriminator loss: 0.571573, acc: 0.554688]  [Adversarial loss: 0.257521, acc: 0.937500]\n",
            "949: [Discriminator loss: 0.556435, acc: 0.554688]  [Adversarial loss: 0.245734, acc: 0.957031]\n",
            "950: [Discriminator loss: 0.633974, acc: 0.464844]  [Adversarial loss: 0.299564, acc: 0.949219]\n",
            "951: [Discriminator loss: 0.672274, acc: 0.441406]  [Adversarial loss: 0.334843, acc: 0.937500]\n",
            "952: [Discriminator loss: 0.558711, acc: 0.519531]  [Adversarial loss: 0.286413, acc: 0.914062]\n",
            "953: [Discriminator loss: 0.541498, acc: 0.566406]  [Adversarial loss: 0.256122, acc: 0.964844]\n",
            "954: [Discriminator loss: 0.570585, acc: 0.488281]  [Adversarial loss: 0.309318, acc: 0.921875]\n",
            "955: [Discriminator loss: 0.521435, acc: 0.566406]  [Adversarial loss: 0.254454, acc: 0.957031]\n",
            "956: [Discriminator loss: 0.547698, acc: 0.519531]  [Adversarial loss: 0.272599, acc: 0.945312]\n",
            "957: [Discriminator loss: 0.560956, acc: 0.500000]  [Adversarial loss: 0.273635, acc: 0.957031]\n",
            "958: [Discriminator loss: 0.539844, acc: 0.570312]  [Adversarial loss: 0.249877, acc: 0.957031]\n",
            "959: [Discriminator loss: 0.622089, acc: 0.476562]  [Adversarial loss: 0.296397, acc: 0.957031]\n",
            "960: [Discriminator loss: 0.585462, acc: 0.519531]  [Adversarial loss: 0.284184, acc: 0.925781]\n",
            "961: [Discriminator loss: 0.567529, acc: 0.539062]  [Adversarial loss: 0.274686, acc: 0.968750]\n",
            "962: [Discriminator loss: 0.542086, acc: 0.542969]  [Adversarial loss: 0.270352, acc: 0.972656]\n",
            "963: [Discriminator loss: 0.512152, acc: 0.566406]  [Adversarial loss: 0.237340, acc: 0.972656]\n",
            "964: [Discriminator loss: 0.681233, acc: 0.480469]  [Adversarial loss: 0.287381, acc: 0.941406]\n",
            "965: [Discriminator loss: 0.617929, acc: 0.503906]  [Adversarial loss: 0.307919, acc: 0.945312]\n",
            "966: [Discriminator loss: 0.607066, acc: 0.539062]  [Adversarial loss: 0.260026, acc: 0.945312]\n",
            "967: [Discriminator loss: 0.522357, acc: 0.531250]  [Adversarial loss: 0.281363, acc: 0.957031]\n",
            "968: [Discriminator loss: 0.561956, acc: 0.550781]  [Adversarial loss: 0.253574, acc: 0.980469]\n",
            "969: [Discriminator loss: 0.586337, acc: 0.496094]  [Adversarial loss: 0.286229, acc: 0.929688]\n",
            "970: [Discriminator loss: 0.522164, acc: 0.535156]  [Adversarial loss: 0.276583, acc: 0.929688]\n",
            "971: [Discriminator loss: 0.583591, acc: 0.500000]  [Adversarial loss: 0.274226, acc: 0.960938]\n",
            "972: [Discriminator loss: 0.531916, acc: 0.558594]  [Adversarial loss: 0.267256, acc: 0.949219]\n",
            "973: [Discriminator loss: 0.494671, acc: 0.566406]  [Adversarial loss: 0.256179, acc: 0.957031]\n",
            "974: [Discriminator loss: 0.549426, acc: 0.496094]  [Adversarial loss: 0.305982, acc: 0.960938]\n",
            "975: [Discriminator loss: 0.553604, acc: 0.562500]  [Adversarial loss: 0.295124, acc: 0.953125]\n",
            "976: [Discriminator loss: 0.604527, acc: 0.523438]  [Adversarial loss: 0.299086, acc: 0.929688]\n",
            "977: [Discriminator loss: 0.537144, acc: 0.550781]  [Adversarial loss: 0.254755, acc: 0.960938]\n",
            "978: [Discriminator loss: 0.513878, acc: 0.570312]  [Adversarial loss: 0.279640, acc: 0.949219]\n",
            "979: [Discriminator loss: 0.604997, acc: 0.519531]  [Adversarial loss: 0.271357, acc: 0.941406]\n",
            "980: [Discriminator loss: 0.572495, acc: 0.562500]  [Adversarial loss: 0.252750, acc: 0.964844]\n",
            "981: [Discriminator loss: 0.582807, acc: 0.542969]  [Adversarial loss: 0.276189, acc: 0.921875]\n",
            "982: [Discriminator loss: 0.537205, acc: 0.539062]  [Adversarial loss: 0.277632, acc: 0.960938]\n",
            "983: [Discriminator loss: 0.601526, acc: 0.523438]  [Adversarial loss: 0.292461, acc: 0.917969]\n",
            "984: [Discriminator loss: 0.626974, acc: 0.488281]  [Adversarial loss: 0.295516, acc: 0.917969]\n",
            "985: [Discriminator loss: 0.580689, acc: 0.515625]  [Adversarial loss: 0.262684, acc: 0.964844]\n",
            "986: [Discriminator loss: 0.687012, acc: 0.464844]  [Adversarial loss: 0.283467, acc: 0.964844]\n",
            "987: [Discriminator loss: 0.518124, acc: 0.554688]  [Adversarial loss: 0.273673, acc: 0.929688]\n",
            "988: [Discriminator loss: 0.598028, acc: 0.503906]  [Adversarial loss: 0.293857, acc: 0.957031]\n",
            "989: [Discriminator loss: 0.492453, acc: 0.585938]  [Adversarial loss: 0.233916, acc: 0.968750]\n",
            "990: [Discriminator loss: 0.478889, acc: 0.582031]  [Adversarial loss: 0.246421, acc: 0.941406]\n",
            "991: [Discriminator loss: 0.541208, acc: 0.519531]  [Adversarial loss: 0.268754, acc: 0.937500]\n",
            "992: [Discriminator loss: 0.608468, acc: 0.492188]  [Adversarial loss: 0.290200, acc: 0.949219]\n",
            "993: [Discriminator loss: 0.583536, acc: 0.523438]  [Adversarial loss: 0.285450, acc: 0.949219]\n",
            "994: [Discriminator loss: 0.567299, acc: 0.500000]  [Adversarial loss: 0.290811, acc: 0.953125]\n",
            "995: [Discriminator loss: 0.536102, acc: 0.523438]  [Adversarial loss: 0.264654, acc: 0.972656]\n",
            "996: [Discriminator loss: 0.603330, acc: 0.496094]  [Adversarial loss: 0.275267, acc: 0.960938]\n",
            "997: [Discriminator loss: 0.575952, acc: 0.511719]  [Adversarial loss: 0.253366, acc: 0.953125]\n",
            "998: [Discriminator loss: 0.565927, acc: 0.527344]  [Adversarial loss: 0.288582, acc: 0.929688]\n",
            "999: [Discriminator loss: 0.506127, acc: 0.578125]  [Adversarial loss: 0.229857, acc: 0.953125]\n",
            "1000: [Discriminator loss: 0.654271, acc: 0.500000]  [Adversarial loss: 0.278244, acc: 0.941406]\n",
            "1001: [Discriminator loss: 0.591554, acc: 0.535156]  [Adversarial loss: 0.262949, acc: 0.964844]\n",
            "1002: [Discriminator loss: 0.534873, acc: 0.539062]  [Adversarial loss: 0.257804, acc: 0.964844]\n",
            "1003: [Discriminator loss: 0.564351, acc: 0.531250]  [Adversarial loss: 0.251800, acc: 0.968750]\n",
            "1004: [Discriminator loss: 0.578829, acc: 0.511719]  [Adversarial loss: 0.289210, acc: 0.929688]\n",
            "1005: [Discriminator loss: 0.599093, acc: 0.539062]  [Adversarial loss: 0.257196, acc: 0.949219]\n",
            "1006: [Discriminator loss: 0.594209, acc: 0.550781]  [Adversarial loss: 0.254545, acc: 0.964844]\n",
            "1007: [Discriminator loss: 0.597651, acc: 0.500000]  [Adversarial loss: 0.291417, acc: 0.949219]\n",
            "1008: [Discriminator loss: 0.609233, acc: 0.496094]  [Adversarial loss: 0.294700, acc: 0.937500]\n",
            "1009: [Discriminator loss: 0.534645, acc: 0.554688]  [Adversarial loss: 0.260995, acc: 0.941406]\n",
            "1010: [Discriminator loss: 0.590995, acc: 0.492188]  [Adversarial loss: 0.310400, acc: 0.960938]\n",
            "1011: [Discriminator loss: 0.561808, acc: 0.523438]  [Adversarial loss: 0.296397, acc: 0.957031]\n",
            "1012: [Discriminator loss: 0.592789, acc: 0.539062]  [Adversarial loss: 0.276381, acc: 0.957031]\n",
            "1013: [Discriminator loss: 0.507621, acc: 0.566406]  [Adversarial loss: 0.261883, acc: 0.945312]\n",
            "1014: [Discriminator loss: 0.591004, acc: 0.527344]  [Adversarial loss: 0.272996, acc: 0.960938]\n",
            "1015: [Discriminator loss: 0.594212, acc: 0.535156]  [Adversarial loss: 0.285548, acc: 0.949219]\n",
            "1016: [Discriminator loss: 0.580894, acc: 0.527344]  [Adversarial loss: 0.269606, acc: 0.941406]\n",
            "1017: [Discriminator loss: 0.548425, acc: 0.539062]  [Adversarial loss: 0.262617, acc: 0.941406]\n",
            "1018: [Discriminator loss: 0.516713, acc: 0.589844]  [Adversarial loss: 0.255309, acc: 0.925781]\n",
            "1019: [Discriminator loss: 0.573960, acc: 0.535156]  [Adversarial loss: 0.277499, acc: 0.921875]\n",
            "1020: [Discriminator loss: 0.563342, acc: 0.535156]  [Adversarial loss: 0.276770, acc: 0.945312]\n",
            "1021: [Discriminator loss: 0.639671, acc: 0.449219]  [Adversarial loss: 0.312971, acc: 0.941406]\n",
            "1022: [Discriminator loss: 0.534020, acc: 0.546875]  [Adversarial loss: 0.327596, acc: 0.953125]\n",
            "1023: [Discriminator loss: 0.485191, acc: 0.582031]  [Adversarial loss: 0.243929, acc: 0.937500]\n",
            "1024: [Discriminator loss: 0.562909, acc: 0.554688]  [Adversarial loss: 0.276119, acc: 0.945312]\n",
            "1025: [Discriminator loss: 0.499161, acc: 0.554688]  [Adversarial loss: 0.251914, acc: 0.968750]\n",
            "1026: [Discriminator loss: 0.587976, acc: 0.542969]  [Adversarial loss: 0.278931, acc: 0.925781]\n",
            "1027: [Discriminator loss: 0.578433, acc: 0.539062]  [Adversarial loss: 0.261671, acc: 0.957031]\n",
            "1028: [Discriminator loss: 0.548823, acc: 0.562500]  [Adversarial loss: 0.273300, acc: 0.925781]\n",
            "1029: [Discriminator loss: 0.594076, acc: 0.519531]  [Adversarial loss: 0.271320, acc: 0.953125]\n",
            "1030: [Discriminator loss: 0.481384, acc: 0.554688]  [Adversarial loss: 0.299912, acc: 0.933594]\n",
            "1031: [Discriminator loss: 0.581625, acc: 0.507812]  [Adversarial loss: 0.296081, acc: 0.945312]\n",
            "1032: [Discriminator loss: 0.559212, acc: 0.539062]  [Adversarial loss: 0.280888, acc: 0.953125]\n",
            "1033: [Discriminator loss: 0.620212, acc: 0.476562]  [Adversarial loss: 0.295901, acc: 0.945312]\n",
            "1034: [Discriminator loss: 0.547981, acc: 0.535156]  [Adversarial loss: 0.280026, acc: 0.957031]\n",
            "1035: [Discriminator loss: 0.528765, acc: 0.566406]  [Adversarial loss: 0.250684, acc: 0.949219]\n",
            "1036: [Discriminator loss: 0.494344, acc: 0.585938]  [Adversarial loss: 0.247211, acc: 0.964844]\n",
            "1037: [Discriminator loss: 0.517093, acc: 0.542969]  [Adversarial loss: 0.285412, acc: 0.941406]\n",
            "1038: [Discriminator loss: 0.479427, acc: 0.574219]  [Adversarial loss: 0.219078, acc: 0.972656]\n",
            "1039: [Discriminator loss: 0.603127, acc: 0.476562]  [Adversarial loss: 0.295481, acc: 0.937500]\n",
            "1040: [Discriminator loss: 0.556779, acc: 0.527344]  [Adversarial loss: 0.267615, acc: 0.941406]\n",
            "1041: [Discriminator loss: 0.653302, acc: 0.511719]  [Adversarial loss: 0.250526, acc: 0.960938]\n",
            "1042: [Discriminator loss: 0.592187, acc: 0.515625]  [Adversarial loss: 0.305512, acc: 0.945312]\n",
            "1043: [Discriminator loss: 0.624686, acc: 0.515625]  [Adversarial loss: 0.279852, acc: 0.953125]\n",
            "1044: [Discriminator loss: 0.582115, acc: 0.605469]  [Adversarial loss: 0.271381, acc: 0.953125]\n",
            "1045: [Discriminator loss: 0.539278, acc: 0.570312]  [Adversarial loss: 0.261283, acc: 0.949219]\n",
            "1046: [Discriminator loss: 0.574709, acc: 0.570312]  [Adversarial loss: 0.237466, acc: 0.957031]\n",
            "1047: [Discriminator loss: 0.505712, acc: 0.589844]  [Adversarial loss: 0.239245, acc: 0.953125]\n",
            "1048: [Discriminator loss: 0.639153, acc: 0.519531]  [Adversarial loss: 0.296650, acc: 0.925781]\n",
            "1049: [Discriminator loss: 0.552367, acc: 0.582031]  [Adversarial loss: 0.249916, acc: 0.945312]\n",
            "1050: [Discriminator loss: 0.540090, acc: 0.515625]  [Adversarial loss: 0.304836, acc: 0.925781]\n",
            "1051: [Discriminator loss: 0.594403, acc: 0.546875]  [Adversarial loss: 0.259534, acc: 0.953125]\n",
            "1052: [Discriminator loss: 0.569755, acc: 0.523438]  [Adversarial loss: 0.294752, acc: 0.937500]\n",
            "1053: [Discriminator loss: 0.584505, acc: 0.515625]  [Adversarial loss: 0.286167, acc: 0.957031]\n",
            "1054: [Discriminator loss: 0.630556, acc: 0.511719]  [Adversarial loss: 0.293158, acc: 0.929688]\n",
            "1055: [Discriminator loss: 0.548666, acc: 0.578125]  [Adversarial loss: 0.251823, acc: 0.972656]\n",
            "1056: [Discriminator loss: 0.517612, acc: 0.546875]  [Adversarial loss: 0.276417, acc: 0.945312]\n",
            "1057: [Discriminator loss: 0.524151, acc: 0.542969]  [Adversarial loss: 0.285838, acc: 0.933594]\n",
            "1058: [Discriminator loss: 0.547245, acc: 0.531250]  [Adversarial loss: 0.273490, acc: 0.953125]\n",
            "1059: [Discriminator loss: 0.552339, acc: 0.554688]  [Adversarial loss: 0.266338, acc: 0.917969]\n",
            "1060: [Discriminator loss: 0.555562, acc: 0.539062]  [Adversarial loss: 0.279312, acc: 0.937500]\n",
            "1061: [Discriminator loss: 0.552540, acc: 0.558594]  [Adversarial loss: 0.263850, acc: 0.960938]\n",
            "1062: [Discriminator loss: 0.600249, acc: 0.539062]  [Adversarial loss: 0.282693, acc: 0.941406]\n",
            "1063: [Discriminator loss: 0.538507, acc: 0.558594]  [Adversarial loss: 0.256566, acc: 0.937500]\n",
            "1064: [Discriminator loss: 0.543731, acc: 0.578125]  [Adversarial loss: 0.249026, acc: 0.964844]\n",
            "1065: [Discriminator loss: 0.495390, acc: 0.554688]  [Adversarial loss: 0.251926, acc: 0.949219]\n",
            "1066: [Discriminator loss: 0.533018, acc: 0.531250]  [Adversarial loss: 0.268502, acc: 0.941406]\n",
            "1067: [Discriminator loss: 0.576070, acc: 0.550781]  [Adversarial loss: 0.273996, acc: 0.945312]\n",
            "1068: [Discriminator loss: 0.618507, acc: 0.550781]  [Adversarial loss: 0.290174, acc: 0.917969]\n",
            "1069: [Discriminator loss: 0.569470, acc: 0.570312]  [Adversarial loss: 0.253778, acc: 0.949219]\n",
            "1070: [Discriminator loss: 0.576158, acc: 0.562500]  [Adversarial loss: 0.262498, acc: 0.941406]\n",
            "1071: [Discriminator loss: 0.649946, acc: 0.484375]  [Adversarial loss: 0.313459, acc: 0.933594]\n",
            "1072: [Discriminator loss: 0.570246, acc: 0.484375]  [Adversarial loss: 0.295487, acc: 0.953125]\n",
            "1073: [Discriminator loss: 0.559941, acc: 0.511719]  [Adversarial loss: 0.292200, acc: 0.949219]\n",
            "1074: [Discriminator loss: 0.514543, acc: 0.574219]  [Adversarial loss: 0.258577, acc: 0.972656]\n",
            "1075: [Discriminator loss: 0.622928, acc: 0.515625]  [Adversarial loss: 0.283918, acc: 0.945312]\n",
            "1076: [Discriminator loss: 0.627126, acc: 0.472656]  [Adversarial loss: 0.279210, acc: 0.960938]\n",
            "1077: [Discriminator loss: 0.534888, acc: 0.535156]  [Adversarial loss: 0.281726, acc: 0.941406]\n",
            "1078: [Discriminator loss: 0.509967, acc: 0.585938]  [Adversarial loss: 0.240712, acc: 0.945312]\n",
            "1079: [Discriminator loss: 0.552752, acc: 0.539062]  [Adversarial loss: 0.270153, acc: 0.964844]\n",
            "1080: [Discriminator loss: 0.525941, acc: 0.535156]  [Adversarial loss: 0.261659, acc: 0.941406]\n",
            "1081: [Discriminator loss: 0.561834, acc: 0.535156]  [Adversarial loss: 0.260907, acc: 0.914062]\n",
            "1082: [Discriminator loss: 0.497470, acc: 0.570312]  [Adversarial loss: 0.256933, acc: 0.933594]\n",
            "1083: [Discriminator loss: 0.545703, acc: 0.554688]  [Adversarial loss: 0.261361, acc: 0.957031]\n",
            "1084: [Discriminator loss: 0.544288, acc: 0.539062]  [Adversarial loss: 0.286372, acc: 0.941406]\n",
            "1085: [Discriminator loss: 0.515878, acc: 0.589844]  [Adversarial loss: 0.247739, acc: 0.972656]\n",
            "1086: [Discriminator loss: 0.602825, acc: 0.515625]  [Adversarial loss: 0.266636, acc: 0.968750]\n",
            "1087: [Discriminator loss: 0.639156, acc: 0.503906]  [Adversarial loss: 0.265992, acc: 0.929688]\n",
            "1088: [Discriminator loss: 0.578433, acc: 0.582031]  [Adversarial loss: 0.240347, acc: 0.976562]\n",
            "1089: [Discriminator loss: 0.542132, acc: 0.531250]  [Adversarial loss: 0.272668, acc: 0.937500]\n",
            "1090: [Discriminator loss: 0.514934, acc: 0.589844]  [Adversarial loss: 0.258099, acc: 0.941406]\n",
            "1091: [Discriminator loss: 0.652639, acc: 0.503906]  [Adversarial loss: 0.258173, acc: 0.960938]\n",
            "1092: [Discriminator loss: 0.576372, acc: 0.527344]  [Adversarial loss: 0.262184, acc: 0.953125]\n",
            "1093: [Discriminator loss: 0.544147, acc: 0.519531]  [Adversarial loss: 0.253663, acc: 0.960938]\n",
            "1094: [Discriminator loss: 0.591070, acc: 0.535156]  [Adversarial loss: 0.269662, acc: 0.933594]\n",
            "1095: [Discriminator loss: 0.507998, acc: 0.574219]  [Adversarial loss: 0.239805, acc: 0.957031]\n",
            "1096: [Discriminator loss: 0.545746, acc: 0.542969]  [Adversarial loss: 0.247281, acc: 0.964844]\n",
            "1097: [Discriminator loss: 0.621841, acc: 0.527344]  [Adversarial loss: 0.265594, acc: 0.953125]\n",
            "1098: [Discriminator loss: 0.569348, acc: 0.527344]  [Adversarial loss: 0.260336, acc: 0.949219]\n",
            "1099: [Discriminator loss: 0.568930, acc: 0.515625]  [Adversarial loss: 0.292780, acc: 0.945312]\n",
            "1100: [Discriminator loss: 0.587412, acc: 0.496094]  [Adversarial loss: 0.288545, acc: 0.917969]\n",
            "1101: [Discriminator loss: 0.602087, acc: 0.484375]  [Adversarial loss: 0.292968, acc: 0.929688]\n",
            "1102: [Discriminator loss: 0.561831, acc: 0.558594]  [Adversarial loss: 0.263187, acc: 0.957031]\n",
            "1103: [Discriminator loss: 0.621687, acc: 0.500000]  [Adversarial loss: 0.279472, acc: 0.957031]\n",
            "1104: [Discriminator loss: 0.550778, acc: 0.554688]  [Adversarial loss: 0.274915, acc: 0.964844]\n",
            "1105: [Discriminator loss: 0.566271, acc: 0.558594]  [Adversarial loss: 0.260288, acc: 0.925781]\n",
            "1106: [Discriminator loss: 0.517570, acc: 0.574219]  [Adversarial loss: 0.235283, acc: 0.957031]\n",
            "1107: [Discriminator loss: 0.569904, acc: 0.531250]  [Adversarial loss: 0.273895, acc: 0.945312]\n",
            "1108: [Discriminator loss: 0.593297, acc: 0.535156]  [Adversarial loss: 0.286393, acc: 0.941406]\n",
            "1109: [Discriminator loss: 0.576849, acc: 0.531250]  [Adversarial loss: 0.298000, acc: 0.937500]\n",
            "1110: [Discriminator loss: 0.562439, acc: 0.542969]  [Adversarial loss: 0.247857, acc: 0.964844]\n",
            "1111: [Discriminator loss: 0.621044, acc: 0.492188]  [Adversarial loss: 0.277190, acc: 0.953125]\n",
            "1112: [Discriminator loss: 0.638178, acc: 0.546875]  [Adversarial loss: 0.254396, acc: 0.945312]\n",
            "1113: [Discriminator loss: 0.650321, acc: 0.464844]  [Adversarial loss: 0.314775, acc: 0.925781]\n",
            "1114: [Discriminator loss: 0.497862, acc: 0.554688]  [Adversarial loss: 0.275198, acc: 0.945312]\n",
            "1115: [Discriminator loss: 0.615004, acc: 0.527344]  [Adversarial loss: 0.282123, acc: 0.953125]\n",
            "1116: [Discriminator loss: 0.521888, acc: 0.554688]  [Adversarial loss: 0.268907, acc: 0.945312]\n",
            "1117: [Discriminator loss: 0.591115, acc: 0.535156]  [Adversarial loss: 0.276624, acc: 0.925781]\n",
            "1118: [Discriminator loss: 0.626881, acc: 0.523438]  [Adversarial loss: 0.276045, acc: 0.968750]\n",
            "1119: [Discriminator loss: 0.629925, acc: 0.500000]  [Adversarial loss: 0.275546, acc: 0.937500]\n",
            "1120: [Discriminator loss: 0.603914, acc: 0.562500]  [Adversarial loss: 0.269136, acc: 0.941406]\n",
            "1121: [Discriminator loss: 0.546107, acc: 0.519531]  [Adversarial loss: 0.302230, acc: 0.902344]\n",
            "1122: [Discriminator loss: 0.581348, acc: 0.531250]  [Adversarial loss: 0.271151, acc: 0.941406]\n",
            "1123: [Discriminator loss: 0.622405, acc: 0.531250]  [Adversarial loss: 0.279740, acc: 0.945312]\n",
            "1124: [Discriminator loss: 0.569109, acc: 0.531250]  [Adversarial loss: 0.299343, acc: 0.914062]\n",
            "1125: [Discriminator loss: 0.519123, acc: 0.542969]  [Adversarial loss: 0.258172, acc: 0.957031]\n",
            "1126: [Discriminator loss: 0.530420, acc: 0.515625]  [Adversarial loss: 0.282489, acc: 0.953125]\n",
            "1127: [Discriminator loss: 0.631121, acc: 0.558594]  [Adversarial loss: 0.280161, acc: 0.914062]\n",
            "1128: [Discriminator loss: 0.522971, acc: 0.531250]  [Adversarial loss: 0.273674, acc: 0.941406]\n",
            "1129: [Discriminator loss: 0.643292, acc: 0.503906]  [Adversarial loss: 0.322954, acc: 0.949219]\n",
            "1130: [Discriminator loss: 0.566443, acc: 0.539062]  [Adversarial loss: 0.278522, acc: 0.929688]\n",
            "1131: [Discriminator loss: 0.590399, acc: 0.515625]  [Adversarial loss: 0.311109, acc: 0.929688]\n",
            "1132: [Discriminator loss: 0.521058, acc: 0.574219]  [Adversarial loss: 0.247055, acc: 0.937500]\n",
            "1133: [Discriminator loss: 0.544040, acc: 0.589844]  [Adversarial loss: 0.278303, acc: 0.933594]\n",
            "1134: [Discriminator loss: 0.554794, acc: 0.511719]  [Adversarial loss: 0.309552, acc: 0.945312]\n",
            "1135: [Discriminator loss: 0.581518, acc: 0.511719]  [Adversarial loss: 0.316968, acc: 0.945312]\n",
            "1136: [Discriminator loss: 0.612406, acc: 0.539062]  [Adversarial loss: 0.279626, acc: 0.937500]\n",
            "1137: [Discriminator loss: 0.573886, acc: 0.519531]  [Adversarial loss: 0.285175, acc: 0.925781]\n",
            "1138: [Discriminator loss: 0.485948, acc: 0.582031]  [Adversarial loss: 0.245498, acc: 0.937500]\n",
            "1139: [Discriminator loss: 0.522368, acc: 0.589844]  [Adversarial loss: 0.257118, acc: 0.953125]\n",
            "1140: [Discriminator loss: 0.516845, acc: 0.570312]  [Adversarial loss: 0.248025, acc: 0.945312]\n",
            "1141: [Discriminator loss: 0.526383, acc: 0.578125]  [Adversarial loss: 0.249007, acc: 0.949219]\n",
            "1142: [Discriminator loss: 0.499241, acc: 0.582031]  [Adversarial loss: 0.255141, acc: 0.949219]\n",
            "1143: [Discriminator loss: 0.602881, acc: 0.519531]  [Adversarial loss: 0.272000, acc: 0.933594]\n",
            "1144: [Discriminator loss: 0.553835, acc: 0.582031]  [Adversarial loss: 0.233918, acc: 0.945312]\n",
            "1145: [Discriminator loss: 0.562344, acc: 0.558594]  [Adversarial loss: 0.260562, acc: 0.960938]\n",
            "1146: [Discriminator loss: 0.549832, acc: 0.546875]  [Adversarial loss: 0.253471, acc: 0.957031]\n",
            "1147: [Discriminator loss: 0.641884, acc: 0.464844]  [Adversarial loss: 0.311126, acc: 0.933594]\n",
            "1148: [Discriminator loss: 0.593598, acc: 0.539062]  [Adversarial loss: 0.258221, acc: 0.960938]\n",
            "1149: [Discriminator loss: 0.522095, acc: 0.566406]  [Adversarial loss: 0.261986, acc: 0.953125]\n",
            "1150: [Discriminator loss: 0.518955, acc: 0.574219]  [Adversarial loss: 0.251070, acc: 0.957031]\n",
            "1151: [Discriminator loss: 0.535613, acc: 0.546875]  [Adversarial loss: 0.298382, acc: 0.964844]\n",
            "1152: [Discriminator loss: 0.640229, acc: 0.519531]  [Adversarial loss: 0.273534, acc: 0.945312]\n",
            "1153: [Discriminator loss: 0.530689, acc: 0.527344]  [Adversarial loss: 0.271803, acc: 0.945312]\n",
            "1154: [Discriminator loss: 0.581342, acc: 0.523438]  [Adversarial loss: 0.279574, acc: 0.941406]\n",
            "1155: [Discriminator loss: 0.600172, acc: 0.531250]  [Adversarial loss: 0.282086, acc: 0.933594]\n",
            "1156: [Discriminator loss: 0.616438, acc: 0.515625]  [Adversarial loss: 0.313699, acc: 0.906250]\n",
            "1157: [Discriminator loss: 0.546882, acc: 0.558594]  [Adversarial loss: 0.274511, acc: 0.945312]\n",
            "1158: [Discriminator loss: 0.530524, acc: 0.582031]  [Adversarial loss: 0.278801, acc: 0.898438]\n",
            "1159: [Discriminator loss: 0.550989, acc: 0.531250]  [Adversarial loss: 0.290652, acc: 0.921875]\n",
            "1160: [Discriminator loss: 0.513083, acc: 0.558594]  [Adversarial loss: 0.252196, acc: 0.957031]\n",
            "1161: [Discriminator loss: 0.514960, acc: 0.605469]  [Adversarial loss: 0.231088, acc: 0.972656]\n",
            "1162: [Discriminator loss: 0.520534, acc: 0.527344]  [Adversarial loss: 0.264668, acc: 0.933594]\n",
            "1163: [Discriminator loss: 0.653605, acc: 0.500000]  [Adversarial loss: 0.288966, acc: 0.953125]\n",
            "1164: [Discriminator loss: 0.547884, acc: 0.550781]  [Adversarial loss: 0.272265, acc: 0.906250]\n",
            "1165: [Discriminator loss: 0.561184, acc: 0.535156]  [Adversarial loss: 0.248243, acc: 0.972656]\n",
            "1166: [Discriminator loss: 0.589097, acc: 0.519531]  [Adversarial loss: 0.312261, acc: 0.921875]\n",
            "1167: [Discriminator loss: 0.633278, acc: 0.531250]  [Adversarial loss: 0.280335, acc: 0.929688]\n",
            "1168: [Discriminator loss: 0.568725, acc: 0.542969]  [Adversarial loss: 0.282198, acc: 0.957031]\n",
            "1169: [Discriminator loss: 0.536287, acc: 0.542969]  [Adversarial loss: 0.254340, acc: 0.937500]\n",
            "1170: [Discriminator loss: 0.523243, acc: 0.613281]  [Adversarial loss: 0.248322, acc: 0.937500]\n",
            "1171: [Discriminator loss: 0.616930, acc: 0.523438]  [Adversarial loss: 0.249537, acc: 0.941406]\n",
            "1172: [Discriminator loss: 0.565120, acc: 0.566406]  [Adversarial loss: 0.248272, acc: 0.953125]\n",
            "1173: [Discriminator loss: 0.588769, acc: 0.550781]  [Adversarial loss: 0.274395, acc: 0.933594]\n",
            "1174: [Discriminator loss: 0.551288, acc: 0.570312]  [Adversarial loss: 0.253118, acc: 0.937500]\n",
            "1175: [Discriminator loss: 0.579084, acc: 0.503906]  [Adversarial loss: 0.270747, acc: 0.976562]\n",
            "1176: [Discriminator loss: 0.587409, acc: 0.496094]  [Adversarial loss: 0.290140, acc: 0.949219]\n",
            "1177: [Discriminator loss: 0.671271, acc: 0.449219]  [Adversarial loss: 0.319825, acc: 0.929688]\n",
            "1178: [Discriminator loss: 0.581271, acc: 0.531250]  [Adversarial loss: 0.278547, acc: 0.933594]\n",
            "1179: [Discriminator loss: 0.577864, acc: 0.539062]  [Adversarial loss: 0.263499, acc: 0.949219]\n",
            "1180: [Discriminator loss: 0.579599, acc: 0.546875]  [Adversarial loss: 0.250620, acc: 0.937500]\n",
            "1181: [Discriminator loss: 0.618076, acc: 0.511719]  [Adversarial loss: 0.296669, acc: 0.921875]\n",
            "1182: [Discriminator loss: 0.578954, acc: 0.531250]  [Adversarial loss: 0.263876, acc: 0.960938]\n",
            "1183: [Discriminator loss: 0.579617, acc: 0.542969]  [Adversarial loss: 0.250334, acc: 0.957031]\n",
            "1184: [Discriminator loss: 0.622560, acc: 0.500000]  [Adversarial loss: 0.287062, acc: 0.925781]\n",
            "1185: [Discriminator loss: 0.615996, acc: 0.523438]  [Adversarial loss: 0.274034, acc: 0.960938]\n",
            "1186: [Discriminator loss: 0.491225, acc: 0.562500]  [Adversarial loss: 0.231432, acc: 0.957031]\n",
            "1187: [Discriminator loss: 0.648664, acc: 0.531250]  [Adversarial loss: 0.248465, acc: 0.964844]\n",
            "1188: [Discriminator loss: 0.568733, acc: 0.539062]  [Adversarial loss: 0.258817, acc: 0.941406]\n",
            "1189: [Discriminator loss: 0.555108, acc: 0.546875]  [Adversarial loss: 0.259461, acc: 0.953125]\n",
            "1190: [Discriminator loss: 0.548559, acc: 0.535156]  [Adversarial loss: 0.257579, acc: 0.945312]\n",
            "1191: [Discriminator loss: 0.587462, acc: 0.531250]  [Adversarial loss: 0.275015, acc: 0.949219]\n",
            "1192: [Discriminator loss: 0.657280, acc: 0.531250]  [Adversarial loss: 0.282633, acc: 0.941406]\n",
            "1193: [Discriminator loss: 0.526067, acc: 0.593750]  [Adversarial loss: 0.272551, acc: 0.953125]\n",
            "1194: [Discriminator loss: 0.590981, acc: 0.515625]  [Adversarial loss: 0.290010, acc: 0.949219]\n",
            "1195: [Discriminator loss: 0.633572, acc: 0.511719]  [Adversarial loss: 0.302769, acc: 0.960938]\n",
            "1196: [Discriminator loss: 0.534688, acc: 0.570312]  [Adversarial loss: 0.251152, acc: 0.957031]\n",
            "1197: [Discriminator loss: 0.614294, acc: 0.562500]  [Adversarial loss: 0.283862, acc: 0.941406]\n",
            "1198: [Discriminator loss: 0.603372, acc: 0.531250]  [Adversarial loss: 0.264554, acc: 0.949219]\n",
            "1199: [Discriminator loss: 0.592758, acc: 0.558594]  [Adversarial loss: 0.273805, acc: 0.945312]\n",
            "1200: [Discriminator loss: 0.599536, acc: 0.519531]  [Adversarial loss: 0.281732, acc: 0.917969]\n",
            "1201: [Discriminator loss: 0.556363, acc: 0.542969]  [Adversarial loss: 0.257365, acc: 0.937500]\n",
            "1202: [Discriminator loss: 0.585176, acc: 0.535156]  [Adversarial loss: 0.273073, acc: 0.945312]\n",
            "1203: [Discriminator loss: 0.524850, acc: 0.554688]  [Adversarial loss: 0.279985, acc: 0.960938]\n",
            "1204: [Discriminator loss: 0.557735, acc: 0.539062]  [Adversarial loss: 0.282668, acc: 0.937500]\n",
            "1205: [Discriminator loss: 0.500719, acc: 0.582031]  [Adversarial loss: 0.243492, acc: 0.937500]\n",
            "1206: [Discriminator loss: 0.562314, acc: 0.558594]  [Adversarial loss: 0.250956, acc: 0.953125]\n",
            "1207: [Discriminator loss: 0.652705, acc: 0.492188]  [Adversarial loss: 0.264643, acc: 0.941406]\n",
            "1208: [Discriminator loss: 0.534825, acc: 0.566406]  [Adversarial loss: 0.252973, acc: 0.945312]\n",
            "1209: [Discriminator loss: 0.508385, acc: 0.570312]  [Adversarial loss: 0.256369, acc: 0.937500]\n",
            "1210: [Discriminator loss: 0.616962, acc: 0.531250]  [Adversarial loss: 0.282564, acc: 0.953125]\n",
            "1211: [Discriminator loss: 0.475628, acc: 0.585938]  [Adversarial loss: 0.262009, acc: 0.917969]\n",
            "1212: [Discriminator loss: 0.516657, acc: 0.605469]  [Adversarial loss: 0.236261, acc: 0.937500]\n",
            "1213: [Discriminator loss: 0.635128, acc: 0.492188]  [Adversarial loss: 0.317539, acc: 0.917969]\n",
            "1214: [Discriminator loss: 0.536765, acc: 0.554688]  [Adversarial loss: 0.259196, acc: 0.945312]\n",
            "1215: [Discriminator loss: 0.543070, acc: 0.542969]  [Adversarial loss: 0.272980, acc: 0.902344]\n",
            "1216: [Discriminator loss: 0.559779, acc: 0.511719]  [Adversarial loss: 0.281881, acc: 0.933594]\n",
            "1217: [Discriminator loss: 0.555744, acc: 0.554688]  [Adversarial loss: 0.252674, acc: 0.972656]\n",
            "1218: [Discriminator loss: 0.526989, acc: 0.554688]  [Adversarial loss: 0.265867, acc: 0.925781]\n",
            "1219: [Discriminator loss: 0.512114, acc: 0.558594]  [Adversarial loss: 0.288604, acc: 0.925781]\n",
            "1220: [Discriminator loss: 0.583923, acc: 0.496094]  [Adversarial loss: 0.296595, acc: 0.933594]\n",
            "1221: [Discriminator loss: 0.581552, acc: 0.546875]  [Adversarial loss: 0.256379, acc: 0.949219]\n",
            "1222: [Discriminator loss: 0.647097, acc: 0.519531]  [Adversarial loss: 0.275995, acc: 0.945312]\n",
            "1223: [Discriminator loss: 0.531985, acc: 0.562500]  [Adversarial loss: 0.257485, acc: 0.972656]\n",
            "1224: [Discriminator loss: 0.575443, acc: 0.589844]  [Adversarial loss: 0.231665, acc: 0.941406]\n",
            "1225: [Discriminator loss: 0.539544, acc: 0.542969]  [Adversarial loss: 0.266318, acc: 0.937500]\n",
            "1226: [Discriminator loss: 0.571836, acc: 0.554688]  [Adversarial loss: 0.292045, acc: 0.902344]\n",
            "1227: [Discriminator loss: 0.730832, acc: 0.476562]  [Adversarial loss: 0.307207, acc: 0.925781]\n",
            "1228: [Discriminator loss: 0.575560, acc: 0.539062]  [Adversarial loss: 0.268383, acc: 0.933594]\n",
            "1229: [Discriminator loss: 0.539266, acc: 0.566406]  [Adversarial loss: 0.276123, acc: 0.925781]\n",
            "1230: [Discriminator loss: 0.604913, acc: 0.531250]  [Adversarial loss: 0.291819, acc: 0.910156]\n",
            "1231: [Discriminator loss: 0.574288, acc: 0.500000]  [Adversarial loss: 0.293578, acc: 0.953125]\n",
            "1232: [Discriminator loss: 0.548941, acc: 0.589844]  [Adversarial loss: 0.239311, acc: 0.929688]\n",
            "1233: [Discriminator loss: 0.584660, acc: 0.558594]  [Adversarial loss: 0.271714, acc: 0.925781]\n",
            "1234: [Discriminator loss: 0.590333, acc: 0.546875]  [Adversarial loss: 0.281039, acc: 0.910156]\n",
            "1235: [Discriminator loss: 0.539020, acc: 0.523438]  [Adversarial loss: 0.282077, acc: 0.914062]\n",
            "1236: [Discriminator loss: 0.597156, acc: 0.582031]  [Adversarial loss: 0.261196, acc: 0.949219]\n",
            "1237: [Discriminator loss: 0.542152, acc: 0.566406]  [Adversarial loss: 0.271263, acc: 0.937500]\n",
            "1238: [Discriminator loss: 0.560114, acc: 0.566406]  [Adversarial loss: 0.254206, acc: 0.945312]\n",
            "1239: [Discriminator loss: 0.565710, acc: 0.550781]  [Adversarial loss: 0.261319, acc: 0.972656]\n",
            "1240: [Discriminator loss: 0.574830, acc: 0.550781]  [Adversarial loss: 0.269331, acc: 0.953125]\n",
            "1241: [Discriminator loss: 0.597740, acc: 0.535156]  [Adversarial loss: 0.271656, acc: 0.917969]\n",
            "1242: [Discriminator loss: 0.571270, acc: 0.527344]  [Adversarial loss: 0.250493, acc: 0.960938]\n",
            "1243: [Discriminator loss: 0.598682, acc: 0.550781]  [Adversarial loss: 0.264573, acc: 0.945312]\n",
            "1244: [Discriminator loss: 0.590417, acc: 0.527344]  [Adversarial loss: 0.270778, acc: 0.917969]\n",
            "1245: [Discriminator loss: 0.574820, acc: 0.531250]  [Adversarial loss: 0.309637, acc: 0.929688]\n",
            "1246: [Discriminator loss: 0.576484, acc: 0.550781]  [Adversarial loss: 0.279495, acc: 0.914062]\n",
            "1247: [Discriminator loss: 0.512946, acc: 0.574219]  [Adversarial loss: 0.277548, acc: 0.933594]\n",
            "1248: [Discriminator loss: 0.557493, acc: 0.550781]  [Adversarial loss: 0.264484, acc: 0.925781]\n",
            "1249: [Discriminator loss: 0.600390, acc: 0.539062]  [Adversarial loss: 0.317055, acc: 0.929688]\n",
            "1250: [Discriminator loss: 0.510061, acc: 0.589844]  [Adversarial loss: 0.261839, acc: 0.929688]\n",
            "1251: [Discriminator loss: 0.495917, acc: 0.585938]  [Adversarial loss: 0.268667, acc: 0.929688]\n",
            "1252: [Discriminator loss: 0.601998, acc: 0.507812]  [Adversarial loss: 0.292083, acc: 0.937500]\n",
            "1253: [Discriminator loss: 0.548096, acc: 0.535156]  [Adversarial loss: 0.303448, acc: 0.929688]\n",
            "1254: [Discriminator loss: 0.483011, acc: 0.582031]  [Adversarial loss: 0.261638, acc: 0.902344]\n",
            "1255: [Discriminator loss: 0.654961, acc: 0.500000]  [Adversarial loss: 0.266499, acc: 0.925781]\n",
            "1256: [Discriminator loss: 0.543550, acc: 0.550781]  [Adversarial loss: 0.285218, acc: 0.914062]\n",
            "1257: [Discriminator loss: 0.491169, acc: 0.597656]  [Adversarial loss: 0.259218, acc: 0.921875]\n",
            "1258: [Discriminator loss: 0.607931, acc: 0.531250]  [Adversarial loss: 0.269352, acc: 0.964844]\n",
            "1259: [Discriminator loss: 0.540690, acc: 0.550781]  [Adversarial loss: 0.279207, acc: 0.929688]\n",
            "1260: [Discriminator loss: 0.563259, acc: 0.562500]  [Adversarial loss: 0.258481, acc: 0.949219]\n",
            "1261: [Discriminator loss: 0.562186, acc: 0.531250]  [Adversarial loss: 0.256060, acc: 0.921875]\n",
            "1262: [Discriminator loss: 0.610204, acc: 0.523438]  [Adversarial loss: 0.264676, acc: 0.933594]\n",
            "1263: [Discriminator loss: 0.645126, acc: 0.515625]  [Adversarial loss: 0.279268, acc: 0.921875]\n",
            "1264: [Discriminator loss: 0.568543, acc: 0.566406]  [Adversarial loss: 0.253860, acc: 0.968750]\n",
            "1265: [Discriminator loss: 0.562766, acc: 0.542969]  [Adversarial loss: 0.271445, acc: 0.941406]\n",
            "1266: [Discriminator loss: 0.581378, acc: 0.554688]  [Adversarial loss: 0.269460, acc: 0.925781]\n",
            "1267: [Discriminator loss: 0.633454, acc: 0.515625]  [Adversarial loss: 0.294000, acc: 0.925781]\n",
            "1268: [Discriminator loss: 0.512152, acc: 0.582031]  [Adversarial loss: 0.266466, acc: 0.953125]\n",
            "1269: [Discriminator loss: 0.603798, acc: 0.531250]  [Adversarial loss: 0.289758, acc: 0.917969]\n",
            "1270: [Discriminator loss: 0.589881, acc: 0.550781]  [Adversarial loss: 0.295797, acc: 0.929688]\n",
            "1271: [Discriminator loss: 0.551896, acc: 0.558594]  [Adversarial loss: 0.259142, acc: 0.929688]\n",
            "1272: [Discriminator loss: 0.577122, acc: 0.554688]  [Adversarial loss: 0.277197, acc: 0.906250]\n",
            "1273: [Discriminator loss: 0.534521, acc: 0.550781]  [Adversarial loss: 0.276581, acc: 0.925781]\n",
            "1274: [Discriminator loss: 0.603142, acc: 0.527344]  [Adversarial loss: 0.302642, acc: 0.941406]\n",
            "1275: [Discriminator loss: 0.538979, acc: 0.546875]  [Adversarial loss: 0.287910, acc: 0.917969]\n",
            "1276: [Discriminator loss: 0.562583, acc: 0.566406]  [Adversarial loss: 0.272729, acc: 0.929688]\n",
            "1277: [Discriminator loss: 0.498269, acc: 0.574219]  [Adversarial loss: 0.258893, acc: 0.941406]\n",
            "1278: [Discriminator loss: 0.549809, acc: 0.558594]  [Adversarial loss: 0.293002, acc: 0.921875]\n",
            "1279: [Discriminator loss: 0.535139, acc: 0.554688]  [Adversarial loss: 0.270693, acc: 0.933594]\n",
            "1280: [Discriminator loss: 0.565325, acc: 0.535156]  [Adversarial loss: 0.272532, acc: 0.941406]\n",
            "1281: [Discriminator loss: 0.519335, acc: 0.562500]  [Adversarial loss: 0.272005, acc: 0.914062]\n",
            "1282: [Discriminator loss: 0.532448, acc: 0.546875]  [Adversarial loss: 0.276296, acc: 0.937500]\n",
            "1283: [Discriminator loss: 0.552711, acc: 0.542969]  [Adversarial loss: 0.270811, acc: 0.937500]\n",
            "1284: [Discriminator loss: 0.578842, acc: 0.562500]  [Adversarial loss: 0.266450, acc: 0.929688]\n",
            "1285: [Discriminator loss: 0.592153, acc: 0.570312]  [Adversarial loss: 0.253847, acc: 0.945312]\n",
            "1286: [Discriminator loss: 0.580934, acc: 0.535156]  [Adversarial loss: 0.262593, acc: 0.964844]\n",
            "1287: [Discriminator loss: 0.602296, acc: 0.535156]  [Adversarial loss: 0.304583, acc: 0.933594]\n",
            "1288: [Discriminator loss: 0.622097, acc: 0.527344]  [Adversarial loss: 0.289297, acc: 0.921875]\n",
            "1289: [Discriminator loss: 0.545614, acc: 0.578125]  [Adversarial loss: 0.257816, acc: 0.945312]\n",
            "1290: [Discriminator loss: 0.557084, acc: 0.535156]  [Adversarial loss: 0.312649, acc: 0.902344]\n",
            "1291: [Discriminator loss: 0.581381, acc: 0.535156]  [Adversarial loss: 0.259147, acc: 0.968750]\n",
            "1292: [Discriminator loss: 0.507213, acc: 0.550781]  [Adversarial loss: 0.276862, acc: 0.941406]\n",
            "1293: [Discriminator loss: 0.533685, acc: 0.554688]  [Adversarial loss: 0.262134, acc: 0.941406]\n",
            "1294: [Discriminator loss: 0.540850, acc: 0.531250]  [Adversarial loss: 0.271375, acc: 0.929688]\n",
            "1295: [Discriminator loss: 0.539281, acc: 0.558594]  [Adversarial loss: 0.242640, acc: 0.941406]\n",
            "1296: [Discriminator loss: 0.515288, acc: 0.558594]  [Adversarial loss: 0.287422, acc: 0.933594]\n",
            "1297: [Discriminator loss: 0.563689, acc: 0.570312]  [Adversarial loss: 0.260913, acc: 0.929688]\n",
            "1298: [Discriminator loss: 0.614892, acc: 0.500000]  [Adversarial loss: 0.330882, acc: 0.933594]\n",
            "1299: [Discriminator loss: 0.585644, acc: 0.531250]  [Adversarial loss: 0.275287, acc: 0.945312]\n",
            "1300: [Discriminator loss: 0.663814, acc: 0.527344]  [Adversarial loss: 0.258909, acc: 0.949219]\n",
            "1301: [Discriminator loss: 0.532064, acc: 0.578125]  [Adversarial loss: 0.257819, acc: 0.949219]\n",
            "1302: [Discriminator loss: 0.608468, acc: 0.542969]  [Adversarial loss: 0.285166, acc: 0.937500]\n",
            "1303: [Discriminator loss: 0.539881, acc: 0.570312]  [Adversarial loss: 0.265213, acc: 0.941406]\n",
            "1304: [Discriminator loss: 0.615354, acc: 0.503906]  [Adversarial loss: 0.283214, acc: 0.945312]\n",
            "1305: [Discriminator loss: 0.586718, acc: 0.515625]  [Adversarial loss: 0.287044, acc: 0.937500]\n",
            "1306: [Discriminator loss: 0.549816, acc: 0.554688]  [Adversarial loss: 0.276442, acc: 0.941406]\n",
            "1307: [Discriminator loss: 0.583878, acc: 0.550781]  [Adversarial loss: 0.251403, acc: 0.921875]\n",
            "1308: [Discriminator loss: 0.565021, acc: 0.558594]  [Adversarial loss: 0.258937, acc: 0.945312]\n",
            "1309: [Discriminator loss: 0.579679, acc: 0.535156]  [Adversarial loss: 0.255698, acc: 0.968750]\n",
            "1310: [Discriminator loss: 0.502551, acc: 0.566406]  [Adversarial loss: 0.244447, acc: 0.960938]\n",
            "1311: [Discriminator loss: 0.605266, acc: 0.585938]  [Adversarial loss: 0.239499, acc: 0.953125]\n",
            "1312: [Discriminator loss: 0.572181, acc: 0.519531]  [Adversarial loss: 0.275194, acc: 0.917969]\n",
            "1313: [Discriminator loss: 0.661932, acc: 0.515625]  [Adversarial loss: 0.257014, acc: 0.945312]\n",
            "1314: [Discriminator loss: 0.581054, acc: 0.535156]  [Adversarial loss: 0.265326, acc: 0.945312]\n",
            "1315: [Discriminator loss: 0.602618, acc: 0.597656]  [Adversarial loss: 0.258175, acc: 0.949219]\n",
            "1316: [Discriminator loss: 0.701250, acc: 0.511719]  [Adversarial loss: 0.324045, acc: 0.921875]\n",
            "1317: [Discriminator loss: 0.560144, acc: 0.585938]  [Adversarial loss: 0.304327, acc: 0.941406]\n",
            "1318: [Discriminator loss: 0.522641, acc: 0.609375]  [Adversarial loss: 0.250816, acc: 0.949219]\n",
            "1319: [Discriminator loss: 0.508391, acc: 0.574219]  [Adversarial loss: 0.305471, acc: 0.921875]\n",
            "1320: [Discriminator loss: 0.619303, acc: 0.511719]  [Adversarial loss: 0.302937, acc: 0.933594]\n",
            "1321: [Discriminator loss: 0.508351, acc: 0.574219]  [Adversarial loss: 0.254874, acc: 0.921875]\n",
            "1322: [Discriminator loss: 0.561286, acc: 0.531250]  [Adversarial loss: 0.274865, acc: 0.937500]\n",
            "1323: [Discriminator loss: 0.520603, acc: 0.566406]  [Adversarial loss: 0.262447, acc: 0.925781]\n",
            "1324: [Discriminator loss: 0.578304, acc: 0.511719]  [Adversarial loss: 0.294533, acc: 0.925781]\n",
            "1325: [Discriminator loss: 0.505692, acc: 0.589844]  [Adversarial loss: 0.272495, acc: 0.957031]\n",
            "1326: [Discriminator loss: 0.582427, acc: 0.507812]  [Adversarial loss: 0.290550, acc: 0.929688]\n",
            "1327: [Discriminator loss: 0.593325, acc: 0.539062]  [Adversarial loss: 0.255730, acc: 0.953125]\n",
            "1328: [Discriminator loss: 0.550549, acc: 0.605469]  [Adversarial loss: 0.258122, acc: 0.941406]\n",
            "1329: [Discriminator loss: 0.564463, acc: 0.574219]  [Adversarial loss: 0.245869, acc: 0.964844]\n",
            "1330: [Discriminator loss: 0.595034, acc: 0.503906]  [Adversarial loss: 0.296319, acc: 0.929688]\n",
            "1331: [Discriminator loss: 0.531747, acc: 0.550781]  [Adversarial loss: 0.292645, acc: 0.902344]\n",
            "1332: [Discriminator loss: 0.568605, acc: 0.554688]  [Adversarial loss: 0.285530, acc: 0.925781]\n",
            "1333: [Discriminator loss: 0.511495, acc: 0.578125]  [Adversarial loss: 0.272728, acc: 0.910156]\n",
            "1334: [Discriminator loss: 0.546419, acc: 0.542969]  [Adversarial loss: 0.279974, acc: 0.937500]\n",
            "1335: [Discriminator loss: 0.523313, acc: 0.566406]  [Adversarial loss: 0.266226, acc: 0.941406]\n",
            "1336: [Discriminator loss: 0.588615, acc: 0.570312]  [Adversarial loss: 0.246503, acc: 0.968750]\n",
            "1337: [Discriminator loss: 0.594665, acc: 0.535156]  [Adversarial loss: 0.286593, acc: 0.921875]\n",
            "1338: [Discriminator loss: 0.634795, acc: 0.519531]  [Adversarial loss: 0.289594, acc: 0.902344]\n",
            "1339: [Discriminator loss: 0.600515, acc: 0.500000]  [Adversarial loss: 0.304739, acc: 0.929688]\n",
            "1340: [Discriminator loss: 0.561459, acc: 0.570312]  [Adversarial loss: 0.247392, acc: 0.929688]\n",
            "1341: [Discriminator loss: 0.619318, acc: 0.515625]  [Adversarial loss: 0.308550, acc: 0.890625]\n",
            "1342: [Discriminator loss: 0.503381, acc: 0.582031]  [Adversarial loss: 0.237587, acc: 0.953125]\n",
            "1343: [Discriminator loss: 0.624201, acc: 0.527344]  [Adversarial loss: 0.274181, acc: 0.945312]\n",
            "1344: [Discriminator loss: 0.576262, acc: 0.554688]  [Adversarial loss: 0.259701, acc: 0.957031]\n",
            "1345: [Discriminator loss: 0.536271, acc: 0.566406]  [Adversarial loss: 0.273500, acc: 0.886719]\n",
            "1346: [Discriminator loss: 0.574253, acc: 0.531250]  [Adversarial loss: 0.286067, acc: 0.933594]\n",
            "1347: [Discriminator loss: 0.576577, acc: 0.535156]  [Adversarial loss: 0.260301, acc: 0.941406]\n",
            "1348: [Discriminator loss: 0.531993, acc: 0.570312]  [Adversarial loss: 0.263271, acc: 0.933594]\n",
            "1349: [Discriminator loss: 0.562429, acc: 0.511719]  [Adversarial loss: 0.277840, acc: 0.945312]\n",
            "1350: [Discriminator loss: 0.577512, acc: 0.488281]  [Adversarial loss: 0.321241, acc: 0.925781]\n",
            "1351: [Discriminator loss: 0.566348, acc: 0.589844]  [Adversarial loss: 0.268432, acc: 0.921875]\n",
            "1352: [Discriminator loss: 0.632254, acc: 0.492188]  [Adversarial loss: 0.297542, acc: 0.902344]\n",
            "1353: [Discriminator loss: 0.533838, acc: 0.546875]  [Adversarial loss: 0.290457, acc: 0.917969]\n",
            "1354: [Discriminator loss: 0.595697, acc: 0.511719]  [Adversarial loss: 0.271814, acc: 0.953125]\n",
            "1355: [Discriminator loss: 0.574386, acc: 0.535156]  [Adversarial loss: 0.275757, acc: 0.945312]\n",
            "1356: [Discriminator loss: 0.586163, acc: 0.507812]  [Adversarial loss: 0.284149, acc: 0.937500]\n",
            "1357: [Discriminator loss: 0.582413, acc: 0.519531]  [Adversarial loss: 0.304583, acc: 0.898438]\n",
            "1358: [Discriminator loss: 0.560509, acc: 0.546875]  [Adversarial loss: 0.271313, acc: 0.929688]\n",
            "1359: [Discriminator loss: 0.535766, acc: 0.539062]  [Adversarial loss: 0.264543, acc: 0.953125]\n",
            "1360: [Discriminator loss: 0.550947, acc: 0.574219]  [Adversarial loss: 0.265823, acc: 0.921875]\n",
            "1361: [Discriminator loss: 0.628599, acc: 0.519531]  [Adversarial loss: 0.283711, acc: 0.933594]\n",
            "1362: [Discriminator loss: 0.565201, acc: 0.527344]  [Adversarial loss: 0.282024, acc: 0.925781]\n",
            "1363: [Discriminator loss: 0.595279, acc: 0.550781]  [Adversarial loss: 0.261661, acc: 0.941406]\n",
            "1364: [Discriminator loss: 0.665056, acc: 0.503906]  [Adversarial loss: 0.322610, acc: 0.902344]\n",
            "1365: [Discriminator loss: 0.639269, acc: 0.492188]  [Adversarial loss: 0.295654, acc: 0.910156]\n",
            "1366: [Discriminator loss: 0.576446, acc: 0.554688]  [Adversarial loss: 0.264852, acc: 0.929688]\n",
            "1367: [Discriminator loss: 0.630019, acc: 0.488281]  [Adversarial loss: 0.296189, acc: 0.925781]\n",
            "1368: [Discriminator loss: 0.525539, acc: 0.539062]  [Adversarial loss: 0.285977, acc: 0.925781]\n",
            "1369: [Discriminator loss: 0.531708, acc: 0.570312]  [Adversarial loss: 0.264096, acc: 0.957031]\n",
            "1370: [Discriminator loss: 0.495222, acc: 0.589844]  [Adversarial loss: 0.262051, acc: 0.929688]\n",
            "1371: [Discriminator loss: 0.543759, acc: 0.523438]  [Adversarial loss: 0.270787, acc: 0.937500]\n",
            "1372: [Discriminator loss: 0.526153, acc: 0.574219]  [Adversarial loss: 0.258323, acc: 0.921875]\n",
            "1373: [Discriminator loss: 0.508469, acc: 0.589844]  [Adversarial loss: 0.249142, acc: 0.941406]\n",
            "1374: [Discriminator loss: 0.504126, acc: 0.621094]  [Adversarial loss: 0.245569, acc: 0.917969]\n",
            "1375: [Discriminator loss: 0.568575, acc: 0.566406]  [Adversarial loss: 0.267567, acc: 0.921875]\n",
            "1376: [Discriminator loss: 0.569288, acc: 0.535156]  [Adversarial loss: 0.283045, acc: 0.902344]\n",
            "1377: [Discriminator loss: 0.647872, acc: 0.535156]  [Adversarial loss: 0.284058, acc: 0.902344]\n",
            "1378: [Discriminator loss: 0.609445, acc: 0.546875]  [Adversarial loss: 0.250787, acc: 0.937500]\n",
            "1379: [Discriminator loss: 0.615988, acc: 0.527344]  [Adversarial loss: 0.284301, acc: 0.925781]\n",
            "1380: [Discriminator loss: 0.520610, acc: 0.566406]  [Adversarial loss: 0.250988, acc: 0.937500]\n",
            "1381: [Discriminator loss: 0.675786, acc: 0.476562]  [Adversarial loss: 0.311513, acc: 0.921875]\n",
            "1382: [Discriminator loss: 0.636366, acc: 0.531250]  [Adversarial loss: 0.268973, acc: 0.953125]\n",
            "1383: [Discriminator loss: 0.589431, acc: 0.550781]  [Adversarial loss: 0.271211, acc: 0.910156]\n",
            "1384: [Discriminator loss: 0.564877, acc: 0.527344]  [Adversarial loss: 0.280348, acc: 0.933594]\n",
            "1385: [Discriminator loss: 0.518114, acc: 0.554688]  [Adversarial loss: 0.265234, acc: 0.917969]\n",
            "1386: [Discriminator loss: 0.573979, acc: 0.550781]  [Adversarial loss: 0.262415, acc: 0.929688]\n",
            "1387: [Discriminator loss: 0.625223, acc: 0.531250]  [Adversarial loss: 0.281680, acc: 0.902344]\n",
            "1388: [Discriminator loss: 0.577616, acc: 0.531250]  [Adversarial loss: 0.287014, acc: 0.902344]\n",
            "1389: [Discriminator loss: 0.533911, acc: 0.621094]  [Adversarial loss: 0.249451, acc: 0.878906]\n",
            "1390: [Discriminator loss: 0.606104, acc: 0.515625]  [Adversarial loss: 0.283291, acc: 0.914062]\n",
            "1391: [Discriminator loss: 0.657722, acc: 0.507812]  [Adversarial loss: 0.312780, acc: 0.898438]\n",
            "1392: [Discriminator loss: 0.567632, acc: 0.566406]  [Adversarial loss: 0.283552, acc: 0.898438]\n",
            "1393: [Discriminator loss: 0.547135, acc: 0.589844]  [Adversarial loss: 0.261825, acc: 0.917969]\n",
            "1394: [Discriminator loss: 0.556178, acc: 0.546875]  [Adversarial loss: 0.275163, acc: 0.929688]\n",
            "1395: [Discriminator loss: 0.622346, acc: 0.527344]  [Adversarial loss: 0.298301, acc: 0.910156]\n",
            "1396: [Discriminator loss: 0.572869, acc: 0.542969]  [Adversarial loss: 0.298234, acc: 0.921875]\n",
            "1397: [Discriminator loss: 0.519641, acc: 0.574219]  [Adversarial loss: 0.258706, acc: 0.929688]\n",
            "1398: [Discriminator loss: 0.559764, acc: 0.574219]  [Adversarial loss: 0.283823, acc: 0.921875]\n",
            "1399: [Discriminator loss: 0.509631, acc: 0.542969]  [Adversarial loss: 0.273586, acc: 0.898438]\n",
            "1400: [Discriminator loss: 0.595357, acc: 0.550781]  [Adversarial loss: 0.282307, acc: 0.933594]\n",
            "1401: [Discriminator loss: 0.512762, acc: 0.601562]  [Adversarial loss: 0.250013, acc: 0.902344]\n",
            "1402: [Discriminator loss: 0.533551, acc: 0.570312]  [Adversarial loss: 0.269069, acc: 0.906250]\n",
            "1403: [Discriminator loss: 0.488890, acc: 0.609375]  [Adversarial loss: 0.245623, acc: 0.933594]\n",
            "1404: [Discriminator loss: 0.510323, acc: 0.578125]  [Adversarial loss: 0.266383, acc: 0.890625]\n",
            "1405: [Discriminator loss: 0.531196, acc: 0.562500]  [Adversarial loss: 0.294530, acc: 0.917969]\n",
            "1406: [Discriminator loss: 0.607423, acc: 0.511719]  [Adversarial loss: 0.283094, acc: 0.917969]\n",
            "1407: [Discriminator loss: 0.485386, acc: 0.628906]  [Adversarial loss: 0.233943, acc: 0.925781]\n",
            "1408: [Discriminator loss: 0.613462, acc: 0.546875]  [Adversarial loss: 0.269688, acc: 0.914062]\n",
            "1409: [Discriminator loss: 0.533900, acc: 0.589844]  [Adversarial loss: 0.261799, acc: 0.898438]\n",
            "1410: [Discriminator loss: 0.526655, acc: 0.585938]  [Adversarial loss: 0.312079, acc: 0.878906]\n",
            "1411: [Discriminator loss: 0.602886, acc: 0.562500]  [Adversarial loss: 0.296924, acc: 0.906250]\n",
            "1412: [Discriminator loss: 0.603384, acc: 0.593750]  [Adversarial loss: 0.275761, acc: 0.914062]\n",
            "1413: [Discriminator loss: 0.662734, acc: 0.562500]  [Adversarial loss: 0.292635, acc: 0.910156]\n",
            "1414: [Discriminator loss: 0.579516, acc: 0.550781]  [Adversarial loss: 0.295322, acc: 0.871094]\n",
            "1415: [Discriminator loss: 0.524546, acc: 0.535156]  [Adversarial loss: 0.307503, acc: 0.925781]\n",
            "1416: [Discriminator loss: 0.513511, acc: 0.578125]  [Adversarial loss: 0.277722, acc: 0.894531]\n",
            "1417: [Discriminator loss: 0.562776, acc: 0.515625]  [Adversarial loss: 0.311803, acc: 0.898438]\n",
            "1418: [Discriminator loss: 0.503468, acc: 0.542969]  [Adversarial loss: 0.282777, acc: 0.878906]\n",
            "1419: [Discriminator loss: 0.526163, acc: 0.535156]  [Adversarial loss: 0.286075, acc: 0.898438]\n",
            "1420: [Discriminator loss: 0.561715, acc: 0.539062]  [Adversarial loss: 0.285852, acc: 0.882812]\n",
            "1421: [Discriminator loss: 0.503612, acc: 0.554688]  [Adversarial loss: 0.266838, acc: 0.910156]\n",
            "1422: [Discriminator loss: 0.571873, acc: 0.535156]  [Adversarial loss: 0.285728, acc: 0.898438]\n",
            "1423: [Discriminator loss: 0.615564, acc: 0.535156]  [Adversarial loss: 0.337253, acc: 0.875000]\n",
            "1424: [Discriminator loss: 0.553488, acc: 0.601562]  [Adversarial loss: 0.253546, acc: 0.886719]\n",
            "1425: [Discriminator loss: 0.578971, acc: 0.550781]  [Adversarial loss: 0.294299, acc: 0.890625]\n",
            "1426: [Discriminator loss: 0.486982, acc: 0.605469]  [Adversarial loss: 0.239977, acc: 0.941406]\n",
            "1427: [Discriminator loss: 0.517346, acc: 0.597656]  [Adversarial loss: 0.268123, acc: 0.914062]\n",
            "1428: [Discriminator loss: 0.533333, acc: 0.562500]  [Adversarial loss: 0.269548, acc: 0.898438]\n",
            "1429: [Discriminator loss: 0.571467, acc: 0.554688]  [Adversarial loss: 0.287122, acc: 0.875000]\n",
            "1430: [Discriminator loss: 0.564393, acc: 0.597656]  [Adversarial loss: 0.272853, acc: 0.910156]\n",
            "1431: [Discriminator loss: 0.552710, acc: 0.570312]  [Adversarial loss: 0.244796, acc: 0.957031]\n",
            "1432: [Discriminator loss: 0.508699, acc: 0.609375]  [Adversarial loss: 0.242150, acc: 0.941406]\n",
            "1433: [Discriminator loss: 0.526791, acc: 0.597656]  [Adversarial loss: 0.273181, acc: 0.898438]\n",
            "1434: [Discriminator loss: 0.616954, acc: 0.558594]  [Adversarial loss: 0.294417, acc: 0.925781]\n",
            "1435: [Discriminator loss: 0.646893, acc: 0.503906]  [Adversarial loss: 0.308550, acc: 0.894531]\n",
            "1436: [Discriminator loss: 0.589416, acc: 0.574219]  [Adversarial loss: 0.261793, acc: 0.906250]\n",
            "1437: [Discriminator loss: 0.553552, acc: 0.582031]  [Adversarial loss: 0.313466, acc: 0.902344]\n",
            "1438: [Discriminator loss: 0.622500, acc: 0.531250]  [Adversarial loss: 0.294931, acc: 0.882812]\n",
            "1439: [Discriminator loss: 0.561158, acc: 0.574219]  [Adversarial loss: 0.274076, acc: 0.929688]\n",
            "1440: [Discriminator loss: 0.604299, acc: 0.546875]  [Adversarial loss: 0.307801, acc: 0.902344]\n",
            "1441: [Discriminator loss: 0.598474, acc: 0.527344]  [Adversarial loss: 0.300152, acc: 0.882812]\n",
            "1442: [Discriminator loss: 0.624137, acc: 0.546875]  [Adversarial loss: 0.292582, acc: 0.906250]\n",
            "1443: [Discriminator loss: 0.531994, acc: 0.597656]  [Adversarial loss: 0.267357, acc: 0.898438]\n",
            "1444: [Discriminator loss: 0.537404, acc: 0.566406]  [Adversarial loss: 0.284480, acc: 0.890625]\n",
            "1445: [Discriminator loss: 0.561149, acc: 0.496094]  [Adversarial loss: 0.300304, acc: 0.890625]\n",
            "1446: [Discriminator loss: 0.508127, acc: 0.597656]  [Adversarial loss: 0.242607, acc: 0.910156]\n",
            "1447: [Discriminator loss: 0.570265, acc: 0.550781]  [Adversarial loss: 0.281803, acc: 0.882812]\n",
            "1448: [Discriminator loss: 0.660437, acc: 0.546875]  [Adversarial loss: 0.289515, acc: 0.886719]\n",
            "1449: [Discriminator loss: 0.520895, acc: 0.593750]  [Adversarial loss: 0.243674, acc: 0.925781]\n",
            "1450: [Discriminator loss: 0.547973, acc: 0.589844]  [Adversarial loss: 0.290564, acc: 0.882812]\n",
            "1451: [Discriminator loss: 0.513481, acc: 0.589844]  [Adversarial loss: 0.262212, acc: 0.917969]\n",
            "1452: [Discriminator loss: 0.574085, acc: 0.546875]  [Adversarial loss: 0.269914, acc: 0.910156]\n",
            "1453: [Discriminator loss: 0.620216, acc: 0.531250]  [Adversarial loss: 0.287439, acc: 0.910156]\n",
            "1454: [Discriminator loss: 0.566450, acc: 0.546875]  [Adversarial loss: 0.267940, acc: 0.945312]\n",
            "1455: [Discriminator loss: 0.586302, acc: 0.562500]  [Adversarial loss: 0.271118, acc: 0.921875]\n",
            "1456: [Discriminator loss: 0.599607, acc: 0.554688]  [Adversarial loss: 0.275278, acc: 0.906250]\n",
            "1457: [Discriminator loss: 0.587110, acc: 0.585938]  [Adversarial loss: 0.275141, acc: 0.910156]\n",
            "1458: [Discriminator loss: 0.550566, acc: 0.578125]  [Adversarial loss: 0.270449, acc: 0.882812]\n",
            "1459: [Discriminator loss: 0.532873, acc: 0.582031]  [Adversarial loss: 0.264867, acc: 0.898438]\n",
            "1460: [Discriminator loss: 0.612131, acc: 0.550781]  [Adversarial loss: 0.273034, acc: 0.929688]\n",
            "1461: [Discriminator loss: 0.589931, acc: 0.566406]  [Adversarial loss: 0.265915, acc: 0.925781]\n",
            "1462: [Discriminator loss: 0.592508, acc: 0.578125]  [Adversarial loss: 0.267510, acc: 0.921875]\n",
            "1463: [Discriminator loss: 0.582752, acc: 0.570312]  [Adversarial loss: 0.308141, acc: 0.875000]\n",
            "1464: [Discriminator loss: 0.535275, acc: 0.613281]  [Adversarial loss: 0.259516, acc: 0.902344]\n",
            "1465: [Discriminator loss: 0.514758, acc: 0.628906]  [Adversarial loss: 0.237677, acc: 0.929688]\n",
            "1466: [Discriminator loss: 0.614817, acc: 0.542969]  [Adversarial loss: 0.289483, acc: 0.933594]\n",
            "1467: [Discriminator loss: 0.569284, acc: 0.609375]  [Adversarial loss: 0.262300, acc: 0.910156]\n",
            "1468: [Discriminator loss: 0.514719, acc: 0.578125]  [Adversarial loss: 0.278896, acc: 0.894531]\n",
            "1469: [Discriminator loss: 0.529174, acc: 0.621094]  [Adversarial loss: 0.249490, acc: 0.910156]\n",
            "1470: [Discriminator loss: 0.540329, acc: 0.621094]  [Adversarial loss: 0.242978, acc: 0.906250]\n",
            "1471: [Discriminator loss: 0.549474, acc: 0.593750]  [Adversarial loss: 0.252710, acc: 0.917969]\n",
            "1472: [Discriminator loss: 0.598781, acc: 0.554688]  [Adversarial loss: 0.284960, acc: 0.898438]\n",
            "1473: [Discriminator loss: 0.557371, acc: 0.578125]  [Adversarial loss: 0.274019, acc: 0.917969]\n",
            "1474: [Discriminator loss: 0.613655, acc: 0.558594]  [Adversarial loss: 0.264163, acc: 0.890625]\n",
            "1475: [Discriminator loss: 0.586926, acc: 0.546875]  [Adversarial loss: 0.315024, acc: 0.878906]\n",
            "1476: [Discriminator loss: 0.616817, acc: 0.515625]  [Adversarial loss: 0.276631, acc: 0.933594]\n",
            "1477: [Discriminator loss: 0.489033, acc: 0.617188]  [Adversarial loss: 0.241884, acc: 0.914062]\n",
            "1478: [Discriminator loss: 0.606426, acc: 0.554688]  [Adversarial loss: 0.288521, acc: 0.898438]\n",
            "1479: [Discriminator loss: 0.576853, acc: 0.605469]  [Adversarial loss: 0.255705, acc: 0.906250]\n",
            "1480: [Discriminator loss: 0.575382, acc: 0.570312]  [Adversarial loss: 0.292608, acc: 0.890625]\n",
            "1481: [Discriminator loss: 0.575330, acc: 0.570312]  [Adversarial loss: 0.270830, acc: 0.921875]\n",
            "1482: [Discriminator loss: 0.597975, acc: 0.574219]  [Adversarial loss: 0.285353, acc: 0.882812]\n",
            "1483: [Discriminator loss: 0.583842, acc: 0.585938]  [Adversarial loss: 0.266108, acc: 0.910156]\n",
            "1484: [Discriminator loss: 0.634992, acc: 0.539062]  [Adversarial loss: 0.297745, acc: 0.878906]\n",
            "1485: [Discriminator loss: 0.604078, acc: 0.542969]  [Adversarial loss: 0.298770, acc: 0.906250]\n",
            "1486: [Discriminator loss: 0.532794, acc: 0.550781]  [Adversarial loss: 0.272602, acc: 0.914062]\n",
            "1487: [Discriminator loss: 0.562579, acc: 0.601562]  [Adversarial loss: 0.283738, acc: 0.910156]\n",
            "1488: [Discriminator loss: 0.594034, acc: 0.550781]  [Adversarial loss: 0.274344, acc: 0.929688]\n",
            "1489: [Discriminator loss: 0.527699, acc: 0.609375]  [Adversarial loss: 0.265522, acc: 0.851562]\n",
            "1490: [Discriminator loss: 0.579437, acc: 0.558594]  [Adversarial loss: 0.295899, acc: 0.890625]\n",
            "1491: [Discriminator loss: 0.487784, acc: 0.632812]  [Adversarial loss: 0.264188, acc: 0.890625]\n",
            "1492: [Discriminator loss: 0.614620, acc: 0.511719]  [Adversarial loss: 0.336733, acc: 0.851562]\n",
            "1493: [Discriminator loss: 0.580723, acc: 0.570312]  [Adversarial loss: 0.293916, acc: 0.878906]\n",
            "1494: [Discriminator loss: 0.597046, acc: 0.589844]  [Adversarial loss: 0.276885, acc: 0.894531]\n",
            "1495: [Discriminator loss: 0.586646, acc: 0.578125]  [Adversarial loss: 0.285918, acc: 0.906250]\n",
            "1496: [Discriminator loss: 0.581290, acc: 0.593750]  [Adversarial loss: 0.274514, acc: 0.882812]\n",
            "1497: [Discriminator loss: 0.457836, acc: 0.652344]  [Adversarial loss: 0.213740, acc: 0.941406]\n",
            "1498: [Discriminator loss: 0.487096, acc: 0.609375]  [Adversarial loss: 0.257394, acc: 0.906250]\n",
            "1499: [Discriminator loss: 0.591298, acc: 0.546875]  [Adversarial loss: 0.308052, acc: 0.882812]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jy90r3iq9X4"
      },
      "source": [
        "#### Plot Diabetes BBGAN Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "BgnmO7EnrBpK",
        "outputId": "90ee4187-00f2-4980-acca-500e7de0224d"
      },
      "source": [
        "plot_graphs(discriminator_losses, adversarial_losses, \"discriminator_loss\", \"adversarial_loss\", \"Discriminator and adversarial losses\", \"steps\", \"loss\")\n",
        "plot_graphs(discriminator_accuracies, adversarial_accuracies, \"discriminator_accuracy\", \"adversarial_accuracy\", \"Discriminator and adversarial accuracies\", \"steps\", \"accuracy\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McuB88ugtCEZ"
      },
      "source": [
        "#### Generate Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "TAByOAvU3eb-",
        "outputId": "6868a5fd-df0b-4d92-b161-3b1b048ef407"
      },
      "source": [
        "gen_samples, c = diabetes_bbgan.generate_samples(num_samples=1000)\n",
        "descaled_gen_samples = diabetes_scaler.inverse_transform(gen_samples)\n",
        "descaled_gen_samples = pd.DataFrame(descaled_gen_samples)\n",
        "\n",
        "descaled_gen_samples\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>122.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.078</td>\n",
              "      <td>21.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17.0</td>\n",
              "      <td>1.481539e-23</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1.886910e-24</td>\n",
              "      <td>3.253154e-23</td>\n",
              "      <td>2.822176e-25</td>\n",
              "      <td>0.078</td>\n",
              "      <td>21.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>122.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.078</td>\n",
              "      <td>21.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>122.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.078</td>\n",
              "      <td>21.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>122.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.078</td>\n",
              "      <td>21.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>122.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.078</td>\n",
              "      <td>21.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>122.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.078</td>\n",
              "      <td>21.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>17.0</td>\n",
              "      <td>9.837843e-07</td>\n",
              "      <td>122.0</td>\n",
              "      <td>2.993918e-07</td>\n",
              "      <td>2.874381e-06</td>\n",
              "      <td>1.209741e-07</td>\n",
              "      <td>0.078</td>\n",
              "      <td>21.00001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>122.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.078</td>\n",
              "      <td>21.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>122.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.078</td>\n",
              "      <td>21.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0             1      2  ...             5      6         7\n",
              "0    17.0  0.000000e+00  122.0  ...  0.000000e+00  0.078  21.00000\n",
              "1    17.0  1.481539e-23  122.0  ...  2.822176e-25  0.078  21.00000\n",
              "2    17.0  0.000000e+00  122.0  ...  0.000000e+00  0.078  21.00000\n",
              "3    17.0  0.000000e+00  122.0  ...  0.000000e+00  0.078  21.00000\n",
              "4    17.0  0.000000e+00  122.0  ...  0.000000e+00  0.078  21.00000\n",
              "..    ...           ...    ...  ...           ...    ...       ...\n",
              "995  17.0  0.000000e+00  122.0  ...  0.000000e+00  0.078  21.00000\n",
              "996  17.0  0.000000e+00  122.0  ...  0.000000e+00  0.078  21.00000\n",
              "997  17.0  9.837843e-07  122.0  ...  1.209741e-07  0.078  21.00001\n",
              "998  17.0  0.000000e+00  122.0  ...  0.000000e+00  0.078  21.00000\n",
              "999  17.0  0.000000e+00  122.0  ...  0.000000e+00  0.078  21.00000\n",
              "\n",
              "[1000 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "b5vAhqxPS1tW",
        "outputId": "6a516d97-b808-4010-8356-4c1b19f731f8"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "principalComponentsOriginal = pca.fit_transform(diabetes_data)\n",
        "originalPrincipalDf = pd.DataFrame(data = principalComponentsOriginal, columns = ['principal component 1', 'principal component 2'])\n",
        "principalComponents = pca.transform(descaled_gen_samples)\n",
        "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
        "principalDf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>principal component 1</th>\n",
              "      <th>principal component 2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-91.902520</td>\n",
              "      <td>104.210165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-91.902520</td>\n",
              "      <td>104.210165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-91.902520</td>\n",
              "      <td>104.210165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-91.902520</td>\n",
              "      <td>104.210165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-91.902520</td>\n",
              "      <td>104.210165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>-91.902520</td>\n",
              "      <td>104.210165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>-91.902520</td>\n",
              "      <td>104.210165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>-91.902517</td>\n",
              "      <td>104.210163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>-91.902520</td>\n",
              "      <td>104.210165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>-91.902520</td>\n",
              "      <td>104.210165</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     principal component 1  principal component 2\n",
              "0               -91.902520             104.210165\n",
              "1               -91.902520             104.210165\n",
              "2               -91.902520             104.210165\n",
              "3               -91.902520             104.210165\n",
              "4               -91.902520             104.210165\n",
              "..                     ...                    ...\n",
              "995             -91.902520             104.210165\n",
              "996             -91.902520             104.210165\n",
              "997             -91.902517             104.210163\n",
              "998             -91.902520             104.210165\n",
              "999             -91.902520             104.210165\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmnXfWFrNC9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae34843c-833f-42d2-e701-dd4329106088"
      },
      "source": [
        "y = diabetes_rf_clf.predict_proba(gen_samples)[:, 1]\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.06,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.05, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.06, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  ,\n",
              "       0.04, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "UvdRAk8BV5-5",
        "outputId": "28ca5ce0-8bd1-489a-f6a2-3551ab4aa169"
      },
      "source": [
        "plt.hist(y, rwidth=0.8)\n",
        "plt.xlabel(\"Confidence\")\n",
        "plt.ylabel(\"Number of cases\")\n",
        "plt.title(\"Confidence scores distribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-38vrFtTTk_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e31bfcf9-ff50-4c0e-880c-7cd3d4f1ad3c"
      },
      "source": [
        "diabetes_rf_clf.predict_proba(diabetes_data)[:, 1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.91, 0.05, 0.67, 0.  , 0.86, 0.04, 0.03, 0.45, 0.92, 0.64, 0.19,\n",
              "       0.93, 0.12, 0.86, 0.77, 0.72, 0.89, 0.88, 0.14, 0.8 , 0.12, 0.12,\n",
              "       0.97, 0.38, 0.67, 0.76, 0.9 , 0.02, 0.27, 0.2 , 0.66, 0.71, 0.02,\n",
              "       0.05, 0.2 , 0.1 , 0.28, 0.85, 0.66, 0.52, 0.33, 0.81, 0.06, 0.99,\n",
              "       0.69, 0.93, 0.19, 0.  , 0.81, 0.14, 0.01, 0.02, 0.03, 0.99, 0.73,\n",
              "       0.01, 0.79, 0.18, 0.33, 0.09, 0.02, 0.87, 0.04, 0.26, 0.9 , 0.37,\n",
              "       0.46, 0.44, 0.  , 0.33, 0.04, 0.18, 0.46, 0.28, 0.02, 0.  , 0.12,\n",
              "       0.1 , 0.74, 0.  , 0.05, 0.01, 0.28, 0.  , 0.54, 0.1 , 0.32, 0.01,\n",
              "       0.77, 0.  , 0.  , 0.17, 0.08, 0.59, 0.08, 0.26, 0.02, 0.01, 0.02,\n",
              "       0.86, 0.94, 0.25, 0.05, 0.  , 0.05, 0.04, 0.04, 0.13, 0.05, 0.05,\n",
              "       0.58, 0.93, 0.03, 0.05, 0.94, 0.86, 0.88, 0.08, 0.01, 0.01, 0.77,\n",
              "       0.08, 0.06, 0.11, 0.67, 0.73, 0.14, 0.03, 0.79, 0.71, 0.92, 0.72,\n",
              "       0.8 , 0.47, 0.03, 0.42, 0.03, 0.01, 0.23, 0.04, 0.15, 0.2 , 0.02,\n",
              "       0.83, 0.05, 0.  , 0.08, 0.12, 0.68, 0.01, 0.17, 0.08, 0.94, 0.22,\n",
              "       0.89, 0.7 , 0.01, 0.  , 0.03, 0.9 , 0.18, 0.16, 0.1 , 0.  , 0.64,\n",
              "       0.4 , 0.12, 0.22, 0.28, 0.17, 0.77, 0.73, 0.05, 0.12, 0.19, 0.99,\n",
              "       0.09, 0.79, 0.23, 0.87, 0.02, 0.19, 0.  , 0.04, 0.16, 0.96, 0.96,\n",
              "       0.86, 0.76, 0.73, 0.01, 0.19, 0.82, 0.8 , 0.05, 0.87, 0.  , 0.62,\n",
              "       0.21, 0.26, 0.09, 0.21, 0.11, 0.02, 0.58, 0.02, 0.98, 0.84, 0.02,\n",
              "       0.87, 0.02, 0.7 , 0.79, 0.4 , 0.87, 0.8 , 0.67, 0.17, 0.3 , 0.81,\n",
              "       0.74, 0.92, 0.1 , 0.22, 0.  , 0.04, 0.1 , 0.65, 0.3 , 0.09, 0.83,\n",
              "       0.57, 0.  , 0.17, 0.03, 0.86, 0.99, 0.92, 0.93, 0.05, 0.  , 0.02,\n",
              "       0.54, 0.64, 0.58, 0.95, 0.35, 0.67, 0.28, 0.05, 0.65, 0.09, 0.  ,\n",
              "       0.05, 0.33, 0.65, 0.15, 0.04, 0.14, 0.83, 0.8 , 0.87, 0.03, 0.23,\n",
              "       0.63, 0.42, 0.89, 0.26, 0.  , 0.81, 0.9 , 0.  , 0.1 , 0.09, 0.14,\n",
              "       0.21, 0.63, 0.01, 0.06, 0.02, 0.85, 0.51, 0.23, 0.91, 0.83, 0.18,\n",
              "       0.81, 0.75, 0.  , 0.08, 0.1 , 0.12, 0.63, 0.69, 0.45, 0.21, 0.44,\n",
              "       0.1 , 0.83, 0.14, 0.82, 0.64, 0.2 , 0.77, 0.23, 0.08, 0.65, 0.07,\n",
              "       0.72, 0.38, 0.1 , 0.1 , 0.78, 0.07, 0.72, 0.02, 0.  , 0.96, 0.05,\n",
              "       0.56, 0.17, 0.66, 0.71, 0.36, 0.2 , 0.16, 0.7 , 0.88, 0.23, 0.33,\n",
              "       0.07, 0.03, 0.96, 0.06, 0.  , 0.31, 0.7 , 0.81, 0.81, 0.96, 0.09,\n",
              "       0.03, 0.05, 0.13, 0.3 , 0.21, 0.26, 0.03, 0.  , 0.73, 0.18, 0.39,\n",
              "       0.21, 0.01, 0.06, 0.8 , 0.41, 0.74, 0.16, 0.95, 0.91, 0.71, 0.39,\n",
              "       0.8 , 0.21, 0.03, 0.67, 0.  , 0.02, 0.84, 0.88, 0.05, 0.01, 0.03,\n",
              "       0.23, 0.75, 0.01, 0.07, 0.93, 0.2 , 0.02, 0.  , 0.03, 0.1 , 0.03,\n",
              "       0.03, 0.81, 0.74, 0.77, 0.05, 0.15, 0.94, 0.05, 0.07, 0.86, 0.19,\n",
              "       0.1 , 0.71, 0.  , 0.9 , 0.74, 0.12, 0.84, 0.04, 0.82, 0.4 , 0.9 ,\n",
              "       0.  , 0.82, 0.79, 0.09, 0.32, 0.22, 0.03, 0.82, 0.86, 0.01, 0.95,\n",
              "       0.01, 0.62, 0.2 , 0.  , 0.12, 0.02, 0.97, 0.79, 0.01, 0.94, 0.55,\n",
              "       0.74, 0.01, 0.21, 0.08, 0.25, 0.03, 0.86, 0.2 , 0.25, 0.  , 0.2 ,\n",
              "       0.96, 0.02, 0.05, 0.7 , 0.67, 0.9 , 0.03, 0.06, 0.14, 0.14, 0.02,\n",
              "       0.3 , 0.07, 0.05, 0.03, 0.84, 0.32, 0.03, 0.93, 0.07, 0.02, 0.  ,\n",
              "       0.29, 0.08, 0.1 , 0.04, 0.  , 0.04, 0.62, 0.36, 0.56, 0.14, 0.12,\n",
              "       0.55, 0.06, 0.12, 0.83, 0.09, 0.03, 0.13, 0.82, 0.29, 0.06, 0.1 ,\n",
              "       0.95, 0.74, 0.56, 0.83, 0.01, 0.2 , 0.05, 0.09, 0.09, 0.83, 0.01,\n",
              "       0.23, 0.05, 0.04, 0.84, 0.32, 0.01, 0.06, 0.83, 0.22, 0.11, 0.05,\n",
              "       0.92, 0.09, 0.02, 0.08, 0.66, 0.12, 0.13, 0.02, 0.05, 0.84, 0.74,\n",
              "       0.19, 0.15, 0.27, 0.01, 0.09, 0.03, 0.92, 0.12, 0.01, 0.  , 0.04,\n",
              "       0.14, 0.12, 0.04, 0.08, 0.09, 0.11, 0.13, 0.41, 0.19, 0.02, 0.36,\n",
              "       0.86, 0.9 , 0.73, 0.79, 0.04, 0.11, 0.99, 0.98, 0.12, 0.72, 0.64,\n",
              "       0.01, 0.02, 0.07, 0.01, 0.08, 0.12, 0.03, 0.21, 0.23, 0.3 , 0.8 ,\n",
              "       0.9 , 0.01, 0.05, 0.05, 0.  , 0.02, 0.13, 0.59, 0.83, 0.17, 0.08,\n",
              "       0.07, 0.  , 0.16, 0.08, 0.13, 0.77, 0.2 , 0.78, 0.86, 0.03, 0.18,\n",
              "       0.48, 0.81, 0.  , 0.86, 0.1 , 0.95, 0.01, 0.88, 0.02, 0.79, 0.04,\n",
              "       0.57, 0.77, 0.12, 0.02, 0.93, 0.03, 0.02, 0.03, 0.06, 0.83, 0.66,\n",
              "       0.36, 0.74, 0.01, 0.42, 0.01, 0.02, 0.97, 0.97, 0.08, 0.86, 0.06,\n",
              "       0.15, 0.  , 0.5 , 0.27, 0.21, 0.02, 0.31, 0.04, 0.1 , 0.07, 0.01,\n",
              "       0.17, 0.21, 0.  , 0.75, 0.05, 0.02, 0.05, 0.05, 0.48, 0.49, 0.1 ,\n",
              "       0.83, 0.02, 0.13, 0.18, 0.79, 0.04, 0.1 , 0.27, 0.81, 0.9 , 0.44,\n",
              "       0.01, 0.  , 0.01, 0.07, 0.05, 0.01, 0.81, 0.  , 0.32, 0.13, 0.15,\n",
              "       0.31, 0.97, 0.85, 0.92, 0.75, 0.03, 0.75, 0.33, 0.51, 0.29, 0.31,\n",
              "       0.01, 0.07, 0.45, 0.16, 0.94, 0.79, 0.1 , 0.73, 0.03, 0.  , 0.93,\n",
              "       0.05, 0.73, 0.09, 0.49, 0.09, 0.04, 0.03, 0.95, 0.2 , 0.65, 0.16,\n",
              "       0.91, 0.01, 0.9 , 0.9 , 0.02, 0.15, 0.2 , 0.04, 0.78, 1.  , 0.84,\n",
              "       0.02, 0.05, 0.23, 0.07, 0.9 , 0.66, 0.13, 0.18, 0.87, 0.06, 0.47,\n",
              "       0.92, 0.89, 0.05, 0.18, 0.36, 0.2 , 0.1 , 0.83, 0.14, 0.15, 0.41,\n",
              "       0.11, 0.17, 0.49, 0.04, 0.74, 0.32, 0.95, 0.02, 0.23, 0.04, 0.03,\n",
              "       0.12, 0.08, 0.8 , 0.89, 0.03, 0.01, 0.85, 0.43, 0.51, 0.77, 0.1 ,\n",
              "       0.97, 0.78, 0.77, 0.02, 0.01, 0.88, 0.71, 0.85, 0.2 , 0.82, 0.16,\n",
              "       0.92, 0.  , 0.97, 0.07, 0.5 , 0.2 , 0.03, 0.83, 0.01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "091Kk4JV5yhG"
      },
      "source": [
        "###German Credit Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoXk_mDk6Ses"
      },
      "source": [
        "####Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "5a9rG62j6QrL",
        "outputId": "93177797-6add-4ef0-d074-06d8d2e301e3"
      },
      "source": [
        "german_data_scaled"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.029412</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.050567</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.375</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.313690</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.053571</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.625</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.101574</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.535714</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.419941</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.464286</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.254209</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.607143</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.125</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>0.625</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.081765</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.382353</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.198470</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0.625</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.030483</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.339286</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.602941</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.087763</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0.375</td>\n",
              "      <td>0.602941</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.238032</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1         2    3     4         5  ...        16     17   18    19    20\n",
              "0    0.125  0.029412  0.1  0.05  0.050567  ...  0.333333  0.125  0.0  0.25  0.25\n",
              "1    0.375  0.647059  0.3  0.05  0.313690  ...  0.000000  0.125  0.0  0.75  0.25\n",
              "2    0.625  0.117647  0.1  0.15  0.101574  ...  0.000000  0.375  1.0  0.75  0.25\n",
              "3    0.125  0.558824  0.3  0.25  0.419941  ...  0.000000  0.125  1.0  0.75  0.25\n",
              "4    0.125  0.294118  0.5  0.35  0.254209  ...  0.333333  0.125  1.0  0.75  0.25\n",
              "..     ...       ...  ...   ...       ...  ...       ...    ...  ...   ...   ...\n",
              "995  0.625  0.117647  0.3  0.25  0.081765  ...  0.000000  0.375  0.0  0.75  0.25\n",
              "996  0.125  0.382353  0.3  0.45  0.198470  ...  0.000000  0.625  0.0  0.25  0.25\n",
              "997  0.625  0.117647  0.3  0.05  0.030483  ...  0.000000  0.125  0.0  0.75  0.25\n",
              "998  0.125  0.602941  0.3  0.05  0.087763  ...  0.000000  0.125  0.0  0.25  0.25\n",
              "999  0.375  0.602941  0.1  0.45  0.238032  ...  0.000000  0.125  0.0  0.75  0.25\n",
              "\n",
              "[1000 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad7fEuV27xTd"
      },
      "source": [
        "X = german_data_scaled\n",
        "y = german_data_class.astype('int')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlJBITpO8Bcr"
      },
      "source": [
        "####German Credit Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuSHPBqp8CPp",
        "outputId": "01fd576e-f8e8-4635-a3cf-8191bfa6184f"
      },
      "source": [
        "german_credit_rf_clf = RandomForestClassifier()\n",
        "german_credit_rf_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bamjr0At8LUi",
        "outputId": "887cfcbe-9935-4bbd-fdf9-434586efa2e1"
      },
      "source": [
        "german_credit_confidence = german_credit_rf_clf.predict_proba(X_test)[:, 1]\n",
        "print(german_credit_confidence)\n",
        "\n",
        "print(f\"max confidence score: {max(german_credit_confidence)}\")\n",
        "print(f\"min confidence score: {min(german_credit_confidence)}\")\n",
        "print(f\"avg confidence score: {np.mean(german_credit_confidence)}\")\n",
        "german_credit_pred = german_credit_rf_clf.predict(X_test)\n",
        "print(f\"german credit rf classifier accuracy: {accuracy_score(y_test, german_credit_pred)}\")\n",
        "\n",
        "\n",
        "german_credit_pred = german_credit_rf_clf.predict(X_test)\n",
        "print(f\"accuracy: {accuracy_score(y_test, german_credit_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.26 0.51 0.73 0.22 0.09 0.36 0.06 0.37 0.2  0.29 0.08 0.63 0.15 0.79\n",
            " 0.7  0.42 0.16 0.08 0.41 0.25 0.24 0.41 0.05 0.01 0.04 0.47 0.54 0.45\n",
            " 0.39 0.08 0.34 0.12 0.4  0.13 0.13 0.25 0.59 0.11 0.09 0.62 0.4  0.04\n",
            " 0.34 0.08 0.14 0.31 0.43 0.46 0.31 0.31 0.55 0.14 0.43 0.52 0.15 0.27\n",
            " 0.09 0.6  0.63 0.2  0.29 0.63 0.37 0.08 0.38 0.32 0.02 0.4  0.38 0.15\n",
            " 0.32 0.11 0.17 0.43 0.29 0.65 0.34 0.31 0.14 0.48 0.15 0.22 0.02 0.4\n",
            " 0.09 0.42 0.72 0.67 0.21 0.13 0.33 0.04 0.82 0.37 0.1  0.21 0.11 0.49\n",
            " 0.7  0.32 0.02 0.36 0.15 0.02 0.23 0.42 0.35 0.42 0.15 0.61 0.77 0.2\n",
            " 0.33 0.11 0.39 0.21 0.35 0.12 0.21 0.41 0.26 0.51 0.33 0.23 0.42 0.21\n",
            " 0.55 0.15 0.2  0.14 0.16 0.19 0.21 0.13 0.4  0.31 0.45 0.12 0.2  0.26\n",
            " 0.66 0.46 0.69 0.02 0.37 0.56 0.43 0.32 0.39 0.65 0.14 0.22 0.19 0.25\n",
            " 0.1  0.4  0.38 0.13 0.36 0.31 0.51 0.21 0.51 0.68 0.37 0.18 0.06 0.32\n",
            " 0.16 0.49 0.06 0.25 0.29 0.26 0.04 0.3  0.37 0.07 0.33 0.6  0.13 0.07\n",
            " 0.17 0.09 0.16 0.05 0.56 0.04 0.27 0.07 0.23 0.59 0.45 0.38 0.23 0.06\n",
            " 0.62 0.53 0.62 0.24 0.66 0.12 0.16 0.3  0.37 0.7  0.08 0.39 0.44 0.12\n",
            " 0.31 0.64 0.67 0.34 0.29 0.08 0.1  0.39 0.36 0.33 0.03 0.1  0.19 0.25\n",
            " 0.52 0.05 0.03 0.35 0.07 0.36 0.22 0.04 0.52 0.27 0.2  0.47 0.15 0.11\n",
            " 0.23 0.46 0.59 0.07 0.16 0.33 0.29 0.25 0.12 0.31 0.09 0.82 0.27 0.19\n",
            " 0.6  0.07 0.3  0.41 0.12 0.17 0.49 0.3  0.34 0.38 0.17 0.14 0.26 0.2\n",
            " 0.15 0.46 0.62 0.58 0.06 0.32 0.21 0.13 0.16 0.75 0.06 0.32 0.15 0.57\n",
            " 0.49 0.18 0.19 0.29 0.08 0.1  0.59 0.19 0.43 0.29 0.53 0.37 0.63 0.28\n",
            " 0.3  0.19 0.48 0.48 0.41 0.51]\n",
            "max confidence score: 0.82\n",
            "min confidence score: 0.01\n",
            "avg confidence score: 0.30590000000000006\n",
            "german credit rf classifier accuracy: 0.7633333333333333\n",
            "accuracy: 0.7633333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "ij47tdbIC5hX",
        "outputId": "ed4cd7d4-457c-4dd7-b3b5-3ebdbd9aafec"
      },
      "source": [
        "plt.hist(german_credit_confidence, rwidth=0.8)\n",
        "plt.xlabel(\"Confidence\")\n",
        "plt.ylabel(\"Number of cases\")\n",
        "plt.title(\"Confidence scores distribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkU4PBV38eZl"
      },
      "source": [
        "####German Credit BBGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjdAh9jH8hII",
        "outputId": "5ba54fbb-42d3-41a0-9da6-8043ce62cf90"
      },
      "source": [
        "german_credit_bbgan = IMPLEMENTED_BBGAN(german_credit_rf_clf, german_data_scaled.shape[1], noise_dim=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator summary\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_17 (InputLayer)           [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_18 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_19 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 22)           0           input_17[0][0]                   \n",
            "                                                                 input_18[0][0]                   \n",
            "                                                                 input_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 256)          5888        concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 256)          0           dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 256)          65792       dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 256)          0           dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 128)          32896       dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 128)          0           dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 1)            129         dropout_14[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 104,705\n",
            "Trainable params: 104,705\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Generator summary\n",
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_23 (InputLayer)           [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_24 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 4)            0           input_23[0][0]                   \n",
            "                                                                 input_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 128)          640         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 128)          0           dense_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 256)          33024       dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 256)          0           dense_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, 256)          65792       dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 256)          0           dense_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "samples_generator_layer (Dense) (None, 20)           5140        dropout_17[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 104,596\n",
            "Trainable params: 104,596\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Adversarial model summary:\n",
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_20 (InputLayer)           [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_21 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_7 (Functional)            (None, 20)           104596      input_20[0][0]                   \n",
            "                                                                 input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_22 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_6 (Functional)            (None, 1)            104705      model_7[0][0]                    \n",
            "                                                                 input_21[0][0]                   \n",
            "                                                                 input_22[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 209,301\n",
            "Trainable params: 209,301\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qqJbXx819HMg",
        "outputId": "eff786ff-cbd5-46ae-a48b-2fc9d0a8f2c4"
      },
      "source": [
        "discriminator_losses, discriminator_accuracies, adversarial_losses, adversarial_accuracies = german_credit_bbgan.train(train_steps=2500, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: [Discriminator loss: 0.724775, acc: 0.492188]  [Adversarial loss: 0.713363, acc: 0.503906]\n",
            "1: [Discriminator loss: 0.714851, acc: 0.496094]  [Adversarial loss: 0.699184, acc: 0.496094]\n",
            "2: [Discriminator loss: 0.713965, acc: 0.445312]  [Adversarial loss: 0.703431, acc: 0.480469]\n",
            "3: [Discriminator loss: 0.701836, acc: 0.476562]  [Adversarial loss: 0.707356, acc: 0.460938]\n",
            "4: [Discriminator loss: 0.693450, acc: 0.527344]  [Adversarial loss: 0.695158, acc: 0.496094]\n",
            "5: [Discriminator loss: 0.695526, acc: 0.519531]  [Adversarial loss: 0.691543, acc: 0.519531]\n",
            "6: [Discriminator loss: 0.693185, acc: 0.531250]  [Adversarial loss: 0.706708, acc: 0.507812]\n",
            "7: [Discriminator loss: 0.696763, acc: 0.503906]  [Adversarial loss: 0.697774, acc: 0.507812]\n",
            "8: [Discriminator loss: 0.702942, acc: 0.441406]  [Adversarial loss: 0.694603, acc: 0.503906]\n",
            "9: [Discriminator loss: 0.695849, acc: 0.562500]  [Adversarial loss: 0.709989, acc: 0.421875]\n",
            "10: [Discriminator loss: 0.693338, acc: 0.492188]  [Adversarial loss: 0.690713, acc: 0.527344]\n",
            "11: [Discriminator loss: 0.697000, acc: 0.503906]  [Adversarial loss: 0.701364, acc: 0.468750]\n",
            "12: [Discriminator loss: 0.698491, acc: 0.460938]  [Adversarial loss: 0.697041, acc: 0.480469]\n",
            "13: [Discriminator loss: 0.695504, acc: 0.457031]  [Adversarial loss: 0.701495, acc: 0.484375]\n",
            "14: [Discriminator loss: 0.698404, acc: 0.511719]  [Adversarial loss: 0.694166, acc: 0.535156]\n",
            "15: [Discriminator loss: 0.697052, acc: 0.511719]  [Adversarial loss: 0.693423, acc: 0.527344]\n",
            "16: [Discriminator loss: 0.686980, acc: 0.527344]  [Adversarial loss: 0.687013, acc: 0.542969]\n",
            "17: [Discriminator loss: 0.701432, acc: 0.460938]  [Adversarial loss: 0.694621, acc: 0.488281]\n",
            "18: [Discriminator loss: 0.693682, acc: 0.503906]  [Adversarial loss: 0.688505, acc: 0.554688]\n",
            "19: [Discriminator loss: 0.696560, acc: 0.496094]  [Adversarial loss: 0.690456, acc: 0.503906]\n",
            "20: [Discriminator loss: 0.705072, acc: 0.472656]  [Adversarial loss: 0.698919, acc: 0.503906]\n",
            "21: [Discriminator loss: 0.695278, acc: 0.492188]  [Adversarial loss: 0.689943, acc: 0.562500]\n",
            "22: [Discriminator loss: 0.697866, acc: 0.468750]  [Adversarial loss: 0.689196, acc: 0.535156]\n",
            "23: [Discriminator loss: 0.699612, acc: 0.488281]  [Adversarial loss: 0.696505, acc: 0.523438]\n",
            "24: [Discriminator loss: 0.693729, acc: 0.542969]  [Adversarial loss: 0.699882, acc: 0.445312]\n",
            "25: [Discriminator loss: 0.694124, acc: 0.519531]  [Adversarial loss: 0.697699, acc: 0.484375]\n",
            "26: [Discriminator loss: 0.692868, acc: 0.515625]  [Adversarial loss: 0.689643, acc: 0.578125]\n",
            "27: [Discriminator loss: 0.696930, acc: 0.488281]  [Adversarial loss: 0.695146, acc: 0.492188]\n",
            "28: [Discriminator loss: 0.689575, acc: 0.519531]  [Adversarial loss: 0.691159, acc: 0.546875]\n",
            "29: [Discriminator loss: 0.692946, acc: 0.531250]  [Adversarial loss: 0.699876, acc: 0.496094]\n",
            "30: [Discriminator loss: 0.694321, acc: 0.507812]  [Adversarial loss: 0.688635, acc: 0.531250]\n",
            "31: [Discriminator loss: 0.697922, acc: 0.480469]  [Adversarial loss: 0.695162, acc: 0.500000]\n",
            "32: [Discriminator loss: 0.698963, acc: 0.457031]  [Adversarial loss: 0.701885, acc: 0.460938]\n",
            "33: [Discriminator loss: 0.698995, acc: 0.472656]  [Adversarial loss: 0.691533, acc: 0.496094]\n",
            "34: [Discriminator loss: 0.691859, acc: 0.492188]  [Adversarial loss: 0.697463, acc: 0.457031]\n",
            "35: [Discriminator loss: 0.697726, acc: 0.468750]  [Adversarial loss: 0.694887, acc: 0.523438]\n",
            "36: [Discriminator loss: 0.700609, acc: 0.457031]  [Adversarial loss: 0.701630, acc: 0.457031]\n",
            "37: [Discriminator loss: 0.692294, acc: 0.507812]  [Adversarial loss: 0.699727, acc: 0.464844]\n",
            "38: [Discriminator loss: 0.694883, acc: 0.488281]  [Adversarial loss: 0.688491, acc: 0.503906]\n",
            "39: [Discriminator loss: 0.697193, acc: 0.476562]  [Adversarial loss: 0.691421, acc: 0.546875]\n",
            "40: [Discriminator loss: 0.696065, acc: 0.484375]  [Adversarial loss: 0.687964, acc: 0.527344]\n",
            "41: [Discriminator loss: 0.694828, acc: 0.480469]  [Adversarial loss: 0.696588, acc: 0.515625]\n",
            "42: [Discriminator loss: 0.696576, acc: 0.503906]  [Adversarial loss: 0.701717, acc: 0.484375]\n",
            "43: [Discriminator loss: 0.694029, acc: 0.496094]  [Adversarial loss: 0.697267, acc: 0.468750]\n",
            "44: [Discriminator loss: 0.694427, acc: 0.484375]  [Adversarial loss: 0.694240, acc: 0.503906]\n",
            "45: [Discriminator loss: 0.691323, acc: 0.523438]  [Adversarial loss: 0.697613, acc: 0.496094]\n",
            "46: [Discriminator loss: 0.696143, acc: 0.492188]  [Adversarial loss: 0.690870, acc: 0.503906]\n",
            "47: [Discriminator loss: 0.689852, acc: 0.503906]  [Adversarial loss: 0.690002, acc: 0.542969]\n",
            "48: [Discriminator loss: 0.694485, acc: 0.507812]  [Adversarial loss: 0.693701, acc: 0.500000]\n",
            "49: [Discriminator loss: 0.692661, acc: 0.542969]  [Adversarial loss: 0.697777, acc: 0.507812]\n",
            "50: [Discriminator loss: 0.697987, acc: 0.472656]  [Adversarial loss: 0.694280, acc: 0.484375]\n",
            "51: [Discriminator loss: 0.690800, acc: 0.523438]  [Adversarial loss: 0.694615, acc: 0.480469]\n",
            "52: [Discriminator loss: 0.693159, acc: 0.531250]  [Adversarial loss: 0.697555, acc: 0.476562]\n",
            "53: [Discriminator loss: 0.695500, acc: 0.468750]  [Adversarial loss: 0.699485, acc: 0.457031]\n",
            "54: [Discriminator loss: 0.694912, acc: 0.480469]  [Adversarial loss: 0.695480, acc: 0.484375]\n",
            "55: [Discriminator loss: 0.694568, acc: 0.519531]  [Adversarial loss: 0.699268, acc: 0.464844]\n",
            "56: [Discriminator loss: 0.698852, acc: 0.449219]  [Adversarial loss: 0.696522, acc: 0.480469]\n",
            "57: [Discriminator loss: 0.701531, acc: 0.453125]  [Adversarial loss: 0.693609, acc: 0.542969]\n",
            "58: [Discriminator loss: 0.697568, acc: 0.425781]  [Adversarial loss: 0.696265, acc: 0.488281]\n",
            "59: [Discriminator loss: 0.692098, acc: 0.511719]  [Adversarial loss: 0.693258, acc: 0.503906]\n",
            "60: [Discriminator loss: 0.695006, acc: 0.445312]  [Adversarial loss: 0.692803, acc: 0.539062]\n",
            "61: [Discriminator loss: 0.694496, acc: 0.457031]  [Adversarial loss: 0.693706, acc: 0.457031]\n",
            "62: [Discriminator loss: 0.696455, acc: 0.496094]  [Adversarial loss: 0.690811, acc: 0.531250]\n",
            "63: [Discriminator loss: 0.693334, acc: 0.503906]  [Adversarial loss: 0.693610, acc: 0.472656]\n",
            "64: [Discriminator loss: 0.695177, acc: 0.468750]  [Adversarial loss: 0.691871, acc: 0.550781]\n",
            "65: [Discriminator loss: 0.695903, acc: 0.519531]  [Adversarial loss: 0.694491, acc: 0.507812]\n",
            "66: [Discriminator loss: 0.696879, acc: 0.468750]  [Adversarial loss: 0.697535, acc: 0.484375]\n",
            "67: [Discriminator loss: 0.694709, acc: 0.460938]  [Adversarial loss: 0.693316, acc: 0.472656]\n",
            "68: [Discriminator loss: 0.694327, acc: 0.511719]  [Adversarial loss: 0.697006, acc: 0.449219]\n",
            "69: [Discriminator loss: 0.692388, acc: 0.507812]  [Adversarial loss: 0.695135, acc: 0.464844]\n",
            "70: [Discriminator loss: 0.693241, acc: 0.492188]  [Adversarial loss: 0.694814, acc: 0.492188]\n",
            "71: [Discriminator loss: 0.691332, acc: 0.531250]  [Adversarial loss: 0.693482, acc: 0.511719]\n",
            "72: [Discriminator loss: 0.691806, acc: 0.449219]  [Adversarial loss: 0.693564, acc: 0.531250]\n",
            "73: [Discriminator loss: 0.694204, acc: 0.472656]  [Adversarial loss: 0.694518, acc: 0.511719]\n",
            "74: [Discriminator loss: 0.693137, acc: 0.492188]  [Adversarial loss: 0.691927, acc: 0.531250]\n",
            "75: [Discriminator loss: 0.695043, acc: 0.476562]  [Adversarial loss: 0.693351, acc: 0.488281]\n",
            "76: [Discriminator loss: 0.694606, acc: 0.511719]  [Adversarial loss: 0.695466, acc: 0.445312]\n",
            "77: [Discriminator loss: 0.693865, acc: 0.515625]  [Adversarial loss: 0.693710, acc: 0.472656]\n",
            "78: [Discriminator loss: 0.696135, acc: 0.480469]  [Adversarial loss: 0.694832, acc: 0.472656]\n",
            "79: [Discriminator loss: 0.689781, acc: 0.515625]  [Adversarial loss: 0.691584, acc: 0.546875]\n",
            "80: [Discriminator loss: 0.695483, acc: 0.472656]  [Adversarial loss: 0.695357, acc: 0.464844]\n",
            "81: [Discriminator loss: 0.696774, acc: 0.480469]  [Adversarial loss: 0.695703, acc: 0.437500]\n",
            "82: [Discriminator loss: 0.693648, acc: 0.527344]  [Adversarial loss: 0.694719, acc: 0.468750]\n",
            "83: [Discriminator loss: 0.696215, acc: 0.468750]  [Adversarial loss: 0.691340, acc: 0.523438]\n",
            "84: [Discriminator loss: 0.692387, acc: 0.500000]  [Adversarial loss: 0.693943, acc: 0.449219]\n",
            "85: [Discriminator loss: 0.689647, acc: 0.554688]  [Adversarial loss: 0.693953, acc: 0.511719]\n",
            "86: [Discriminator loss: 0.694640, acc: 0.484375]  [Adversarial loss: 0.693040, acc: 0.515625]\n",
            "87: [Discriminator loss: 0.693413, acc: 0.496094]  [Adversarial loss: 0.695386, acc: 0.492188]\n",
            "88: [Discriminator loss: 0.690355, acc: 0.539062]  [Adversarial loss: 0.690816, acc: 0.546875]\n",
            "89: [Discriminator loss: 0.691858, acc: 0.519531]  [Adversarial loss: 0.695133, acc: 0.484375]\n",
            "90: [Discriminator loss: 0.691997, acc: 0.539062]  [Adversarial loss: 0.693826, acc: 0.523438]\n",
            "91: [Discriminator loss: 0.695085, acc: 0.488281]  [Adversarial loss: 0.689879, acc: 0.546875]\n",
            "92: [Discriminator loss: 0.693635, acc: 0.507812]  [Adversarial loss: 0.695123, acc: 0.460938]\n",
            "93: [Discriminator loss: 0.693946, acc: 0.464844]  [Adversarial loss: 0.694582, acc: 0.468750]\n",
            "94: [Discriminator loss: 0.695166, acc: 0.562500]  [Adversarial loss: 0.693121, acc: 0.527344]\n",
            "95: [Discriminator loss: 0.693258, acc: 0.503906]  [Adversarial loss: 0.688427, acc: 0.554688]\n",
            "96: [Discriminator loss: 0.697354, acc: 0.468750]  [Adversarial loss: 0.691804, acc: 0.503906]\n",
            "97: [Discriminator loss: 0.690402, acc: 0.550781]  [Adversarial loss: 0.696857, acc: 0.480469]\n",
            "98: [Discriminator loss: 0.694080, acc: 0.507812]  [Adversarial loss: 0.693577, acc: 0.480469]\n",
            "99: [Discriminator loss: 0.695327, acc: 0.523438]  [Adversarial loss: 0.694850, acc: 0.542969]\n",
            "100: [Discriminator loss: 0.696075, acc: 0.441406]  [Adversarial loss: 0.691372, acc: 0.507812]\n",
            "101: [Discriminator loss: 0.692690, acc: 0.488281]  [Adversarial loss: 0.696248, acc: 0.445312]\n",
            "102: [Discriminator loss: 0.693910, acc: 0.484375]  [Adversarial loss: 0.693553, acc: 0.539062]\n",
            "103: [Discriminator loss: 0.693966, acc: 0.511719]  [Adversarial loss: 0.694050, acc: 0.511719]\n",
            "104: [Discriminator loss: 0.693795, acc: 0.472656]  [Adversarial loss: 0.693123, acc: 0.546875]\n",
            "105: [Discriminator loss: 0.695549, acc: 0.464844]  [Adversarial loss: 0.692878, acc: 0.546875]\n",
            "106: [Discriminator loss: 0.694484, acc: 0.476562]  [Adversarial loss: 0.694432, acc: 0.488281]\n",
            "107: [Discriminator loss: 0.693609, acc: 0.503906]  [Adversarial loss: 0.693893, acc: 0.503906]\n",
            "108: [Discriminator loss: 0.693538, acc: 0.480469]  [Adversarial loss: 0.693316, acc: 0.515625]\n",
            "109: [Discriminator loss: 0.691516, acc: 0.523438]  [Adversarial loss: 0.692728, acc: 0.464844]\n",
            "110: [Discriminator loss: 0.691994, acc: 0.519531]  [Adversarial loss: 0.690868, acc: 0.539062]\n",
            "111: [Discriminator loss: 0.688833, acc: 0.562500]  [Adversarial loss: 0.692480, acc: 0.527344]\n",
            "112: [Discriminator loss: 0.693409, acc: 0.496094]  [Adversarial loss: 0.695146, acc: 0.468750]\n",
            "113: [Discriminator loss: 0.694388, acc: 0.441406]  [Adversarial loss: 0.693535, acc: 0.492188]\n",
            "114: [Discriminator loss: 0.694126, acc: 0.515625]  [Adversarial loss: 0.694425, acc: 0.472656]\n",
            "115: [Discriminator loss: 0.691306, acc: 0.527344]  [Adversarial loss: 0.694054, acc: 0.507812]\n",
            "116: [Discriminator loss: 0.690126, acc: 0.570312]  [Adversarial loss: 0.694113, acc: 0.492188]\n",
            "117: [Discriminator loss: 0.693265, acc: 0.511719]  [Adversarial loss: 0.694229, acc: 0.488281]\n",
            "118: [Discriminator loss: 0.691676, acc: 0.562500]  [Adversarial loss: 0.692046, acc: 0.519531]\n",
            "119: [Discriminator loss: 0.692986, acc: 0.523438]  [Adversarial loss: 0.696873, acc: 0.460938]\n",
            "120: [Discriminator loss: 0.693075, acc: 0.523438]  [Adversarial loss: 0.692792, acc: 0.500000]\n",
            "121: [Discriminator loss: 0.693761, acc: 0.507812]  [Adversarial loss: 0.697223, acc: 0.441406]\n",
            "122: [Discriminator loss: 0.691980, acc: 0.511719]  [Adversarial loss: 0.692456, acc: 0.585938]\n",
            "123: [Discriminator loss: 0.693998, acc: 0.484375]  [Adversarial loss: 0.694305, acc: 0.484375]\n",
            "124: [Discriminator loss: 0.695327, acc: 0.464844]  [Adversarial loss: 0.696527, acc: 0.445312]\n",
            "125: [Discriminator loss: 0.690738, acc: 0.539062]  [Adversarial loss: 0.694271, acc: 0.472656]\n",
            "126: [Discriminator loss: 0.695797, acc: 0.460938]  [Adversarial loss: 0.696530, acc: 0.488281]\n",
            "127: [Discriminator loss: 0.691513, acc: 0.550781]  [Adversarial loss: 0.695533, acc: 0.488281]\n",
            "128: [Discriminator loss: 0.693299, acc: 0.468750]  [Adversarial loss: 0.693543, acc: 0.503906]\n",
            "129: [Discriminator loss: 0.691666, acc: 0.535156]  [Adversarial loss: 0.693039, acc: 0.535156]\n",
            "130: [Discriminator loss: 0.692054, acc: 0.515625]  [Adversarial loss: 0.693259, acc: 0.496094]\n",
            "131: [Discriminator loss: 0.694951, acc: 0.507812]  [Adversarial loss: 0.692187, acc: 0.542969]\n",
            "132: [Discriminator loss: 0.693405, acc: 0.500000]  [Adversarial loss: 0.690757, acc: 0.515625]\n",
            "133: [Discriminator loss: 0.693173, acc: 0.542969]  [Adversarial loss: 0.692456, acc: 0.527344]\n",
            "134: [Discriminator loss: 0.696023, acc: 0.464844]  [Adversarial loss: 0.691163, acc: 0.535156]\n",
            "135: [Discriminator loss: 0.692324, acc: 0.546875]  [Adversarial loss: 0.695104, acc: 0.457031]\n",
            "136: [Discriminator loss: 0.692021, acc: 0.523438]  [Adversarial loss: 0.693877, acc: 0.480469]\n",
            "137: [Discriminator loss: 0.691507, acc: 0.503906]  [Adversarial loss: 0.692284, acc: 0.515625]\n",
            "138: [Discriminator loss: 0.695631, acc: 0.453125]  [Adversarial loss: 0.695911, acc: 0.464844]\n",
            "139: [Discriminator loss: 0.695098, acc: 0.476562]  [Adversarial loss: 0.694010, acc: 0.476562]\n",
            "140: [Discriminator loss: 0.692391, acc: 0.558594]  [Adversarial loss: 0.693769, acc: 0.484375]\n",
            "141: [Discriminator loss: 0.696395, acc: 0.453125]  [Adversarial loss: 0.692649, acc: 0.546875]\n",
            "142: [Discriminator loss: 0.694535, acc: 0.515625]  [Adversarial loss: 0.691035, acc: 0.523438]\n",
            "143: [Discriminator loss: 0.695727, acc: 0.457031]  [Adversarial loss: 0.692419, acc: 0.519531]\n",
            "144: [Discriminator loss: 0.697131, acc: 0.476562]  [Adversarial loss: 0.693256, acc: 0.515625]\n",
            "145: [Discriminator loss: 0.695394, acc: 0.492188]  [Adversarial loss: 0.690853, acc: 0.550781]\n",
            "146: [Discriminator loss: 0.693438, acc: 0.488281]  [Adversarial loss: 0.696121, acc: 0.433594]\n",
            "147: [Discriminator loss: 0.696475, acc: 0.480469]  [Adversarial loss: 0.692345, acc: 0.531250]\n",
            "148: [Discriminator loss: 0.693073, acc: 0.515625]  [Adversarial loss: 0.695091, acc: 0.496094]\n",
            "149: [Discriminator loss: 0.692369, acc: 0.531250]  [Adversarial loss: 0.692761, acc: 0.511719]\n",
            "150: [Discriminator loss: 0.693502, acc: 0.535156]  [Adversarial loss: 0.692347, acc: 0.531250]\n",
            "151: [Discriminator loss: 0.695313, acc: 0.460938]  [Adversarial loss: 0.690455, acc: 0.535156]\n",
            "152: [Discriminator loss: 0.692679, acc: 0.496094]  [Adversarial loss: 0.695160, acc: 0.488281]\n",
            "153: [Discriminator loss: 0.694726, acc: 0.496094]  [Adversarial loss: 0.691123, acc: 0.492188]\n",
            "154: [Discriminator loss: 0.698921, acc: 0.425781]  [Adversarial loss: 0.691020, acc: 0.542969]\n",
            "155: [Discriminator loss: 0.693551, acc: 0.503906]  [Adversarial loss: 0.694646, acc: 0.515625]\n",
            "156: [Discriminator loss: 0.693923, acc: 0.519531]  [Adversarial loss: 0.693930, acc: 0.449219]\n",
            "157: [Discriminator loss: 0.693449, acc: 0.484375]  [Adversarial loss: 0.694175, acc: 0.492188]\n",
            "158: [Discriminator loss: 0.695268, acc: 0.468750]  [Adversarial loss: 0.691737, acc: 0.546875]\n",
            "159: [Discriminator loss: 0.691400, acc: 0.507812]  [Adversarial loss: 0.693302, acc: 0.500000]\n",
            "160: [Discriminator loss: 0.693496, acc: 0.496094]  [Adversarial loss: 0.693755, acc: 0.500000]\n",
            "161: [Discriminator loss: 0.694514, acc: 0.480469]  [Adversarial loss: 0.691617, acc: 0.554688]\n",
            "162: [Discriminator loss: 0.693279, acc: 0.484375]  [Adversarial loss: 0.691069, acc: 0.554688]\n",
            "163: [Discriminator loss: 0.695696, acc: 0.449219]  [Adversarial loss: 0.692603, acc: 0.507812]\n",
            "164: [Discriminator loss: 0.692933, acc: 0.464844]  [Adversarial loss: 0.695905, acc: 0.453125]\n",
            "165: [Discriminator loss: 0.693005, acc: 0.480469]  [Adversarial loss: 0.695931, acc: 0.453125]\n",
            "166: [Discriminator loss: 0.692925, acc: 0.515625]  [Adversarial loss: 0.690412, acc: 0.535156]\n",
            "167: [Discriminator loss: 0.692846, acc: 0.535156]  [Adversarial loss: 0.694827, acc: 0.503906]\n",
            "168: [Discriminator loss: 0.693079, acc: 0.511719]  [Adversarial loss: 0.693546, acc: 0.480469]\n",
            "169: [Discriminator loss: 0.691475, acc: 0.562500]  [Adversarial loss: 0.693170, acc: 0.503906]\n",
            "170: [Discriminator loss: 0.691979, acc: 0.523438]  [Adversarial loss: 0.693434, acc: 0.507812]\n",
            "171: [Discriminator loss: 0.691731, acc: 0.527344]  [Adversarial loss: 0.692823, acc: 0.519531]\n",
            "172: [Discriminator loss: 0.694889, acc: 0.425781]  [Adversarial loss: 0.695006, acc: 0.476562]\n",
            "173: [Discriminator loss: 0.693493, acc: 0.468750]  [Adversarial loss: 0.691528, acc: 0.542969]\n",
            "174: [Discriminator loss: 0.693254, acc: 0.492188]  [Adversarial loss: 0.691707, acc: 0.464844]\n",
            "175: [Discriminator loss: 0.697278, acc: 0.429688]  [Adversarial loss: 0.694491, acc: 0.511719]\n",
            "176: [Discriminator loss: 0.696422, acc: 0.421875]  [Adversarial loss: 0.692534, acc: 0.511719]\n",
            "177: [Discriminator loss: 0.693073, acc: 0.527344]  [Adversarial loss: 0.693179, acc: 0.492188]\n",
            "178: [Discriminator loss: 0.695830, acc: 0.468750]  [Adversarial loss: 0.692828, acc: 0.531250]\n",
            "179: [Discriminator loss: 0.693530, acc: 0.542969]  [Adversarial loss: 0.694716, acc: 0.480469]\n",
            "180: [Discriminator loss: 0.695896, acc: 0.488281]  [Adversarial loss: 0.695394, acc: 0.484375]\n",
            "181: [Discriminator loss: 0.695325, acc: 0.488281]  [Adversarial loss: 0.695897, acc: 0.453125]\n",
            "182: [Discriminator loss: 0.693290, acc: 0.535156]  [Adversarial loss: 0.693499, acc: 0.480469]\n",
            "183: [Discriminator loss: 0.693532, acc: 0.488281]  [Adversarial loss: 0.694201, acc: 0.496094]\n",
            "184: [Discriminator loss: 0.694913, acc: 0.468750]  [Adversarial loss: 0.691668, acc: 0.542969]\n",
            "185: [Discriminator loss: 0.693868, acc: 0.484375]  [Adversarial loss: 0.693182, acc: 0.480469]\n",
            "186: [Discriminator loss: 0.693500, acc: 0.503906]  [Adversarial loss: 0.694208, acc: 0.500000]\n",
            "187: [Discriminator loss: 0.692458, acc: 0.507812]  [Adversarial loss: 0.691277, acc: 0.546875]\n",
            "188: [Discriminator loss: 0.697002, acc: 0.402344]  [Adversarial loss: 0.689709, acc: 0.542969]\n",
            "189: [Discriminator loss: 0.694870, acc: 0.472656]  [Adversarial loss: 0.693670, acc: 0.492188]\n",
            "190: [Discriminator loss: 0.693459, acc: 0.519531]  [Adversarial loss: 0.694162, acc: 0.519531]\n",
            "191: [Discriminator loss: 0.694554, acc: 0.500000]  [Adversarial loss: 0.695751, acc: 0.480469]\n",
            "192: [Discriminator loss: 0.693529, acc: 0.507812]  [Adversarial loss: 0.697257, acc: 0.433594]\n",
            "193: [Discriminator loss: 0.693322, acc: 0.492188]  [Adversarial loss: 0.694519, acc: 0.496094]\n",
            "194: [Discriminator loss: 0.694231, acc: 0.484375]  [Adversarial loss: 0.694394, acc: 0.449219]\n",
            "195: [Discriminator loss: 0.692358, acc: 0.500000]  [Adversarial loss: 0.692325, acc: 0.511719]\n",
            "196: [Discriminator loss: 0.692327, acc: 0.511719]  [Adversarial loss: 0.694301, acc: 0.464844]\n",
            "197: [Discriminator loss: 0.693909, acc: 0.500000]  [Adversarial loss: 0.694924, acc: 0.464844]\n",
            "198: [Discriminator loss: 0.694704, acc: 0.480469]  [Adversarial loss: 0.691212, acc: 0.519531]\n",
            "199: [Discriminator loss: 0.692577, acc: 0.527344]  [Adversarial loss: 0.693249, acc: 0.519531]\n",
            "200: [Discriminator loss: 0.693348, acc: 0.507812]  [Adversarial loss: 0.691460, acc: 0.574219]\n",
            "201: [Discriminator loss: 0.692805, acc: 0.523438]  [Adversarial loss: 0.693505, acc: 0.511719]\n",
            "202: [Discriminator loss: 0.689434, acc: 0.507812]  [Adversarial loss: 0.690456, acc: 0.558594]\n",
            "203: [Discriminator loss: 0.692351, acc: 0.484375]  [Adversarial loss: 0.693246, acc: 0.523438]\n",
            "204: [Discriminator loss: 0.694254, acc: 0.472656]  [Adversarial loss: 0.691775, acc: 0.519531]\n",
            "205: [Discriminator loss: 0.693564, acc: 0.496094]  [Adversarial loss: 0.694143, acc: 0.515625]\n",
            "206: [Discriminator loss: 0.691384, acc: 0.527344]  [Adversarial loss: 0.694176, acc: 0.492188]\n",
            "207: [Discriminator loss: 0.693851, acc: 0.503906]  [Adversarial loss: 0.693069, acc: 0.535156]\n",
            "208: [Discriminator loss: 0.694437, acc: 0.449219]  [Adversarial loss: 0.694810, acc: 0.437500]\n",
            "209: [Discriminator loss: 0.693661, acc: 0.519531]  [Adversarial loss: 0.694932, acc: 0.441406]\n",
            "210: [Discriminator loss: 0.693696, acc: 0.496094]  [Adversarial loss: 0.692495, acc: 0.511719]\n",
            "211: [Discriminator loss: 0.690915, acc: 0.511719]  [Adversarial loss: 0.691967, acc: 0.527344]\n",
            "212: [Discriminator loss: 0.690000, acc: 0.539062]  [Adversarial loss: 0.693075, acc: 0.503906]\n",
            "213: [Discriminator loss: 0.694451, acc: 0.511719]  [Adversarial loss: 0.693240, acc: 0.488281]\n",
            "214: [Discriminator loss: 0.692573, acc: 0.500000]  [Adversarial loss: 0.693920, acc: 0.476562]\n",
            "215: [Discriminator loss: 0.695007, acc: 0.476562]  [Adversarial loss: 0.695192, acc: 0.464844]\n",
            "216: [Discriminator loss: 0.693279, acc: 0.511719]  [Adversarial loss: 0.690727, acc: 0.523438]\n",
            "217: [Discriminator loss: 0.690021, acc: 0.527344]  [Adversarial loss: 0.690168, acc: 0.554688]\n",
            "218: [Discriminator loss: 0.694046, acc: 0.472656]  [Adversarial loss: 0.695450, acc: 0.468750]\n",
            "219: [Discriminator loss: 0.690904, acc: 0.542969]  [Adversarial loss: 0.692889, acc: 0.535156]\n",
            "220: [Discriminator loss: 0.693447, acc: 0.515625]  [Adversarial loss: 0.695049, acc: 0.425781]\n",
            "221: [Discriminator loss: 0.692138, acc: 0.503906]  [Adversarial loss: 0.692731, acc: 0.515625]\n",
            "222: [Discriminator loss: 0.693549, acc: 0.500000]  [Adversarial loss: 0.695928, acc: 0.472656]\n",
            "223: [Discriminator loss: 0.691535, acc: 0.515625]  [Adversarial loss: 0.695991, acc: 0.453125]\n",
            "224: [Discriminator loss: 0.693875, acc: 0.511719]  [Adversarial loss: 0.692641, acc: 0.515625]\n",
            "225: [Discriminator loss: 0.694277, acc: 0.480469]  [Adversarial loss: 0.696614, acc: 0.460938]\n",
            "226: [Discriminator loss: 0.695280, acc: 0.476562]  [Adversarial loss: 0.693450, acc: 0.535156]\n",
            "227: [Discriminator loss: 0.693684, acc: 0.511719]  [Adversarial loss: 0.695740, acc: 0.468750]\n",
            "228: [Discriminator loss: 0.692730, acc: 0.511719]  [Adversarial loss: 0.693055, acc: 0.503906]\n",
            "229: [Discriminator loss: 0.692818, acc: 0.507812]  [Adversarial loss: 0.692744, acc: 0.535156]\n",
            "230: [Discriminator loss: 0.690554, acc: 0.562500]  [Adversarial loss: 0.691891, acc: 0.531250]\n",
            "231: [Discriminator loss: 0.692280, acc: 0.488281]  [Adversarial loss: 0.692578, acc: 0.527344]\n",
            "232: [Discriminator loss: 0.692881, acc: 0.542969]  [Adversarial loss: 0.695160, acc: 0.468750]\n",
            "233: [Discriminator loss: 0.690748, acc: 0.589844]  [Adversarial loss: 0.694507, acc: 0.480469]\n",
            "234: [Discriminator loss: 0.695534, acc: 0.480469]  [Adversarial loss: 0.694768, acc: 0.468750]\n",
            "235: [Discriminator loss: 0.690464, acc: 0.562500]  [Adversarial loss: 0.693516, acc: 0.476562]\n",
            "236: [Discriminator loss: 0.693990, acc: 0.472656]  [Adversarial loss: 0.692546, acc: 0.531250]\n",
            "237: [Discriminator loss: 0.692580, acc: 0.492188]  [Adversarial loss: 0.691900, acc: 0.515625]\n",
            "238: [Discriminator loss: 0.690930, acc: 0.562500]  [Adversarial loss: 0.690904, acc: 0.566406]\n",
            "239: [Discriminator loss: 0.691437, acc: 0.566406]  [Adversarial loss: 0.694420, acc: 0.460938]\n",
            "240: [Discriminator loss: 0.693463, acc: 0.511719]  [Adversarial loss: 0.694093, acc: 0.480469]\n",
            "241: [Discriminator loss: 0.690336, acc: 0.546875]  [Adversarial loss: 0.690878, acc: 0.566406]\n",
            "242: [Discriminator loss: 0.692463, acc: 0.507812]  [Adversarial loss: 0.694673, acc: 0.476562]\n",
            "243: [Discriminator loss: 0.691477, acc: 0.558594]  [Adversarial loss: 0.692227, acc: 0.519531]\n",
            "244: [Discriminator loss: 0.690611, acc: 0.570312]  [Adversarial loss: 0.691904, acc: 0.527344]\n",
            "245: [Discriminator loss: 0.693439, acc: 0.496094]  [Adversarial loss: 0.692443, acc: 0.496094]\n",
            "246: [Discriminator loss: 0.691377, acc: 0.523438]  [Adversarial loss: 0.691493, acc: 0.523438]\n",
            "247: [Discriminator loss: 0.691783, acc: 0.515625]  [Adversarial loss: 0.694153, acc: 0.476562]\n",
            "248: [Discriminator loss: 0.691674, acc: 0.484375]  [Adversarial loss: 0.691076, acc: 0.554688]\n",
            "249: [Discriminator loss: 0.690546, acc: 0.500000]  [Adversarial loss: 0.691030, acc: 0.523438]\n",
            "250: [Discriminator loss: 0.688792, acc: 0.554688]  [Adversarial loss: 0.690633, acc: 0.539062]\n",
            "251: [Discriminator loss: 0.688744, acc: 0.546875]  [Adversarial loss: 0.690990, acc: 0.531250]\n",
            "252: [Discriminator loss: 0.687279, acc: 0.535156]  [Adversarial loss: 0.692187, acc: 0.503906]\n",
            "253: [Discriminator loss: 0.687815, acc: 0.562500]  [Adversarial loss: 0.686460, acc: 0.570312]\n",
            "254: [Discriminator loss: 0.691864, acc: 0.492188]  [Adversarial loss: 0.686284, acc: 0.585938]\n",
            "255: [Discriminator loss: 0.687301, acc: 0.550781]  [Adversarial loss: 0.689760, acc: 0.554688]\n",
            "256: [Discriminator loss: 0.687744, acc: 0.496094]  [Adversarial loss: 0.683870, acc: 0.546875]\n",
            "257: [Discriminator loss: 0.686925, acc: 0.546875]  [Adversarial loss: 0.677686, acc: 0.632812]\n",
            "258: [Discriminator loss: 0.685022, acc: 0.519531]  [Adversarial loss: 0.675201, acc: 0.636719]\n",
            "259: [Discriminator loss: 0.681007, acc: 0.519531]  [Adversarial loss: 0.674871, acc: 0.605469]\n",
            "260: [Discriminator loss: 0.693822, acc: 0.503906]  [Adversarial loss: 0.670746, acc: 0.585938]\n",
            "261: [Discriminator loss: 0.706802, acc: 0.441406]  [Adversarial loss: 0.661512, acc: 0.648438]\n",
            "262: [Discriminator loss: 0.698258, acc: 0.507812]  [Adversarial loss: 0.661226, acc: 0.660156]\n",
            "263: [Discriminator loss: 0.683726, acc: 0.550781]  [Adversarial loss: 0.656771, acc: 0.667969]\n",
            "264: [Discriminator loss: 0.709449, acc: 0.507812]  [Adversarial loss: 0.637573, acc: 0.730469]\n",
            "265: [Discriminator loss: 0.729605, acc: 0.535156]  [Adversarial loss: 0.634819, acc: 0.699219]\n",
            "266: [Discriminator loss: 0.726109, acc: 0.464844]  [Adversarial loss: 0.611198, acc: 0.804688]\n",
            "267: [Discriminator loss: 0.713466, acc: 0.523438]  [Adversarial loss: 0.644010, acc: 0.699219]\n",
            "268: [Discriminator loss: 0.706794, acc: 0.496094]  [Adversarial loss: 0.638997, acc: 0.734375]\n",
            "269: [Discriminator loss: 0.727300, acc: 0.535156]  [Adversarial loss: 0.622310, acc: 0.703125]\n",
            "270: [Discriminator loss: 0.706878, acc: 0.527344]  [Adversarial loss: 0.659338, acc: 0.648438]\n",
            "271: [Discriminator loss: 0.730611, acc: 0.507812]  [Adversarial loss: 0.636956, acc: 0.683594]\n",
            "272: [Discriminator loss: 0.699399, acc: 0.488281]  [Adversarial loss: 0.638269, acc: 0.691406]\n",
            "273: [Discriminator loss: 0.746265, acc: 0.492188]  [Adversarial loss: 0.658480, acc: 0.640625]\n",
            "274: [Discriminator loss: 0.726158, acc: 0.480469]  [Adversarial loss: 0.647753, acc: 0.628906]\n",
            "275: [Discriminator loss: 0.716648, acc: 0.500000]  [Adversarial loss: 0.631488, acc: 0.722656]\n",
            "276: [Discriminator loss: 0.691869, acc: 0.507812]  [Adversarial loss: 0.662814, acc: 0.636719]\n",
            "277: [Discriminator loss: 0.714314, acc: 0.500000]  [Adversarial loss: 0.654624, acc: 0.660156]\n",
            "278: [Discriminator loss: 0.691638, acc: 0.542969]  [Adversarial loss: 0.671238, acc: 0.605469]\n",
            "279: [Discriminator loss: 0.717149, acc: 0.500000]  [Adversarial loss: 0.666676, acc: 0.644531]\n",
            "280: [Discriminator loss: 0.682861, acc: 0.542969]  [Adversarial loss: 0.656586, acc: 0.632812]\n",
            "281: [Discriminator loss: 0.706461, acc: 0.527344]  [Adversarial loss: 0.661332, acc: 0.613281]\n",
            "282: [Discriminator loss: 0.693901, acc: 0.523438]  [Adversarial loss: 0.675150, acc: 0.578125]\n",
            "283: [Discriminator loss: 0.687846, acc: 0.546875]  [Adversarial loss: 0.665335, acc: 0.628906]\n",
            "284: [Discriminator loss: 0.670589, acc: 0.566406]  [Adversarial loss: 0.674911, acc: 0.617188]\n",
            "285: [Discriminator loss: 0.691922, acc: 0.503906]  [Adversarial loss: 0.681824, acc: 0.570312]\n",
            "286: [Discriminator loss: 0.655197, acc: 0.570312]  [Adversarial loss: 0.678130, acc: 0.593750]\n",
            "287: [Discriminator loss: 0.650328, acc: 0.601562]  [Adversarial loss: 0.665955, acc: 0.636719]\n",
            "288: [Discriminator loss: 0.660019, acc: 0.546875]  [Adversarial loss: 0.671411, acc: 0.625000]\n",
            "289: [Discriminator loss: 0.654728, acc: 0.570312]  [Adversarial loss: 0.662405, acc: 0.664062]\n",
            "290: [Discriminator loss: 0.672736, acc: 0.531250]  [Adversarial loss: 0.665214, acc: 0.652344]\n",
            "291: [Discriminator loss: 0.681765, acc: 0.500000]  [Adversarial loss: 0.664121, acc: 0.621094]\n",
            "292: [Discriminator loss: 0.679111, acc: 0.523438]  [Adversarial loss: 0.660488, acc: 0.625000]\n",
            "293: [Discriminator loss: 0.685193, acc: 0.527344]  [Adversarial loss: 0.658564, acc: 0.613281]\n",
            "294: [Discriminator loss: 0.673510, acc: 0.550781]  [Adversarial loss: 0.656840, acc: 0.640625]\n",
            "295: [Discriminator loss: 0.671991, acc: 0.531250]  [Adversarial loss: 0.651203, acc: 0.613281]\n",
            "296: [Discriminator loss: 0.646244, acc: 0.570312]  [Adversarial loss: 0.660672, acc: 0.613281]\n",
            "297: [Discriminator loss: 0.679681, acc: 0.507812]  [Adversarial loss: 0.661692, acc: 0.601562]\n",
            "298: [Discriminator loss: 0.669084, acc: 0.546875]  [Adversarial loss: 0.648175, acc: 0.644531]\n",
            "299: [Discriminator loss: 0.638880, acc: 0.546875]  [Adversarial loss: 0.648943, acc: 0.656250]\n",
            "300: [Discriminator loss: 0.623009, acc: 0.570312]  [Adversarial loss: 0.658883, acc: 0.609375]\n",
            "301: [Discriminator loss: 0.662490, acc: 0.535156]  [Adversarial loss: 0.635517, acc: 0.636719]\n",
            "302: [Discriminator loss: 0.676594, acc: 0.531250]  [Adversarial loss: 0.663971, acc: 0.593750]\n",
            "303: [Discriminator loss: 0.623534, acc: 0.582031]  [Adversarial loss: 0.638775, acc: 0.640625]\n",
            "304: [Discriminator loss: 0.667676, acc: 0.539062]  [Adversarial loss: 0.658175, acc: 0.589844]\n",
            "305: [Discriminator loss: 0.663735, acc: 0.566406]  [Adversarial loss: 0.664940, acc: 0.582031]\n",
            "306: [Discriminator loss: 0.673930, acc: 0.566406]  [Adversarial loss: 0.649583, acc: 0.589844]\n",
            "307: [Discriminator loss: 0.683273, acc: 0.589844]  [Adversarial loss: 0.659074, acc: 0.609375]\n",
            "308: [Discriminator loss: 0.646665, acc: 0.621094]  [Adversarial loss: 0.646250, acc: 0.609375]\n",
            "309: [Discriminator loss: 0.638195, acc: 0.621094]  [Adversarial loss: 0.646806, acc: 0.605469]\n",
            "310: [Discriminator loss: 0.660925, acc: 0.578125]  [Adversarial loss: 0.653334, acc: 0.550781]\n",
            "311: [Discriminator loss: 0.591727, acc: 0.664062]  [Adversarial loss: 0.647183, acc: 0.609375]\n",
            "312: [Discriminator loss: 0.615795, acc: 0.617188]  [Adversarial loss: 0.639547, acc: 0.625000]\n",
            "313: [Discriminator loss: 0.626569, acc: 0.648438]  [Adversarial loss: 0.646361, acc: 0.605469]\n",
            "314: [Discriminator loss: 0.605888, acc: 0.671875]  [Adversarial loss: 0.618516, acc: 0.656250]\n",
            "315: [Discriminator loss: 0.624574, acc: 0.617188]  [Adversarial loss: 0.630658, acc: 0.644531]\n",
            "316: [Discriminator loss: 0.628594, acc: 0.582031]  [Adversarial loss: 0.633638, acc: 0.628906]\n",
            "317: [Discriminator loss: 0.606307, acc: 0.625000]  [Adversarial loss: 0.648748, acc: 0.570312]\n",
            "318: [Discriminator loss: 0.670540, acc: 0.601562]  [Adversarial loss: 0.629689, acc: 0.609375]\n",
            "319: [Discriminator loss: 0.614794, acc: 0.628906]  [Adversarial loss: 0.651023, acc: 0.605469]\n",
            "320: [Discriminator loss: 0.652773, acc: 0.570312]  [Adversarial loss: 0.614085, acc: 0.675781]\n",
            "321: [Discriminator loss: 0.599911, acc: 0.621094]  [Adversarial loss: 0.625108, acc: 0.617188]\n",
            "322: [Discriminator loss: 0.642945, acc: 0.578125]  [Adversarial loss: 0.607226, acc: 0.656250]\n",
            "323: [Discriminator loss: 0.644576, acc: 0.574219]  [Adversarial loss: 0.602741, acc: 0.695312]\n",
            "324: [Discriminator loss: 0.635909, acc: 0.605469]  [Adversarial loss: 0.589128, acc: 0.687500]\n",
            "325: [Discriminator loss: 0.647934, acc: 0.562500]  [Adversarial loss: 0.625452, acc: 0.597656]\n",
            "326: [Discriminator loss: 0.615561, acc: 0.585938]  [Adversarial loss: 0.622232, acc: 0.617188]\n",
            "327: [Discriminator loss: 0.668263, acc: 0.542969]  [Adversarial loss: 0.597469, acc: 0.664062]\n",
            "328: [Discriminator loss: 0.613104, acc: 0.582031]  [Adversarial loss: 0.610351, acc: 0.648438]\n",
            "329: [Discriminator loss: 0.644259, acc: 0.585938]  [Adversarial loss: 0.623313, acc: 0.628906]\n",
            "330: [Discriminator loss: 0.649907, acc: 0.601562]  [Adversarial loss: 0.626931, acc: 0.636719]\n",
            "331: [Discriminator loss: 0.603370, acc: 0.582031]  [Adversarial loss: 0.596715, acc: 0.675781]\n",
            "332: [Discriminator loss: 0.634882, acc: 0.597656]  [Adversarial loss: 0.624372, acc: 0.625000]\n",
            "333: [Discriminator loss: 0.615128, acc: 0.574219]  [Adversarial loss: 0.585817, acc: 0.695312]\n",
            "334: [Discriminator loss: 0.658418, acc: 0.535156]  [Adversarial loss: 0.589039, acc: 0.703125]\n",
            "335: [Discriminator loss: 0.654329, acc: 0.515625]  [Adversarial loss: 0.590890, acc: 0.679688]\n",
            "336: [Discriminator loss: 0.665604, acc: 0.527344]  [Adversarial loss: 0.600624, acc: 0.691406]\n",
            "337: [Discriminator loss: 0.596829, acc: 0.566406]  [Adversarial loss: 0.582969, acc: 0.734375]\n",
            "338: [Discriminator loss: 0.631358, acc: 0.597656]  [Adversarial loss: 0.608114, acc: 0.660156]\n",
            "339: [Discriminator loss: 0.619076, acc: 0.554688]  [Adversarial loss: 0.614743, acc: 0.640625]\n",
            "340: [Discriminator loss: 0.602611, acc: 0.593750]  [Adversarial loss: 0.573875, acc: 0.675781]\n",
            "341: [Discriminator loss: 0.614959, acc: 0.589844]  [Adversarial loss: 0.578656, acc: 0.726562]\n",
            "342: [Discriminator loss: 0.582433, acc: 0.648438]  [Adversarial loss: 0.570526, acc: 0.695312]\n",
            "343: [Discriminator loss: 0.628467, acc: 0.570312]  [Adversarial loss: 0.528145, acc: 0.769531]\n",
            "344: [Discriminator loss: 0.651357, acc: 0.550781]  [Adversarial loss: 0.574768, acc: 0.671875]\n",
            "345: [Discriminator loss: 0.612181, acc: 0.601562]  [Adversarial loss: 0.611549, acc: 0.636719]\n",
            "346: [Discriminator loss: 0.565694, acc: 0.679688]  [Adversarial loss: 0.566569, acc: 0.691406]\n",
            "347: [Discriminator loss: 0.600425, acc: 0.640625]  [Adversarial loss: 0.604012, acc: 0.636719]\n",
            "348: [Discriminator loss: 0.629532, acc: 0.640625]  [Adversarial loss: 0.596786, acc: 0.636719]\n",
            "349: [Discriminator loss: 0.643173, acc: 0.605469]  [Adversarial loss: 0.556933, acc: 0.683594]\n",
            "350: [Discriminator loss: 0.593028, acc: 0.628906]  [Adversarial loss: 0.618483, acc: 0.636719]\n",
            "351: [Discriminator loss: 0.625139, acc: 0.605469]  [Adversarial loss: 0.622471, acc: 0.609375]\n",
            "352: [Discriminator loss: 0.598720, acc: 0.609375]  [Adversarial loss: 0.571392, acc: 0.632812]\n",
            "353: [Discriminator loss: 0.592415, acc: 0.660156]  [Adversarial loss: 0.557068, acc: 0.691406]\n",
            "354: [Discriminator loss: 0.608863, acc: 0.636719]  [Adversarial loss: 0.609427, acc: 0.613281]\n",
            "355: [Discriminator loss: 0.639399, acc: 0.582031]  [Adversarial loss: 0.592281, acc: 0.636719]\n",
            "356: [Discriminator loss: 0.594897, acc: 0.660156]  [Adversarial loss: 0.570652, acc: 0.679688]\n",
            "357: [Discriminator loss: 0.575446, acc: 0.683594]  [Adversarial loss: 0.604329, acc: 0.613281]\n",
            "358: [Discriminator loss: 0.602425, acc: 0.644531]  [Adversarial loss: 0.568071, acc: 0.687500]\n",
            "359: [Discriminator loss: 0.614905, acc: 0.652344]  [Adversarial loss: 0.554764, acc: 0.687500]\n",
            "360: [Discriminator loss: 0.588934, acc: 0.632812]  [Adversarial loss: 0.578212, acc: 0.664062]\n",
            "361: [Discriminator loss: 0.622976, acc: 0.621094]  [Adversarial loss: 0.548349, acc: 0.695312]\n",
            "362: [Discriminator loss: 0.552677, acc: 0.648438]  [Adversarial loss: 0.569363, acc: 0.683594]\n",
            "363: [Discriminator loss: 0.620758, acc: 0.566406]  [Adversarial loss: 0.568611, acc: 0.687500]\n",
            "364: [Discriminator loss: 0.594736, acc: 0.628906]  [Adversarial loss: 0.571966, acc: 0.671875]\n",
            "365: [Discriminator loss: 0.537559, acc: 0.656250]  [Adversarial loss: 0.572914, acc: 0.648438]\n",
            "366: [Discriminator loss: 0.623983, acc: 0.589844]  [Adversarial loss: 0.554245, acc: 0.683594]\n",
            "367: [Discriminator loss: 0.567887, acc: 0.628906]  [Adversarial loss: 0.534763, acc: 0.718750]\n",
            "368: [Discriminator loss: 0.553449, acc: 0.652344]  [Adversarial loss: 0.537120, acc: 0.703125]\n",
            "369: [Discriminator loss: 0.558041, acc: 0.632812]  [Adversarial loss: 0.570428, acc: 0.621094]\n",
            "370: [Discriminator loss: 0.583234, acc: 0.652344]  [Adversarial loss: 0.567501, acc: 0.636719]\n",
            "371: [Discriminator loss: 0.627532, acc: 0.601562]  [Adversarial loss: 0.551355, acc: 0.691406]\n",
            "372: [Discriminator loss: 0.622099, acc: 0.593750]  [Adversarial loss: 0.576273, acc: 0.625000]\n",
            "373: [Discriminator loss: 0.534254, acc: 0.671875]  [Adversarial loss: 0.547548, acc: 0.730469]\n",
            "374: [Discriminator loss: 0.551229, acc: 0.664062]  [Adversarial loss: 0.559528, acc: 0.660156]\n",
            "375: [Discriminator loss: 0.562759, acc: 0.679688]  [Adversarial loss: 0.538511, acc: 0.734375]\n",
            "376: [Discriminator loss: 0.539298, acc: 0.664062]  [Adversarial loss: 0.569750, acc: 0.656250]\n",
            "377: [Discriminator loss: 0.564749, acc: 0.671875]  [Adversarial loss: 0.533056, acc: 0.691406]\n",
            "378: [Discriminator loss: 0.519959, acc: 0.671875]  [Adversarial loss: 0.568505, acc: 0.617188]\n",
            "379: [Discriminator loss: 0.600368, acc: 0.648438]  [Adversarial loss: 0.516033, acc: 0.734375]\n",
            "380: [Discriminator loss: 0.584298, acc: 0.664062]  [Adversarial loss: 0.568241, acc: 0.664062]\n",
            "381: [Discriminator loss: 0.552095, acc: 0.679688]  [Adversarial loss: 0.560259, acc: 0.667969]\n",
            "382: [Discriminator loss: 0.572729, acc: 0.644531]  [Adversarial loss: 0.500167, acc: 0.718750]\n",
            "383: [Discriminator loss: 0.591184, acc: 0.656250]  [Adversarial loss: 0.545668, acc: 0.660156]\n",
            "384: [Discriminator loss: 0.561396, acc: 0.628906]  [Adversarial loss: 0.628259, acc: 0.554688]\n",
            "385: [Discriminator loss: 0.529211, acc: 0.640625]  [Adversarial loss: 0.547743, acc: 0.667969]\n",
            "386: [Discriminator loss: 0.576593, acc: 0.667969]  [Adversarial loss: 0.524697, acc: 0.671875]\n",
            "387: [Discriminator loss: 0.550464, acc: 0.687500]  [Adversarial loss: 0.578071, acc: 0.621094]\n",
            "388: [Discriminator loss: 0.515426, acc: 0.683594]  [Adversarial loss: 0.500024, acc: 0.699219]\n",
            "389: [Discriminator loss: 0.565754, acc: 0.652344]  [Adversarial loss: 0.522887, acc: 0.699219]\n",
            "390: [Discriminator loss: 0.611859, acc: 0.609375]  [Adversarial loss: 0.565463, acc: 0.628906]\n",
            "391: [Discriminator loss: 0.583225, acc: 0.617188]  [Adversarial loss: 0.559841, acc: 0.687500]\n",
            "392: [Discriminator loss: 0.545380, acc: 0.648438]  [Adversarial loss: 0.569994, acc: 0.652344]\n",
            "393: [Discriminator loss: 0.544435, acc: 0.636719]  [Adversarial loss: 0.559777, acc: 0.683594]\n",
            "394: [Discriminator loss: 0.532130, acc: 0.675781]  [Adversarial loss: 0.528281, acc: 0.687500]\n",
            "395: [Discriminator loss: 0.544602, acc: 0.644531]  [Adversarial loss: 0.534503, acc: 0.714844]\n",
            "396: [Discriminator loss: 0.511711, acc: 0.652344]  [Adversarial loss: 0.534181, acc: 0.703125]\n",
            "397: [Discriminator loss: 0.577654, acc: 0.621094]  [Adversarial loss: 0.541047, acc: 0.714844]\n",
            "398: [Discriminator loss: 0.586570, acc: 0.605469]  [Adversarial loss: 0.521626, acc: 0.722656]\n",
            "399: [Discriminator loss: 0.555757, acc: 0.636719]  [Adversarial loss: 0.575699, acc: 0.652344]\n",
            "400: [Discriminator loss: 0.532486, acc: 0.652344]  [Adversarial loss: 0.489178, acc: 0.730469]\n",
            "401: [Discriminator loss: 0.538839, acc: 0.652344]  [Adversarial loss: 0.551015, acc: 0.660156]\n",
            "402: [Discriminator loss: 0.512442, acc: 0.675781]  [Adversarial loss: 0.540885, acc: 0.664062]\n",
            "403: [Discriminator loss: 0.500399, acc: 0.722656]  [Adversarial loss: 0.516493, acc: 0.667969]\n",
            "404: [Discriminator loss: 0.544324, acc: 0.660156]  [Adversarial loss: 0.535854, acc: 0.660156]\n",
            "405: [Discriminator loss: 0.476757, acc: 0.726562]  [Adversarial loss: 0.509302, acc: 0.742188]\n",
            "406: [Discriminator loss: 0.518745, acc: 0.675781]  [Adversarial loss: 0.476719, acc: 0.753906]\n",
            "407: [Discriminator loss: 0.586028, acc: 0.593750]  [Adversarial loss: 0.528224, acc: 0.691406]\n",
            "408: [Discriminator loss: 0.602654, acc: 0.589844]  [Adversarial loss: 0.500706, acc: 0.710938]\n",
            "409: [Discriminator loss: 0.499165, acc: 0.648438]  [Adversarial loss: 0.555012, acc: 0.699219]\n",
            "410: [Discriminator loss: 0.534954, acc: 0.632812]  [Adversarial loss: 0.489527, acc: 0.753906]\n",
            "411: [Discriminator loss: 0.543363, acc: 0.625000]  [Adversarial loss: 0.550671, acc: 0.718750]\n",
            "412: [Discriminator loss: 0.564402, acc: 0.613281]  [Adversarial loss: 0.522595, acc: 0.703125]\n",
            "413: [Discriminator loss: 0.506669, acc: 0.652344]  [Adversarial loss: 0.503737, acc: 0.750000]\n",
            "414: [Discriminator loss: 0.510663, acc: 0.632812]  [Adversarial loss: 0.487526, acc: 0.750000]\n",
            "415: [Discriminator loss: 0.509838, acc: 0.652344]  [Adversarial loss: 0.564983, acc: 0.687500]\n",
            "416: [Discriminator loss: 0.571915, acc: 0.597656]  [Adversarial loss: 0.569264, acc: 0.703125]\n",
            "417: [Discriminator loss: 0.589538, acc: 0.605469]  [Adversarial loss: 0.500190, acc: 0.718750]\n",
            "418: [Discriminator loss: 0.557034, acc: 0.605469]  [Adversarial loss: 0.517775, acc: 0.750000]\n",
            "419: [Discriminator loss: 0.519629, acc: 0.617188]  [Adversarial loss: 0.495376, acc: 0.738281]\n",
            "420: [Discriminator loss: 0.531006, acc: 0.648438]  [Adversarial loss: 0.538776, acc: 0.722656]\n",
            "421: [Discriminator loss: 0.539230, acc: 0.628906]  [Adversarial loss: 0.524799, acc: 0.707031]\n",
            "422: [Discriminator loss: 0.530832, acc: 0.656250]  [Adversarial loss: 0.530755, acc: 0.722656]\n",
            "423: [Discriminator loss: 0.568377, acc: 0.585938]  [Adversarial loss: 0.515231, acc: 0.722656]\n",
            "424: [Discriminator loss: 0.566997, acc: 0.589844]  [Adversarial loss: 0.529765, acc: 0.710938]\n",
            "425: [Discriminator loss: 0.541304, acc: 0.605469]  [Adversarial loss: 0.513453, acc: 0.738281]\n",
            "426: [Discriminator loss: 0.535799, acc: 0.621094]  [Adversarial loss: 0.473903, acc: 0.757812]\n",
            "427: [Discriminator loss: 0.543880, acc: 0.562500]  [Adversarial loss: 0.562118, acc: 0.734375]\n",
            "428: [Discriminator loss: 0.525226, acc: 0.593750]  [Adversarial loss: 0.519925, acc: 0.722656]\n",
            "429: [Discriminator loss: 0.515859, acc: 0.613281]  [Adversarial loss: 0.534174, acc: 0.703125]\n",
            "430: [Discriminator loss: 0.499191, acc: 0.628906]  [Adversarial loss: 0.516489, acc: 0.769531]\n",
            "431: [Discriminator loss: 0.473033, acc: 0.656250]  [Adversarial loss: 0.464724, acc: 0.812500]\n",
            "432: [Discriminator loss: 0.569828, acc: 0.546875]  [Adversarial loss: 0.501942, acc: 0.777344]\n",
            "433: [Discriminator loss: 0.524952, acc: 0.589844]  [Adversarial loss: 0.467547, acc: 0.765625]\n",
            "434: [Discriminator loss: 0.565536, acc: 0.628906]  [Adversarial loss: 0.529384, acc: 0.726562]\n",
            "435: [Discriminator loss: 0.542878, acc: 0.613281]  [Adversarial loss: 0.500634, acc: 0.722656]\n",
            "436: [Discriminator loss: 0.475015, acc: 0.597656]  [Adversarial loss: 0.476094, acc: 0.765625]\n",
            "437: [Discriminator loss: 0.479391, acc: 0.667969]  [Adversarial loss: 0.499781, acc: 0.746094]\n",
            "438: [Discriminator loss: 0.538536, acc: 0.605469]  [Adversarial loss: 0.540124, acc: 0.679688]\n",
            "439: [Discriminator loss: 0.564777, acc: 0.628906]  [Adversarial loss: 0.469770, acc: 0.765625]\n",
            "440: [Discriminator loss: 0.501207, acc: 0.601562]  [Adversarial loss: 0.466062, acc: 0.699219]\n",
            "441: [Discriminator loss: 0.503972, acc: 0.621094]  [Adversarial loss: 0.500836, acc: 0.761719]\n",
            "442: [Discriminator loss: 0.484599, acc: 0.617188]  [Adversarial loss: 0.503184, acc: 0.730469]\n",
            "443: [Discriminator loss: 0.564149, acc: 0.597656]  [Adversarial loss: 0.506164, acc: 0.722656]\n",
            "444: [Discriminator loss: 0.555328, acc: 0.585938]  [Adversarial loss: 0.485681, acc: 0.773438]\n",
            "445: [Discriminator loss: 0.480443, acc: 0.632812]  [Adversarial loss: 0.470398, acc: 0.761719]\n",
            "446: [Discriminator loss: 0.564676, acc: 0.554688]  [Adversarial loss: 0.513143, acc: 0.757812]\n",
            "447: [Discriminator loss: 0.549820, acc: 0.601562]  [Adversarial loss: 0.520131, acc: 0.738281]\n",
            "448: [Discriminator loss: 0.560004, acc: 0.593750]  [Adversarial loss: 0.535397, acc: 0.746094]\n",
            "449: [Discriminator loss: 0.539729, acc: 0.625000]  [Adversarial loss: 0.491411, acc: 0.738281]\n",
            "450: [Discriminator loss: 0.531252, acc: 0.636719]  [Adversarial loss: 0.487632, acc: 0.777344]\n",
            "451: [Discriminator loss: 0.523566, acc: 0.636719]  [Adversarial loss: 0.459615, acc: 0.808594]\n",
            "452: [Discriminator loss: 0.477688, acc: 0.695312]  [Adversarial loss: 0.475183, acc: 0.753906]\n",
            "453: [Discriminator loss: 0.481402, acc: 0.652344]  [Adversarial loss: 0.485305, acc: 0.773438]\n",
            "454: [Discriminator loss: 0.508500, acc: 0.656250]  [Adversarial loss: 0.463134, acc: 0.769531]\n",
            "455: [Discriminator loss: 0.508651, acc: 0.648438]  [Adversarial loss: 0.460028, acc: 0.773438]\n",
            "456: [Discriminator loss: 0.509372, acc: 0.648438]  [Adversarial loss: 0.436553, acc: 0.785156]\n",
            "457: [Discriminator loss: 0.498788, acc: 0.636719]  [Adversarial loss: 0.472392, acc: 0.718750]\n",
            "458: [Discriminator loss: 0.518787, acc: 0.664062]  [Adversarial loss: 0.448896, acc: 0.718750]\n",
            "459: [Discriminator loss: 0.527503, acc: 0.640625]  [Adversarial loss: 0.446715, acc: 0.765625]\n",
            "460: [Discriminator loss: 0.537437, acc: 0.609375]  [Adversarial loss: 0.459050, acc: 0.800781]\n",
            "461: [Discriminator loss: 0.448320, acc: 0.699219]  [Adversarial loss: 0.417848, acc: 0.808594]\n",
            "462: [Discriminator loss: 0.532136, acc: 0.617188]  [Adversarial loss: 0.471819, acc: 0.777344]\n",
            "463: [Discriminator loss: 0.503575, acc: 0.617188]  [Adversarial loss: 0.473773, acc: 0.792969]\n",
            "464: [Discriminator loss: 0.528248, acc: 0.613281]  [Adversarial loss: 0.476438, acc: 0.753906]\n",
            "465: [Discriminator loss: 0.492635, acc: 0.617188]  [Adversarial loss: 0.460102, acc: 0.796875]\n",
            "466: [Discriminator loss: 0.496952, acc: 0.636719]  [Adversarial loss: 0.430002, acc: 0.859375]\n",
            "467: [Discriminator loss: 0.532499, acc: 0.585938]  [Adversarial loss: 0.490991, acc: 0.796875]\n",
            "468: [Discriminator loss: 0.579525, acc: 0.562500]  [Adversarial loss: 0.484799, acc: 0.757812]\n",
            "469: [Discriminator loss: 0.584350, acc: 0.554688]  [Adversarial loss: 0.458235, acc: 0.765625]\n",
            "470: [Discriminator loss: 0.554431, acc: 0.558594]  [Adversarial loss: 0.496839, acc: 0.777344]\n",
            "471: [Discriminator loss: 0.562824, acc: 0.601562]  [Adversarial loss: 0.470566, acc: 0.750000]\n",
            "472: [Discriminator loss: 0.530468, acc: 0.632812]  [Adversarial loss: 0.471027, acc: 0.734375]\n",
            "473: [Discriminator loss: 0.488643, acc: 0.664062]  [Adversarial loss: 0.429172, acc: 0.761719]\n",
            "474: [Discriminator loss: 0.522031, acc: 0.648438]  [Adversarial loss: 0.461571, acc: 0.781250]\n",
            "475: [Discriminator loss: 0.533155, acc: 0.601562]  [Adversarial loss: 0.458645, acc: 0.777344]\n",
            "476: [Discriminator loss: 0.489484, acc: 0.632812]  [Adversarial loss: 0.462054, acc: 0.781250]\n",
            "477: [Discriminator loss: 0.540702, acc: 0.601562]  [Adversarial loss: 0.450814, acc: 0.804688]\n",
            "478: [Discriminator loss: 0.487126, acc: 0.648438]  [Adversarial loss: 0.428231, acc: 0.796875]\n",
            "479: [Discriminator loss: 0.500527, acc: 0.625000]  [Adversarial loss: 0.454345, acc: 0.789062]\n",
            "480: [Discriminator loss: 0.563569, acc: 0.582031]  [Adversarial loss: 0.474888, acc: 0.761719]\n",
            "481: [Discriminator loss: 0.469933, acc: 0.628906]  [Adversarial loss: 0.418089, acc: 0.820312]\n",
            "482: [Discriminator loss: 0.493655, acc: 0.609375]  [Adversarial loss: 0.482817, acc: 0.769531]\n",
            "483: [Discriminator loss: 0.550356, acc: 0.601562]  [Adversarial loss: 0.470959, acc: 0.781250]\n",
            "484: [Discriminator loss: 0.549587, acc: 0.593750]  [Adversarial loss: 0.453998, acc: 0.808594]\n",
            "485: [Discriminator loss: 0.530492, acc: 0.613281]  [Adversarial loss: 0.457679, acc: 0.796875]\n",
            "486: [Discriminator loss: 0.517581, acc: 0.644531]  [Adversarial loss: 0.453852, acc: 0.757812]\n",
            "487: [Discriminator loss: 0.555415, acc: 0.585938]  [Adversarial loss: 0.429290, acc: 0.808594]\n",
            "488: [Discriminator loss: 0.454589, acc: 0.640625]  [Adversarial loss: 0.418138, acc: 0.824219]\n",
            "489: [Discriminator loss: 0.525948, acc: 0.570312]  [Adversarial loss: 0.441260, acc: 0.820312]\n",
            "490: [Discriminator loss: 0.532161, acc: 0.546875]  [Adversarial loss: 0.459128, acc: 0.765625]\n",
            "491: [Discriminator loss: 0.516572, acc: 0.593750]  [Adversarial loss: 0.454320, acc: 0.789062]\n",
            "492: [Discriminator loss: 0.518732, acc: 0.609375]  [Adversarial loss: 0.501305, acc: 0.761719]\n",
            "493: [Discriminator loss: 0.504703, acc: 0.570312]  [Adversarial loss: 0.472879, acc: 0.792969]\n",
            "494: [Discriminator loss: 0.442733, acc: 0.675781]  [Adversarial loss: 0.432359, acc: 0.812500]\n",
            "495: [Discriminator loss: 0.545546, acc: 0.550781]  [Adversarial loss: 0.487162, acc: 0.761719]\n",
            "496: [Discriminator loss: 0.537836, acc: 0.597656]  [Adversarial loss: 0.465144, acc: 0.792969]\n",
            "497: [Discriminator loss: 0.518513, acc: 0.628906]  [Adversarial loss: 0.450036, acc: 0.757812]\n",
            "498: [Discriminator loss: 0.522097, acc: 0.636719]  [Adversarial loss: 0.466074, acc: 0.750000]\n",
            "499: [Discriminator loss: 0.530909, acc: 0.609375]  [Adversarial loss: 0.474389, acc: 0.722656]\n",
            "500: [Discriminator loss: 0.513848, acc: 0.640625]  [Adversarial loss: 0.494805, acc: 0.738281]\n",
            "501: [Discriminator loss: 0.499831, acc: 0.656250]  [Adversarial loss: 0.530565, acc: 0.699219]\n",
            "502: [Discriminator loss: 0.510077, acc: 0.593750]  [Adversarial loss: 0.500245, acc: 0.746094]\n",
            "503: [Discriminator loss: 0.548555, acc: 0.554688]  [Adversarial loss: 0.492058, acc: 0.757812]\n",
            "504: [Discriminator loss: 0.517607, acc: 0.593750]  [Adversarial loss: 0.459174, acc: 0.804688]\n",
            "505: [Discriminator loss: 0.505138, acc: 0.601562]  [Adversarial loss: 0.468200, acc: 0.777344]\n",
            "506: [Discriminator loss: 0.540241, acc: 0.542969]  [Adversarial loss: 0.476470, acc: 0.750000]\n",
            "507: [Discriminator loss: 0.541250, acc: 0.578125]  [Adversarial loss: 0.482895, acc: 0.742188]\n",
            "508: [Discriminator loss: 0.489629, acc: 0.644531]  [Adversarial loss: 0.472840, acc: 0.769531]\n",
            "509: [Discriminator loss: 0.511726, acc: 0.605469]  [Adversarial loss: 0.494881, acc: 0.730469]\n",
            "510: [Discriminator loss: 0.469557, acc: 0.679688]  [Adversarial loss: 0.445647, acc: 0.746094]\n",
            "511: [Discriminator loss: 0.507209, acc: 0.667969]  [Adversarial loss: 0.465556, acc: 0.742188]\n",
            "512: [Discriminator loss: 0.473177, acc: 0.687500]  [Adversarial loss: 0.520381, acc: 0.679688]\n",
            "513: [Discriminator loss: 0.438368, acc: 0.695312]  [Adversarial loss: 0.446120, acc: 0.765625]\n",
            "514: [Discriminator loss: 0.446022, acc: 0.636719]  [Adversarial loss: 0.450908, acc: 0.765625]\n",
            "515: [Discriminator loss: 0.528427, acc: 0.613281]  [Adversarial loss: 0.442643, acc: 0.792969]\n",
            "516: [Discriminator loss: 0.502592, acc: 0.613281]  [Adversarial loss: 0.424300, acc: 0.812500]\n",
            "517: [Discriminator loss: 0.559742, acc: 0.593750]  [Adversarial loss: 0.443347, acc: 0.800781]\n",
            "518: [Discriminator loss: 0.460360, acc: 0.621094]  [Adversarial loss: 0.436782, acc: 0.781250]\n",
            "519: [Discriminator loss: 0.523856, acc: 0.605469]  [Adversarial loss: 0.460393, acc: 0.746094]\n",
            "520: [Discriminator loss: 0.481425, acc: 0.667969]  [Adversarial loss: 0.420673, acc: 0.750000]\n",
            "521: [Discriminator loss: 0.472367, acc: 0.644531]  [Adversarial loss: 0.479302, acc: 0.750000]\n",
            "522: [Discriminator loss: 0.479691, acc: 0.636719]  [Adversarial loss: 0.378993, acc: 0.804688]\n",
            "523: [Discriminator loss: 0.484676, acc: 0.675781]  [Adversarial loss: 0.435226, acc: 0.753906]\n",
            "524: [Discriminator loss: 0.508112, acc: 0.582031]  [Adversarial loss: 0.456095, acc: 0.750000]\n",
            "525: [Discriminator loss: 0.459386, acc: 0.628906]  [Adversarial loss: 0.405118, acc: 0.773438]\n",
            "526: [Discriminator loss: 0.513158, acc: 0.578125]  [Adversarial loss: 0.460983, acc: 0.714844]\n",
            "527: [Discriminator loss: 0.450604, acc: 0.648438]  [Adversarial loss: 0.412906, acc: 0.816406]\n",
            "528: [Discriminator loss: 0.466326, acc: 0.605469]  [Adversarial loss: 0.383133, acc: 0.835938]\n",
            "529: [Discriminator loss: 0.517862, acc: 0.605469]  [Adversarial loss: 0.407597, acc: 0.816406]\n",
            "530: [Discriminator loss: 0.492209, acc: 0.617188]  [Adversarial loss: 0.412568, acc: 0.820312]\n",
            "531: [Discriminator loss: 0.467932, acc: 0.632812]  [Adversarial loss: 0.427484, acc: 0.781250]\n",
            "532: [Discriminator loss: 0.451876, acc: 0.605469]  [Adversarial loss: 0.383647, acc: 0.835938]\n",
            "533: [Discriminator loss: 0.468450, acc: 0.628906]  [Adversarial loss: 0.379133, acc: 0.851562]\n",
            "534: [Discriminator loss: 0.450808, acc: 0.613281]  [Adversarial loss: 0.424222, acc: 0.789062]\n",
            "535: [Discriminator loss: 0.493369, acc: 0.589844]  [Adversarial loss: 0.431148, acc: 0.796875]\n",
            "536: [Discriminator loss: 0.488070, acc: 0.605469]  [Adversarial loss: 0.422572, acc: 0.765625]\n",
            "537: [Discriminator loss: 0.470192, acc: 0.609375]  [Adversarial loss: 0.425110, acc: 0.824219]\n",
            "538: [Discriminator loss: 0.540007, acc: 0.558594]  [Adversarial loss: 0.423428, acc: 0.824219]\n",
            "539: [Discriminator loss: 0.432873, acc: 0.652344]  [Adversarial loss: 0.382250, acc: 0.843750]\n",
            "540: [Discriminator loss: 0.476435, acc: 0.605469]  [Adversarial loss: 0.412263, acc: 0.828125]\n",
            "541: [Discriminator loss: 0.499391, acc: 0.570312]  [Adversarial loss: 0.453788, acc: 0.820312]\n",
            "542: [Discriminator loss: 0.488279, acc: 0.578125]  [Adversarial loss: 0.398909, acc: 0.789062]\n",
            "543: [Discriminator loss: 0.552580, acc: 0.550781]  [Adversarial loss: 0.431419, acc: 0.785156]\n",
            "544: [Discriminator loss: 0.508916, acc: 0.605469]  [Adversarial loss: 0.405923, acc: 0.820312]\n",
            "545: [Discriminator loss: 0.420535, acc: 0.671875]  [Adversarial loss: 0.439727, acc: 0.800781]\n",
            "546: [Discriminator loss: 0.454668, acc: 0.667969]  [Adversarial loss: 0.373537, acc: 0.851562]\n",
            "547: [Discriminator loss: 0.497058, acc: 0.621094]  [Adversarial loss: 0.415181, acc: 0.800781]\n",
            "548: [Discriminator loss: 0.471065, acc: 0.640625]  [Adversarial loss: 0.464740, acc: 0.773438]\n",
            "549: [Discriminator loss: 0.447087, acc: 0.648438]  [Adversarial loss: 0.410937, acc: 0.792969]\n",
            "550: [Discriminator loss: 0.464085, acc: 0.605469]  [Adversarial loss: 0.420243, acc: 0.816406]\n",
            "551: [Discriminator loss: 0.482728, acc: 0.621094]  [Adversarial loss: 0.454082, acc: 0.824219]\n",
            "552: [Discriminator loss: 0.510604, acc: 0.585938]  [Adversarial loss: 0.411598, acc: 0.847656]\n",
            "553: [Discriminator loss: 0.597135, acc: 0.519531]  [Adversarial loss: 0.416840, acc: 0.867188]\n",
            "554: [Discriminator loss: 0.476450, acc: 0.566406]  [Adversarial loss: 0.431896, acc: 0.820312]\n",
            "555: [Discriminator loss: 0.552165, acc: 0.558594]  [Adversarial loss: 0.402691, acc: 0.855469]\n",
            "556: [Discriminator loss: 0.491311, acc: 0.539062]  [Adversarial loss: 0.437531, acc: 0.835938]\n",
            "557: [Discriminator loss: 0.462850, acc: 0.613281]  [Adversarial loss: 0.398592, acc: 0.851562]\n",
            "558: [Discriminator loss: 0.502736, acc: 0.605469]  [Adversarial loss: 0.399664, acc: 0.820312]\n",
            "559: [Discriminator loss: 0.526271, acc: 0.574219]  [Adversarial loss: 0.403980, acc: 0.812500]\n",
            "560: [Discriminator loss: 0.486708, acc: 0.636719]  [Adversarial loss: 0.408083, acc: 0.800781]\n",
            "561: [Discriminator loss: 0.489800, acc: 0.632812]  [Adversarial loss: 0.414547, acc: 0.796875]\n",
            "562: [Discriminator loss: 0.478349, acc: 0.593750]  [Adversarial loss: 0.353128, acc: 0.851562]\n",
            "563: [Discriminator loss: 0.479534, acc: 0.589844]  [Adversarial loss: 0.428876, acc: 0.832031]\n",
            "564: [Discriminator loss: 0.472936, acc: 0.605469]  [Adversarial loss: 0.423567, acc: 0.808594]\n",
            "565: [Discriminator loss: 0.484999, acc: 0.554688]  [Adversarial loss: 0.399482, acc: 0.855469]\n",
            "566: [Discriminator loss: 0.513080, acc: 0.578125]  [Adversarial loss: 0.395738, acc: 0.871094]\n",
            "567: [Discriminator loss: 0.463031, acc: 0.613281]  [Adversarial loss: 0.382003, acc: 0.859375]\n",
            "568: [Discriminator loss: 0.556769, acc: 0.527344]  [Adversarial loss: 0.446597, acc: 0.816406]\n",
            "569: [Discriminator loss: 0.511907, acc: 0.585938]  [Adversarial loss: 0.409066, acc: 0.843750]\n",
            "570: [Discriminator loss: 0.516330, acc: 0.609375]  [Adversarial loss: 0.447148, acc: 0.804688]\n",
            "571: [Discriminator loss: 0.481543, acc: 0.613281]  [Adversarial loss: 0.454298, acc: 0.789062]\n",
            "572: [Discriminator loss: 0.503586, acc: 0.632812]  [Adversarial loss: 0.420094, acc: 0.808594]\n",
            "573: [Discriminator loss: 0.488820, acc: 0.621094]  [Adversarial loss: 0.492229, acc: 0.742188]\n",
            "574: [Discriminator loss: 0.490034, acc: 0.648438]  [Adversarial loss: 0.499751, acc: 0.667969]\n",
            "575: [Discriminator loss: 0.514830, acc: 0.609375]  [Adversarial loss: 0.504815, acc: 0.710938]\n",
            "576: [Discriminator loss: 0.457499, acc: 0.675781]  [Adversarial loss: 0.490956, acc: 0.683594]\n",
            "577: [Discriminator loss: 0.508057, acc: 0.632812]  [Adversarial loss: 0.518524, acc: 0.679688]\n",
            "578: [Discriminator loss: 0.546848, acc: 0.601562]  [Adversarial loss: 0.522041, acc: 0.714844]\n",
            "579: [Discriminator loss: 0.492392, acc: 0.636719]  [Adversarial loss: 0.473029, acc: 0.757812]\n",
            "580: [Discriminator loss: 0.500204, acc: 0.593750]  [Adversarial loss: 0.518105, acc: 0.687500]\n",
            "581: [Discriminator loss: 0.497155, acc: 0.621094]  [Adversarial loss: 0.482236, acc: 0.750000]\n",
            "582: [Discriminator loss: 0.480741, acc: 0.582031]  [Adversarial loss: 0.465077, acc: 0.777344]\n",
            "583: [Discriminator loss: 0.487013, acc: 0.593750]  [Adversarial loss: 0.501293, acc: 0.757812]\n",
            "584: [Discriminator loss: 0.474084, acc: 0.632812]  [Adversarial loss: 0.519323, acc: 0.757812]\n",
            "585: [Discriminator loss: 0.495835, acc: 0.605469]  [Adversarial loss: 0.482677, acc: 0.792969]\n",
            "586: [Discriminator loss: 0.502096, acc: 0.613281]  [Adversarial loss: 0.449216, acc: 0.804688]\n",
            "587: [Discriminator loss: 0.439991, acc: 0.652344]  [Adversarial loss: 0.454923, acc: 0.789062]\n",
            "588: [Discriminator loss: 0.454019, acc: 0.628906]  [Adversarial loss: 0.507174, acc: 0.726562]\n",
            "589: [Discriminator loss: 0.495521, acc: 0.585938]  [Adversarial loss: 0.503135, acc: 0.746094]\n",
            "590: [Discriminator loss: 0.492354, acc: 0.570312]  [Adversarial loss: 0.476793, acc: 0.777344]\n",
            "591: [Discriminator loss: 0.514428, acc: 0.621094]  [Adversarial loss: 0.524467, acc: 0.742188]\n",
            "592: [Discriminator loss: 0.449572, acc: 0.625000]  [Adversarial loss: 0.480443, acc: 0.746094]\n",
            "593: [Discriminator loss: 0.464633, acc: 0.636719]  [Adversarial loss: 0.497765, acc: 0.730469]\n",
            "594: [Discriminator loss: 0.465022, acc: 0.628906]  [Adversarial loss: 0.463959, acc: 0.777344]\n",
            "595: [Discriminator loss: 0.528147, acc: 0.601562]  [Adversarial loss: 0.471980, acc: 0.753906]\n",
            "596: [Discriminator loss: 0.484683, acc: 0.582031]  [Adversarial loss: 0.481519, acc: 0.718750]\n",
            "597: [Discriminator loss: 0.524199, acc: 0.550781]  [Adversarial loss: 0.514977, acc: 0.722656]\n",
            "598: [Discriminator loss: 0.470669, acc: 0.636719]  [Adversarial loss: 0.480334, acc: 0.734375]\n",
            "599: [Discriminator loss: 0.511060, acc: 0.601562]  [Adversarial loss: 0.570967, acc: 0.691406]\n",
            "600: [Discriminator loss: 0.475327, acc: 0.617188]  [Adversarial loss: 0.428853, acc: 0.789062]\n",
            "601: [Discriminator loss: 0.464771, acc: 0.660156]  [Adversarial loss: 0.449142, acc: 0.750000]\n",
            "602: [Discriminator loss: 0.457838, acc: 0.628906]  [Adversarial loss: 0.443149, acc: 0.820312]\n",
            "603: [Discriminator loss: 0.479872, acc: 0.605469]  [Adversarial loss: 0.530582, acc: 0.761719]\n",
            "604: [Discriminator loss: 0.509714, acc: 0.562500]  [Adversarial loss: 0.511198, acc: 0.777344]\n",
            "605: [Discriminator loss: 0.471712, acc: 0.625000]  [Adversarial loss: 0.407682, acc: 0.828125]\n",
            "606: [Discriminator loss: 0.477007, acc: 0.625000]  [Adversarial loss: 0.430062, acc: 0.812500]\n",
            "607: [Discriminator loss: 0.453633, acc: 0.632812]  [Adversarial loss: 0.430176, acc: 0.851562]\n",
            "608: [Discriminator loss: 0.460297, acc: 0.613281]  [Adversarial loss: 0.402970, acc: 0.839844]\n",
            "609: [Discriminator loss: 0.441184, acc: 0.652344]  [Adversarial loss: 0.420266, acc: 0.835938]\n",
            "610: [Discriminator loss: 0.498558, acc: 0.562500]  [Adversarial loss: 0.466797, acc: 0.828125]\n",
            "611: [Discriminator loss: 0.467009, acc: 0.636719]  [Adversarial loss: 0.419747, acc: 0.789062]\n",
            "612: [Discriminator loss: 0.500553, acc: 0.585938]  [Adversarial loss: 0.476314, acc: 0.812500]\n",
            "613: [Discriminator loss: 0.458706, acc: 0.617188]  [Adversarial loss: 0.430819, acc: 0.820312]\n",
            "614: [Discriminator loss: 0.488490, acc: 0.562500]  [Adversarial loss: 0.429164, acc: 0.839844]\n",
            "615: [Discriminator loss: 0.517776, acc: 0.601562]  [Adversarial loss: 0.431699, acc: 0.812500]\n",
            "616: [Discriminator loss: 0.485822, acc: 0.609375]  [Adversarial loss: 0.379979, acc: 0.839844]\n",
            "617: [Discriminator loss: 0.586401, acc: 0.519531]  [Adversarial loss: 0.477932, acc: 0.789062]\n",
            "618: [Discriminator loss: 0.539762, acc: 0.535156]  [Adversarial loss: 0.426763, acc: 0.828125]\n",
            "619: [Discriminator loss: 0.529351, acc: 0.527344]  [Adversarial loss: 0.460454, acc: 0.808594]\n",
            "620: [Discriminator loss: 0.478766, acc: 0.621094]  [Adversarial loss: 0.501510, acc: 0.765625]\n",
            "621: [Discriminator loss: 0.522118, acc: 0.566406]  [Adversarial loss: 0.432925, acc: 0.847656]\n",
            "622: [Discriminator loss: 0.466210, acc: 0.585938]  [Adversarial loss: 0.422490, acc: 0.800781]\n",
            "623: [Discriminator loss: 0.502010, acc: 0.617188]  [Adversarial loss: 0.476813, acc: 0.816406]\n",
            "624: [Discriminator loss: 0.519702, acc: 0.566406]  [Adversarial loss: 0.443074, acc: 0.820312]\n",
            "625: [Discriminator loss: 0.506688, acc: 0.562500]  [Adversarial loss: 0.385071, acc: 0.886719]\n",
            "626: [Discriminator loss: 0.575251, acc: 0.539062]  [Adversarial loss: 0.476025, acc: 0.816406]\n",
            "627: [Discriminator loss: 0.484638, acc: 0.613281]  [Adversarial loss: 0.414826, acc: 0.816406]\n",
            "628: [Discriminator loss: 0.569725, acc: 0.546875]  [Adversarial loss: 0.413103, acc: 0.847656]\n",
            "629: [Discriminator loss: 0.521724, acc: 0.554688]  [Adversarial loss: 0.462916, acc: 0.824219]\n",
            "630: [Discriminator loss: 0.524014, acc: 0.585938]  [Adversarial loss: 0.376616, acc: 0.871094]\n",
            "631: [Discriminator loss: 0.521026, acc: 0.531250]  [Adversarial loss: 0.429278, acc: 0.832031]\n",
            "632: [Discriminator loss: 0.547324, acc: 0.558594]  [Adversarial loss: 0.463954, acc: 0.785156]\n",
            "633: [Discriminator loss: 0.456449, acc: 0.605469]  [Adversarial loss: 0.396952, acc: 0.855469]\n",
            "634: [Discriminator loss: 0.524910, acc: 0.578125]  [Adversarial loss: 0.410495, acc: 0.843750]\n",
            "635: [Discriminator loss: 0.508984, acc: 0.589844]  [Adversarial loss: 0.392714, acc: 0.863281]\n",
            "636: [Discriminator loss: 0.506134, acc: 0.554688]  [Adversarial loss: 0.446341, acc: 0.828125]\n",
            "637: [Discriminator loss: 0.490291, acc: 0.570312]  [Adversarial loss: 0.453609, acc: 0.792969]\n",
            "638: [Discriminator loss: 0.489124, acc: 0.578125]  [Adversarial loss: 0.411794, acc: 0.816406]\n",
            "639: [Discriminator loss: 0.522592, acc: 0.558594]  [Adversarial loss: 0.411860, acc: 0.839844]\n",
            "640: [Discriminator loss: 0.539296, acc: 0.519531]  [Adversarial loss: 0.432023, acc: 0.832031]\n",
            "641: [Discriminator loss: 0.528501, acc: 0.570312]  [Adversarial loss: 0.412803, acc: 0.843750]\n",
            "642: [Discriminator loss: 0.529012, acc: 0.574219]  [Adversarial loss: 0.431627, acc: 0.839844]\n",
            "643: [Discriminator loss: 0.504661, acc: 0.570312]  [Adversarial loss: 0.428951, acc: 0.843750]\n",
            "644: [Discriminator loss: 0.473999, acc: 0.582031]  [Adversarial loss: 0.428982, acc: 0.808594]\n",
            "645: [Discriminator loss: 0.488831, acc: 0.574219]  [Adversarial loss: 0.403413, acc: 0.828125]\n",
            "646: [Discriminator loss: 0.482731, acc: 0.585938]  [Adversarial loss: 0.427937, acc: 0.832031]\n",
            "647: [Discriminator loss: 0.551300, acc: 0.539062]  [Adversarial loss: 0.461192, acc: 0.847656]\n",
            "648: [Discriminator loss: 0.479726, acc: 0.609375]  [Adversarial loss: 0.378850, acc: 0.847656]\n",
            "649: [Discriminator loss: 0.506125, acc: 0.574219]  [Adversarial loss: 0.469281, acc: 0.800781]\n",
            "650: [Discriminator loss: 0.460856, acc: 0.585938]  [Adversarial loss: 0.446449, acc: 0.796875]\n",
            "651: [Discriminator loss: 0.532636, acc: 0.503906]  [Adversarial loss: 0.446638, acc: 0.816406]\n",
            "652: [Discriminator loss: 0.474544, acc: 0.609375]  [Adversarial loss: 0.417758, acc: 0.847656]\n",
            "653: [Discriminator loss: 0.435940, acc: 0.621094]  [Adversarial loss: 0.361390, acc: 0.855469]\n",
            "654: [Discriminator loss: 0.496112, acc: 0.550781]  [Adversarial loss: 0.397535, acc: 0.855469]\n",
            "655: [Discriminator loss: 0.459756, acc: 0.621094]  [Adversarial loss: 0.401071, acc: 0.839844]\n",
            "656: [Discriminator loss: 0.508033, acc: 0.593750]  [Adversarial loss: 0.421840, acc: 0.824219]\n",
            "657: [Discriminator loss: 0.558744, acc: 0.550781]  [Adversarial loss: 0.420748, acc: 0.800781]\n",
            "658: [Discriminator loss: 0.508310, acc: 0.546875]  [Adversarial loss: 0.391936, acc: 0.835938]\n",
            "659: [Discriminator loss: 0.492280, acc: 0.578125]  [Adversarial loss: 0.390346, acc: 0.855469]\n",
            "660: [Discriminator loss: 0.468964, acc: 0.613281]  [Adversarial loss: 0.390741, acc: 0.804688]\n",
            "661: [Discriminator loss: 0.485394, acc: 0.593750]  [Adversarial loss: 0.420567, acc: 0.808594]\n",
            "662: [Discriminator loss: 0.498320, acc: 0.597656]  [Adversarial loss: 0.441504, acc: 0.828125]\n",
            "663: [Discriminator loss: 0.490192, acc: 0.574219]  [Adversarial loss: 0.400013, acc: 0.824219]\n",
            "664: [Discriminator loss: 0.486367, acc: 0.640625]  [Adversarial loss: 0.464717, acc: 0.824219]\n",
            "665: [Discriminator loss: 0.466107, acc: 0.605469]  [Adversarial loss: 0.426775, acc: 0.835938]\n",
            "666: [Discriminator loss: 0.444002, acc: 0.609375]  [Adversarial loss: 0.405496, acc: 0.835938]\n",
            "667: [Discriminator loss: 0.516560, acc: 0.570312]  [Adversarial loss: 0.440905, acc: 0.828125]\n",
            "668: [Discriminator loss: 0.534462, acc: 0.535156]  [Adversarial loss: 0.449557, acc: 0.808594]\n",
            "669: [Discriminator loss: 0.552319, acc: 0.570312]  [Adversarial loss: 0.438240, acc: 0.812500]\n",
            "670: [Discriminator loss: 0.537858, acc: 0.558594]  [Adversarial loss: 0.399965, acc: 0.847656]\n",
            "671: [Discriminator loss: 0.451617, acc: 0.628906]  [Adversarial loss: 0.381765, acc: 0.832031]\n",
            "672: [Discriminator loss: 0.517408, acc: 0.589844]  [Adversarial loss: 0.454438, acc: 0.792969]\n",
            "673: [Discriminator loss: 0.473033, acc: 0.605469]  [Adversarial loss: 0.372457, acc: 0.859375]\n",
            "674: [Discriminator loss: 0.507835, acc: 0.566406]  [Adversarial loss: 0.448945, acc: 0.843750]\n",
            "675: [Discriminator loss: 0.515662, acc: 0.523438]  [Adversarial loss: 0.395292, acc: 0.859375]\n",
            "676: [Discriminator loss: 0.504237, acc: 0.582031]  [Adversarial loss: 0.436884, acc: 0.824219]\n",
            "677: [Discriminator loss: 0.533297, acc: 0.621094]  [Adversarial loss: 0.412137, acc: 0.808594]\n",
            "678: [Discriminator loss: 0.526571, acc: 0.578125]  [Adversarial loss: 0.436736, acc: 0.863281]\n",
            "679: [Discriminator loss: 0.504938, acc: 0.550781]  [Adversarial loss: 0.408085, acc: 0.828125]\n",
            "680: [Discriminator loss: 0.476780, acc: 0.609375]  [Adversarial loss: 0.432179, acc: 0.820312]\n",
            "681: [Discriminator loss: 0.483271, acc: 0.601562]  [Adversarial loss: 0.401204, acc: 0.839844]\n",
            "682: [Discriminator loss: 0.497012, acc: 0.601562]  [Adversarial loss: 0.407155, acc: 0.863281]\n",
            "683: [Discriminator loss: 0.477786, acc: 0.589844]  [Adversarial loss: 0.421348, acc: 0.824219]\n",
            "684: [Discriminator loss: 0.476526, acc: 0.554688]  [Adversarial loss: 0.402197, acc: 0.820312]\n",
            "685: [Discriminator loss: 0.512605, acc: 0.601562]  [Adversarial loss: 0.452376, acc: 0.824219]\n",
            "686: [Discriminator loss: 0.577167, acc: 0.535156]  [Adversarial loss: 0.487973, acc: 0.820312]\n",
            "687: [Discriminator loss: 0.620599, acc: 0.531250]  [Adversarial loss: 0.426396, acc: 0.816406]\n",
            "688: [Discriminator loss: 0.503912, acc: 0.578125]  [Adversarial loss: 0.389565, acc: 0.847656]\n",
            "689: [Discriminator loss: 0.530986, acc: 0.531250]  [Adversarial loss: 0.401939, acc: 0.863281]\n",
            "690: [Discriminator loss: 0.500662, acc: 0.589844]  [Adversarial loss: 0.413703, acc: 0.832031]\n",
            "691: [Discriminator loss: 0.479601, acc: 0.597656]  [Adversarial loss: 0.402620, acc: 0.859375]\n",
            "692: [Discriminator loss: 0.538212, acc: 0.570312]  [Adversarial loss: 0.448205, acc: 0.867188]\n",
            "693: [Discriminator loss: 0.470250, acc: 0.609375]  [Adversarial loss: 0.381462, acc: 0.824219]\n",
            "694: [Discriminator loss: 0.491447, acc: 0.582031]  [Adversarial loss: 0.411785, acc: 0.816406]\n",
            "695: [Discriminator loss: 0.509637, acc: 0.558594]  [Adversarial loss: 0.421216, acc: 0.808594]\n",
            "696: [Discriminator loss: 0.492503, acc: 0.597656]  [Adversarial loss: 0.413160, acc: 0.851562]\n",
            "697: [Discriminator loss: 0.508660, acc: 0.570312]  [Adversarial loss: 0.444153, acc: 0.808594]\n",
            "698: [Discriminator loss: 0.538988, acc: 0.531250]  [Adversarial loss: 0.475739, acc: 0.785156]\n",
            "699: [Discriminator loss: 0.527568, acc: 0.519531]  [Adversarial loss: 0.440385, acc: 0.808594]\n",
            "700: [Discriminator loss: 0.497222, acc: 0.593750]  [Adversarial loss: 0.434565, acc: 0.792969]\n",
            "701: [Discriminator loss: 0.475313, acc: 0.632812]  [Adversarial loss: 0.415520, acc: 0.839844]\n",
            "702: [Discriminator loss: 0.529026, acc: 0.585938]  [Adversarial loss: 0.420304, acc: 0.828125]\n",
            "703: [Discriminator loss: 0.573535, acc: 0.558594]  [Adversarial loss: 0.452766, acc: 0.843750]\n",
            "704: [Discriminator loss: 0.484173, acc: 0.593750]  [Adversarial loss: 0.416816, acc: 0.847656]\n",
            "705: [Discriminator loss: 0.528636, acc: 0.527344]  [Adversarial loss: 0.400325, acc: 0.863281]\n",
            "706: [Discriminator loss: 0.482663, acc: 0.574219]  [Adversarial loss: 0.421430, acc: 0.855469]\n",
            "707: [Discriminator loss: 0.488337, acc: 0.628906]  [Adversarial loss: 0.424942, acc: 0.828125]\n",
            "708: [Discriminator loss: 0.490885, acc: 0.597656]  [Adversarial loss: 0.405782, acc: 0.816406]\n",
            "709: [Discriminator loss: 0.480013, acc: 0.613281]  [Adversarial loss: 0.435137, acc: 0.796875]\n",
            "710: [Discriminator loss: 0.452540, acc: 0.625000]  [Adversarial loss: 0.372014, acc: 0.871094]\n",
            "711: [Discriminator loss: 0.505227, acc: 0.582031]  [Adversarial loss: 0.454103, acc: 0.804688]\n",
            "712: [Discriminator loss: 0.508420, acc: 0.585938]  [Adversarial loss: 0.404244, acc: 0.820312]\n",
            "713: [Discriminator loss: 0.457086, acc: 0.640625]  [Adversarial loss: 0.406260, acc: 0.835938]\n",
            "714: [Discriminator loss: 0.577169, acc: 0.515625]  [Adversarial loss: 0.434010, acc: 0.804688]\n",
            "715: [Discriminator loss: 0.528273, acc: 0.511719]  [Adversarial loss: 0.378428, acc: 0.875000]\n",
            "716: [Discriminator loss: 0.502454, acc: 0.585938]  [Adversarial loss: 0.415843, acc: 0.839844]\n",
            "717: [Discriminator loss: 0.472843, acc: 0.585938]  [Adversarial loss: 0.458928, acc: 0.820312]\n",
            "718: [Discriminator loss: 0.501046, acc: 0.578125]  [Adversarial loss: 0.446909, acc: 0.777344]\n",
            "719: [Discriminator loss: 0.478082, acc: 0.609375]  [Adversarial loss: 0.485160, acc: 0.796875]\n",
            "720: [Discriminator loss: 0.472798, acc: 0.558594]  [Adversarial loss: 0.438770, acc: 0.832031]\n",
            "721: [Discriminator loss: 0.505313, acc: 0.578125]  [Adversarial loss: 0.433188, acc: 0.855469]\n",
            "722: [Discriminator loss: 0.480706, acc: 0.589844]  [Adversarial loss: 0.481549, acc: 0.804688]\n",
            "723: [Discriminator loss: 0.533180, acc: 0.542969]  [Adversarial loss: 0.471777, acc: 0.792969]\n",
            "724: [Discriminator loss: 0.484490, acc: 0.582031]  [Adversarial loss: 0.429399, acc: 0.832031]\n",
            "725: [Discriminator loss: 0.531765, acc: 0.593750]  [Adversarial loss: 0.497579, acc: 0.773438]\n",
            "726: [Discriminator loss: 0.478977, acc: 0.652344]  [Adversarial loss: 0.470201, acc: 0.816406]\n",
            "727: [Discriminator loss: 0.477518, acc: 0.593750]  [Adversarial loss: 0.472085, acc: 0.773438]\n",
            "728: [Discriminator loss: 0.524609, acc: 0.601562]  [Adversarial loss: 0.439338, acc: 0.789062]\n",
            "729: [Discriminator loss: 0.474697, acc: 0.621094]  [Adversarial loss: 0.462132, acc: 0.804688]\n",
            "730: [Discriminator loss: 0.527882, acc: 0.609375]  [Adversarial loss: 0.423395, acc: 0.808594]\n",
            "731: [Discriminator loss: 0.479412, acc: 0.562500]  [Adversarial loss: 0.442442, acc: 0.832031]\n",
            "732: [Discriminator loss: 0.517169, acc: 0.558594]  [Adversarial loss: 0.473032, acc: 0.765625]\n",
            "733: [Discriminator loss: 0.450069, acc: 0.613281]  [Adversarial loss: 0.464121, acc: 0.789062]\n",
            "734: [Discriminator loss: 0.479638, acc: 0.574219]  [Adversarial loss: 0.413155, acc: 0.878906]\n",
            "735: [Discriminator loss: 0.479273, acc: 0.585938]  [Adversarial loss: 0.407101, acc: 0.847656]\n",
            "736: [Discriminator loss: 0.534099, acc: 0.550781]  [Adversarial loss: 0.430007, acc: 0.824219]\n",
            "737: [Discriminator loss: 0.473284, acc: 0.605469]  [Adversarial loss: 0.427241, acc: 0.847656]\n",
            "738: [Discriminator loss: 0.511738, acc: 0.578125]  [Adversarial loss: 0.374608, acc: 0.867188]\n",
            "739: [Discriminator loss: 0.534602, acc: 0.554688]  [Adversarial loss: 0.387725, acc: 0.820312]\n",
            "740: [Discriminator loss: 0.502809, acc: 0.582031]  [Adversarial loss: 0.428016, acc: 0.804688]\n",
            "741: [Discriminator loss: 0.479429, acc: 0.582031]  [Adversarial loss: 0.418932, acc: 0.851562]\n",
            "742: [Discriminator loss: 0.519482, acc: 0.562500]  [Adversarial loss: 0.438287, acc: 0.832031]\n",
            "743: [Discriminator loss: 0.474059, acc: 0.605469]  [Adversarial loss: 0.393195, acc: 0.855469]\n",
            "744: [Discriminator loss: 0.512239, acc: 0.582031]  [Adversarial loss: 0.398720, acc: 0.843750]\n",
            "745: [Discriminator loss: 0.511499, acc: 0.574219]  [Adversarial loss: 0.429714, acc: 0.816406]\n",
            "746: [Discriminator loss: 0.466619, acc: 0.605469]  [Adversarial loss: 0.420941, acc: 0.824219]\n",
            "747: [Discriminator loss: 0.478230, acc: 0.613281]  [Adversarial loss: 0.410834, acc: 0.824219]\n",
            "748: [Discriminator loss: 0.474964, acc: 0.621094]  [Adversarial loss: 0.365791, acc: 0.847656]\n",
            "749: [Discriminator loss: 0.479227, acc: 0.601562]  [Adversarial loss: 0.375836, acc: 0.867188]\n",
            "750: [Discriminator loss: 0.488844, acc: 0.578125]  [Adversarial loss: 0.381669, acc: 0.832031]\n",
            "751: [Discriminator loss: 0.490969, acc: 0.585938]  [Adversarial loss: 0.393416, acc: 0.835938]\n",
            "752: [Discriminator loss: 0.507986, acc: 0.570312]  [Adversarial loss: 0.407767, acc: 0.855469]\n",
            "753: [Discriminator loss: 0.531962, acc: 0.558594]  [Adversarial loss: 0.421183, acc: 0.804688]\n",
            "754: [Discriminator loss: 0.477813, acc: 0.574219]  [Adversarial loss: 0.400347, acc: 0.828125]\n",
            "755: [Discriminator loss: 0.467214, acc: 0.589844]  [Adversarial loss: 0.406664, acc: 0.816406]\n",
            "756: [Discriminator loss: 0.491109, acc: 0.617188]  [Adversarial loss: 0.421106, acc: 0.792969]\n",
            "757: [Discriminator loss: 0.475236, acc: 0.628906]  [Adversarial loss: 0.363003, acc: 0.828125]\n",
            "758: [Discriminator loss: 0.503950, acc: 0.578125]  [Adversarial loss: 0.429666, acc: 0.781250]\n",
            "759: [Discriminator loss: 0.502418, acc: 0.585938]  [Adversarial loss: 0.419363, acc: 0.785156]\n",
            "760: [Discriminator loss: 0.495632, acc: 0.609375]  [Adversarial loss: 0.418396, acc: 0.792969]\n",
            "761: [Discriminator loss: 0.496396, acc: 0.585938]  [Adversarial loss: 0.446862, acc: 0.781250]\n",
            "762: [Discriminator loss: 0.481448, acc: 0.625000]  [Adversarial loss: 0.413257, acc: 0.781250]\n",
            "763: [Discriminator loss: 0.481380, acc: 0.632812]  [Adversarial loss: 0.390684, acc: 0.824219]\n",
            "764: [Discriminator loss: 0.476653, acc: 0.632812]  [Adversarial loss: 0.407713, acc: 0.789062]\n",
            "765: [Discriminator loss: 0.499450, acc: 0.605469]  [Adversarial loss: 0.434841, acc: 0.757812]\n",
            "766: [Discriminator loss: 0.472580, acc: 0.632812]  [Adversarial loss: 0.415016, acc: 0.808594]\n",
            "767: [Discriminator loss: 0.535878, acc: 0.527344]  [Adversarial loss: 0.476370, acc: 0.785156]\n",
            "768: [Discriminator loss: 0.505932, acc: 0.566406]  [Adversarial loss: 0.393766, acc: 0.812500]\n",
            "769: [Discriminator loss: 0.485259, acc: 0.601562]  [Adversarial loss: 0.467943, acc: 0.792969]\n",
            "770: [Discriminator loss: 0.496426, acc: 0.609375]  [Adversarial loss: 0.451118, acc: 0.789062]\n",
            "771: [Discriminator loss: 0.499514, acc: 0.582031]  [Adversarial loss: 0.441193, acc: 0.781250]\n",
            "772: [Discriminator loss: 0.522591, acc: 0.519531]  [Adversarial loss: 0.451036, acc: 0.843750]\n",
            "773: [Discriminator loss: 0.495003, acc: 0.535156]  [Adversarial loss: 0.427231, acc: 0.808594]\n",
            "774: [Discriminator loss: 0.504880, acc: 0.566406]  [Adversarial loss: 0.437093, acc: 0.804688]\n",
            "775: [Discriminator loss: 0.507856, acc: 0.589844]  [Adversarial loss: 0.460787, acc: 0.792969]\n",
            "776: [Discriminator loss: 0.487499, acc: 0.582031]  [Adversarial loss: 0.433042, acc: 0.828125]\n",
            "777: [Discriminator loss: 0.504176, acc: 0.601562]  [Adversarial loss: 0.413098, acc: 0.757812]\n",
            "778: [Discriminator loss: 0.475060, acc: 0.609375]  [Adversarial loss: 0.429825, acc: 0.777344]\n",
            "779: [Discriminator loss: 0.510381, acc: 0.589844]  [Adversarial loss: 0.455968, acc: 0.800781]\n",
            "780: [Discriminator loss: 0.441041, acc: 0.628906]  [Adversarial loss: 0.378790, acc: 0.824219]\n",
            "781: [Discriminator loss: 0.484991, acc: 0.601562]  [Adversarial loss: 0.478624, acc: 0.753906]\n",
            "782: [Discriminator loss: 0.474327, acc: 0.585938]  [Adversarial loss: 0.480964, acc: 0.777344]\n",
            "783: [Discriminator loss: 0.488312, acc: 0.593750]  [Adversarial loss: 0.412436, acc: 0.812500]\n",
            "784: [Discriminator loss: 0.518292, acc: 0.582031]  [Adversarial loss: 0.388434, acc: 0.855469]\n",
            "785: [Discriminator loss: 0.488820, acc: 0.601562]  [Adversarial loss: 0.416504, acc: 0.820312]\n",
            "786: [Discriminator loss: 0.458954, acc: 0.632812]  [Adversarial loss: 0.384872, acc: 0.824219]\n",
            "787: [Discriminator loss: 0.547163, acc: 0.578125]  [Adversarial loss: 0.448535, acc: 0.773438]\n",
            "788: [Discriminator loss: 0.531179, acc: 0.574219]  [Adversarial loss: 0.409393, acc: 0.792969]\n",
            "789: [Discriminator loss: 0.530476, acc: 0.585938]  [Adversarial loss: 0.451302, acc: 0.738281]\n",
            "790: [Discriminator loss: 0.494698, acc: 0.609375]  [Adversarial loss: 0.444582, acc: 0.757812]\n",
            "791: [Discriminator loss: 0.496265, acc: 0.593750]  [Adversarial loss: 0.403113, acc: 0.855469]\n",
            "792: [Discriminator loss: 0.442278, acc: 0.652344]  [Adversarial loss: 0.370167, acc: 0.835938]\n",
            "793: [Discriminator loss: 0.474431, acc: 0.609375]  [Adversarial loss: 0.409514, acc: 0.820312]\n",
            "794: [Discriminator loss: 0.477607, acc: 0.574219]  [Adversarial loss: 0.405314, acc: 0.859375]\n",
            "795: [Discriminator loss: 0.459609, acc: 0.589844]  [Adversarial loss: 0.430108, acc: 0.820312]\n",
            "796: [Discriminator loss: 0.482201, acc: 0.617188]  [Adversarial loss: 0.390202, acc: 0.835938]\n",
            "797: [Discriminator loss: 0.494846, acc: 0.609375]  [Adversarial loss: 0.445412, acc: 0.824219]\n",
            "798: [Discriminator loss: 0.514782, acc: 0.554688]  [Adversarial loss: 0.429397, acc: 0.800781]\n",
            "799: [Discriminator loss: 0.464980, acc: 0.621094]  [Adversarial loss: 0.436052, acc: 0.835938]\n",
            "800: [Discriminator loss: 0.516029, acc: 0.613281]  [Adversarial loss: 0.403245, acc: 0.816406]\n",
            "801: [Discriminator loss: 0.483312, acc: 0.601562]  [Adversarial loss: 0.421372, acc: 0.828125]\n",
            "802: [Discriminator loss: 0.441763, acc: 0.628906]  [Adversarial loss: 0.404188, acc: 0.855469]\n",
            "803: [Discriminator loss: 0.500259, acc: 0.566406]  [Adversarial loss: 0.423449, acc: 0.835938]\n",
            "804: [Discriminator loss: 0.449820, acc: 0.660156]  [Adversarial loss: 0.449948, acc: 0.820312]\n",
            "805: [Discriminator loss: 0.446125, acc: 0.609375]  [Adversarial loss: 0.427564, acc: 0.820312]\n",
            "806: [Discriminator loss: 0.418994, acc: 0.636719]  [Adversarial loss: 0.369781, acc: 0.832031]\n",
            "807: [Discriminator loss: 0.498127, acc: 0.582031]  [Adversarial loss: 0.427593, acc: 0.800781]\n",
            "808: [Discriminator loss: 0.502488, acc: 0.558594]  [Adversarial loss: 0.451190, acc: 0.757812]\n",
            "809: [Discriminator loss: 0.519611, acc: 0.582031]  [Adversarial loss: 0.468314, acc: 0.734375]\n",
            "810: [Discriminator loss: 0.448883, acc: 0.636719]  [Adversarial loss: 0.435058, acc: 0.804688]\n",
            "811: [Discriminator loss: 0.466226, acc: 0.597656]  [Adversarial loss: 0.392775, acc: 0.839844]\n",
            "812: [Discriminator loss: 0.433844, acc: 0.648438]  [Adversarial loss: 0.372133, acc: 0.859375]\n",
            "813: [Discriminator loss: 0.436853, acc: 0.660156]  [Adversarial loss: 0.365582, acc: 0.851562]\n",
            "814: [Discriminator loss: 0.539402, acc: 0.539062]  [Adversarial loss: 0.444885, acc: 0.835938]\n",
            "815: [Discriminator loss: 0.464980, acc: 0.574219]  [Adversarial loss: 0.440338, acc: 0.812500]\n",
            "816: [Discriminator loss: 0.503518, acc: 0.644531]  [Adversarial loss: 0.435656, acc: 0.804688]\n",
            "817: [Discriminator loss: 0.482207, acc: 0.617188]  [Adversarial loss: 0.423054, acc: 0.773438]\n",
            "818: [Discriminator loss: 0.501478, acc: 0.593750]  [Adversarial loss: 0.433426, acc: 0.828125]\n",
            "819: [Discriminator loss: 0.491684, acc: 0.570312]  [Adversarial loss: 0.453159, acc: 0.796875]\n",
            "820: [Discriminator loss: 0.487468, acc: 0.574219]  [Adversarial loss: 0.449049, acc: 0.757812]\n",
            "821: [Discriminator loss: 0.537556, acc: 0.613281]  [Adversarial loss: 0.443913, acc: 0.746094]\n",
            "822: [Discriminator loss: 0.519130, acc: 0.597656]  [Adversarial loss: 0.478473, acc: 0.753906]\n",
            "823: [Discriminator loss: 0.474906, acc: 0.660156]  [Adversarial loss: 0.414832, acc: 0.800781]\n",
            "824: [Discriminator loss: 0.485720, acc: 0.570312]  [Adversarial loss: 0.440602, acc: 0.761719]\n",
            "825: [Discriminator loss: 0.484303, acc: 0.648438]  [Adversarial loss: 0.461902, acc: 0.753906]\n",
            "826: [Discriminator loss: 0.435356, acc: 0.699219]  [Adversarial loss: 0.406247, acc: 0.781250]\n",
            "827: [Discriminator loss: 0.471193, acc: 0.636719]  [Adversarial loss: 0.449786, acc: 0.753906]\n",
            "828: [Discriminator loss: 0.448911, acc: 0.671875]  [Adversarial loss: 0.424011, acc: 0.777344]\n",
            "829: [Discriminator loss: 0.505577, acc: 0.621094]  [Adversarial loss: 0.484398, acc: 0.691406]\n",
            "830: [Discriminator loss: 0.483271, acc: 0.613281]  [Adversarial loss: 0.443211, acc: 0.757812]\n",
            "831: [Discriminator loss: 0.447126, acc: 0.675781]  [Adversarial loss: 0.438023, acc: 0.734375]\n",
            "832: [Discriminator loss: 0.485015, acc: 0.613281]  [Adversarial loss: 0.439278, acc: 0.781250]\n",
            "833: [Discriminator loss: 0.514171, acc: 0.597656]  [Adversarial loss: 0.436461, acc: 0.808594]\n",
            "834: [Discriminator loss: 0.456399, acc: 0.625000]  [Adversarial loss: 0.410936, acc: 0.804688]\n",
            "835: [Discriminator loss: 0.462865, acc: 0.589844]  [Adversarial loss: 0.440350, acc: 0.808594]\n",
            "836: [Discriminator loss: 0.496413, acc: 0.613281]  [Adversarial loss: 0.432169, acc: 0.757812]\n",
            "837: [Discriminator loss: 0.470860, acc: 0.636719]  [Adversarial loss: 0.455371, acc: 0.773438]\n",
            "838: [Discriminator loss: 0.533581, acc: 0.617188]  [Adversarial loss: 0.480681, acc: 0.753906]\n",
            "839: [Discriminator loss: 0.458216, acc: 0.589844]  [Adversarial loss: 0.419240, acc: 0.800781]\n",
            "840: [Discriminator loss: 0.439420, acc: 0.632812]  [Adversarial loss: 0.389244, acc: 0.800781]\n",
            "841: [Discriminator loss: 0.452901, acc: 0.640625]  [Adversarial loss: 0.435114, acc: 0.753906]\n",
            "842: [Discriminator loss: 0.451896, acc: 0.644531]  [Adversarial loss: 0.435979, acc: 0.746094]\n",
            "843: [Discriminator loss: 0.394299, acc: 0.710938]  [Adversarial loss: 0.377782, acc: 0.785156]\n",
            "844: [Discriminator loss: 0.441876, acc: 0.636719]  [Adversarial loss: 0.445598, acc: 0.769531]\n",
            "845: [Discriminator loss: 0.465741, acc: 0.628906]  [Adversarial loss: 0.439164, acc: 0.785156]\n",
            "846: [Discriminator loss: 0.439449, acc: 0.625000]  [Adversarial loss: 0.414103, acc: 0.816406]\n",
            "847: [Discriminator loss: 0.443372, acc: 0.621094]  [Adversarial loss: 0.406068, acc: 0.839844]\n",
            "848: [Discriminator loss: 0.475258, acc: 0.597656]  [Adversarial loss: 0.428387, acc: 0.808594]\n",
            "849: [Discriminator loss: 0.439372, acc: 0.652344]  [Adversarial loss: 0.349882, acc: 0.820312]\n",
            "850: [Discriminator loss: 0.412449, acc: 0.679688]  [Adversarial loss: 0.418813, acc: 0.800781]\n",
            "851: [Discriminator loss: 0.370181, acc: 0.683594]  [Adversarial loss: 0.380321, acc: 0.828125]\n",
            "852: [Discriminator loss: 0.459338, acc: 0.656250]  [Adversarial loss: 0.385531, acc: 0.777344]\n",
            "853: [Discriminator loss: 0.467695, acc: 0.613281]  [Adversarial loss: 0.399110, acc: 0.828125]\n",
            "854: [Discriminator loss: 0.444709, acc: 0.656250]  [Adversarial loss: 0.446660, acc: 0.769531]\n",
            "855: [Discriminator loss: 0.500590, acc: 0.601562]  [Adversarial loss: 0.445036, acc: 0.757812]\n",
            "856: [Discriminator loss: 0.387651, acc: 0.722656]  [Adversarial loss: 0.410292, acc: 0.800781]\n",
            "857: [Discriminator loss: 0.481770, acc: 0.589844]  [Adversarial loss: 0.418287, acc: 0.800781]\n",
            "858: [Discriminator loss: 0.478965, acc: 0.601562]  [Adversarial loss: 0.473257, acc: 0.765625]\n",
            "859: [Discriminator loss: 0.450366, acc: 0.605469]  [Adversarial loss: 0.395935, acc: 0.812500]\n",
            "860: [Discriminator loss: 0.516694, acc: 0.566406]  [Adversarial loss: 0.437580, acc: 0.804688]\n",
            "861: [Discriminator loss: 0.483617, acc: 0.609375]  [Adversarial loss: 0.420457, acc: 0.812500]\n",
            "862: [Discriminator loss: 0.521214, acc: 0.582031]  [Adversarial loss: 0.454964, acc: 0.734375]\n",
            "863: [Discriminator loss: 0.419430, acc: 0.664062]  [Adversarial loss: 0.394828, acc: 0.808594]\n",
            "864: [Discriminator loss: 0.427584, acc: 0.664062]  [Adversarial loss: 0.387296, acc: 0.820312]\n",
            "865: [Discriminator loss: 0.481605, acc: 0.628906]  [Adversarial loss: 0.392479, acc: 0.820312]\n",
            "866: [Discriminator loss: 0.461791, acc: 0.644531]  [Adversarial loss: 0.417678, acc: 0.789062]\n",
            "867: [Discriminator loss: 0.442565, acc: 0.628906]  [Adversarial loss: 0.386599, acc: 0.812500]\n",
            "868: [Discriminator loss: 0.432532, acc: 0.679688]  [Adversarial loss: 0.392067, acc: 0.792969]\n",
            "869: [Discriminator loss: 0.424735, acc: 0.656250]  [Adversarial loss: 0.386173, acc: 0.796875]\n",
            "870: [Discriminator loss: 0.441514, acc: 0.625000]  [Adversarial loss: 0.384892, acc: 0.812500]\n",
            "871: [Discriminator loss: 0.501423, acc: 0.589844]  [Adversarial loss: 0.462101, acc: 0.753906]\n",
            "872: [Discriminator loss: 0.461744, acc: 0.648438]  [Adversarial loss: 0.390293, acc: 0.769531]\n",
            "873: [Discriminator loss: 0.420482, acc: 0.687500]  [Adversarial loss: 0.403109, acc: 0.796875]\n",
            "874: [Discriminator loss: 0.438678, acc: 0.695312]  [Adversarial loss: 0.350390, acc: 0.812500]\n",
            "875: [Discriminator loss: 0.447359, acc: 0.644531]  [Adversarial loss: 0.427203, acc: 0.742188]\n",
            "876: [Discriminator loss: 0.481329, acc: 0.648438]  [Adversarial loss: 0.378377, acc: 0.796875]\n",
            "877: [Discriminator loss: 0.442835, acc: 0.667969]  [Adversarial loss: 0.431696, acc: 0.785156]\n",
            "878: [Discriminator loss: 0.474206, acc: 0.617188]  [Adversarial loss: 0.382914, acc: 0.820312]\n",
            "879: [Discriminator loss: 0.494390, acc: 0.605469]  [Adversarial loss: 0.416275, acc: 0.769531]\n",
            "880: [Discriminator loss: 0.479264, acc: 0.609375]  [Adversarial loss: 0.443243, acc: 0.738281]\n",
            "881: [Discriminator loss: 0.427830, acc: 0.632812]  [Adversarial loss: 0.398982, acc: 0.792969]\n",
            "882: [Discriminator loss: 0.457036, acc: 0.589844]  [Adversarial loss: 0.417403, acc: 0.785156]\n",
            "883: [Discriminator loss: 0.454798, acc: 0.648438]  [Adversarial loss: 0.428827, acc: 0.753906]\n",
            "884: [Discriminator loss: 0.483956, acc: 0.628906]  [Adversarial loss: 0.445854, acc: 0.777344]\n",
            "885: [Discriminator loss: 0.409569, acc: 0.671875]  [Adversarial loss: 0.387376, acc: 0.769531]\n",
            "886: [Discriminator loss: 0.448565, acc: 0.636719]  [Adversarial loss: 0.412626, acc: 0.769531]\n",
            "887: [Discriminator loss: 0.467675, acc: 0.644531]  [Adversarial loss: 0.414434, acc: 0.761719]\n",
            "888: [Discriminator loss: 0.469338, acc: 0.644531]  [Adversarial loss: 0.440506, acc: 0.710938]\n",
            "889: [Discriminator loss: 0.430601, acc: 0.718750]  [Adversarial loss: 0.374869, acc: 0.777344]\n",
            "890: [Discriminator loss: 0.429492, acc: 0.628906]  [Adversarial loss: 0.413335, acc: 0.761719]\n",
            "891: [Discriminator loss: 0.421903, acc: 0.679688]  [Adversarial loss: 0.422922, acc: 0.769531]\n",
            "892: [Discriminator loss: 0.428690, acc: 0.683594]  [Adversarial loss: 0.380450, acc: 0.753906]\n",
            "893: [Discriminator loss: 0.476827, acc: 0.628906]  [Adversarial loss: 0.394103, acc: 0.777344]\n",
            "894: [Discriminator loss: 0.435402, acc: 0.687500]  [Adversarial loss: 0.423111, acc: 0.742188]\n",
            "895: [Discriminator loss: 0.504078, acc: 0.640625]  [Adversarial loss: 0.482196, acc: 0.703125]\n",
            "896: [Discriminator loss: 0.459521, acc: 0.648438]  [Adversarial loss: 0.454730, acc: 0.757812]\n",
            "897: [Discriminator loss: 0.450664, acc: 0.628906]  [Adversarial loss: 0.414035, acc: 0.750000]\n",
            "898: [Discriminator loss: 0.435485, acc: 0.691406]  [Adversarial loss: 0.376948, acc: 0.769531]\n",
            "899: [Discriminator loss: 0.529044, acc: 0.597656]  [Adversarial loss: 0.443502, acc: 0.726562]\n",
            "900: [Discriminator loss: 0.482237, acc: 0.625000]  [Adversarial loss: 0.398169, acc: 0.777344]\n",
            "901: [Discriminator loss: 0.468757, acc: 0.664062]  [Adversarial loss: 0.401254, acc: 0.777344]\n",
            "902: [Discriminator loss: 0.482840, acc: 0.628906]  [Adversarial loss: 0.427196, acc: 0.761719]\n",
            "903: [Discriminator loss: 0.449290, acc: 0.640625]  [Adversarial loss: 0.469750, acc: 0.714844]\n",
            "904: [Discriminator loss: 0.449598, acc: 0.679688]  [Adversarial loss: 0.485082, acc: 0.714844]\n",
            "905: [Discriminator loss: 0.470092, acc: 0.644531]  [Adversarial loss: 0.468226, acc: 0.707031]\n",
            "906: [Discriminator loss: 0.429349, acc: 0.699219]  [Adversarial loss: 0.405391, acc: 0.746094]\n",
            "907: [Discriminator loss: 0.422801, acc: 0.707031]  [Adversarial loss: 0.415908, acc: 0.726562]\n",
            "908: [Discriminator loss: 0.489925, acc: 0.660156]  [Adversarial loss: 0.496132, acc: 0.699219]\n",
            "909: [Discriminator loss: 0.452019, acc: 0.679688]  [Adversarial loss: 0.391803, acc: 0.734375]\n",
            "910: [Discriminator loss: 0.428456, acc: 0.707031]  [Adversarial loss: 0.408393, acc: 0.750000]\n",
            "911: [Discriminator loss: 0.451126, acc: 0.699219]  [Adversarial loss: 0.433546, acc: 0.722656]\n",
            "912: [Discriminator loss: 0.451125, acc: 0.671875]  [Adversarial loss: 0.436424, acc: 0.714844]\n",
            "913: [Discriminator loss: 0.441517, acc: 0.695312]  [Adversarial loss: 0.407679, acc: 0.753906]\n",
            "914: [Discriminator loss: 0.435659, acc: 0.687500]  [Adversarial loss: 0.411439, acc: 0.734375]\n",
            "915: [Discriminator loss: 0.503553, acc: 0.679688]  [Adversarial loss: 0.485580, acc: 0.632812]\n",
            "916: [Discriminator loss: 0.461702, acc: 0.660156]  [Adversarial loss: 0.455893, acc: 0.687500]\n",
            "917: [Discriminator loss: 0.447303, acc: 0.664062]  [Adversarial loss: 0.446949, acc: 0.734375]\n",
            "918: [Discriminator loss: 0.453499, acc: 0.695312]  [Adversarial loss: 0.411240, acc: 0.761719]\n",
            "919: [Discriminator loss: 0.447239, acc: 0.664062]  [Adversarial loss: 0.472659, acc: 0.718750]\n",
            "920: [Discriminator loss: 0.454629, acc: 0.695312]  [Adversarial loss: 0.403468, acc: 0.714844]\n",
            "921: [Discriminator loss: 0.451703, acc: 0.660156]  [Adversarial loss: 0.411258, acc: 0.792969]\n",
            "922: [Discriminator loss: 0.420130, acc: 0.687500]  [Adversarial loss: 0.416792, acc: 0.773438]\n",
            "923: [Discriminator loss: 0.480208, acc: 0.628906]  [Adversarial loss: 0.436028, acc: 0.722656]\n",
            "924: [Discriminator loss: 0.412381, acc: 0.710938]  [Adversarial loss: 0.391637, acc: 0.753906]\n",
            "925: [Discriminator loss: 0.481258, acc: 0.671875]  [Adversarial loss: 0.385540, acc: 0.773438]\n",
            "926: [Discriminator loss: 0.468075, acc: 0.675781]  [Adversarial loss: 0.405873, acc: 0.687500]\n",
            "927: [Discriminator loss: 0.488392, acc: 0.695312]  [Adversarial loss: 0.441318, acc: 0.738281]\n",
            "928: [Discriminator loss: 0.461542, acc: 0.664062]  [Adversarial loss: 0.412162, acc: 0.734375]\n",
            "929: [Discriminator loss: 0.445738, acc: 0.648438]  [Adversarial loss: 0.423426, acc: 0.707031]\n",
            "930: [Discriminator loss: 0.459895, acc: 0.652344]  [Adversarial loss: 0.368640, acc: 0.781250]\n",
            "931: [Discriminator loss: 0.427904, acc: 0.695312]  [Adversarial loss: 0.406781, acc: 0.792969]\n",
            "932: [Discriminator loss: 0.494117, acc: 0.640625]  [Adversarial loss: 0.400490, acc: 0.746094]\n",
            "933: [Discriminator loss: 0.509658, acc: 0.636719]  [Adversarial loss: 0.451332, acc: 0.773438]\n",
            "934: [Discriminator loss: 0.474903, acc: 0.656250]  [Adversarial loss: 0.442449, acc: 0.750000]\n",
            "935: [Discriminator loss: 0.458777, acc: 0.609375]  [Adversarial loss: 0.426795, acc: 0.777344]\n",
            "936: [Discriminator loss: 0.466960, acc: 0.656250]  [Adversarial loss: 0.461290, acc: 0.726562]\n",
            "937: [Discriminator loss: 0.477881, acc: 0.675781]  [Adversarial loss: 0.448290, acc: 0.753906]\n",
            "938: [Discriminator loss: 0.492946, acc: 0.609375]  [Adversarial loss: 0.411828, acc: 0.726562]\n",
            "939: [Discriminator loss: 0.491954, acc: 0.687500]  [Adversarial loss: 0.472402, acc: 0.707031]\n",
            "940: [Discriminator loss: 0.493760, acc: 0.597656]  [Adversarial loss: 0.513214, acc: 0.679688]\n",
            "941: [Discriminator loss: 0.506382, acc: 0.625000]  [Adversarial loss: 0.472995, acc: 0.722656]\n",
            "942: [Discriminator loss: 0.448062, acc: 0.687500]  [Adversarial loss: 0.468442, acc: 0.675781]\n",
            "943: [Discriminator loss: 0.459320, acc: 0.664062]  [Adversarial loss: 0.435417, acc: 0.742188]\n",
            "944: [Discriminator loss: 0.422398, acc: 0.718750]  [Adversarial loss: 0.422894, acc: 0.738281]\n",
            "945: [Discriminator loss: 0.511099, acc: 0.679688]  [Adversarial loss: 0.463522, acc: 0.695312]\n",
            "946: [Discriminator loss: 0.476125, acc: 0.675781]  [Adversarial loss: 0.474168, acc: 0.699219]\n",
            "947: [Discriminator loss: 0.464852, acc: 0.687500]  [Adversarial loss: 0.479502, acc: 0.718750]\n",
            "948: [Discriminator loss: 0.463963, acc: 0.699219]  [Adversarial loss: 0.457605, acc: 0.710938]\n",
            "949: [Discriminator loss: 0.479427, acc: 0.699219]  [Adversarial loss: 0.467292, acc: 0.671875]\n",
            "950: [Discriminator loss: 0.468071, acc: 0.722656]  [Adversarial loss: 0.494966, acc: 0.679688]\n",
            "951: [Discriminator loss: 0.457367, acc: 0.656250]  [Adversarial loss: 0.412635, acc: 0.757812]\n",
            "952: [Discriminator loss: 0.400935, acc: 0.753906]  [Adversarial loss: 0.401009, acc: 0.734375]\n",
            "953: [Discriminator loss: 0.452738, acc: 0.710938]  [Adversarial loss: 0.455964, acc: 0.714844]\n",
            "954: [Discriminator loss: 0.491023, acc: 0.652344]  [Adversarial loss: 0.458483, acc: 0.718750]\n",
            "955: [Discriminator loss: 0.448084, acc: 0.691406]  [Adversarial loss: 0.435949, acc: 0.781250]\n",
            "956: [Discriminator loss: 0.429038, acc: 0.742188]  [Adversarial loss: 0.415949, acc: 0.746094]\n",
            "957: [Discriminator loss: 0.485368, acc: 0.667969]  [Adversarial loss: 0.453684, acc: 0.703125]\n",
            "958: [Discriminator loss: 0.516855, acc: 0.648438]  [Adversarial loss: 0.434001, acc: 0.753906]\n",
            "959: [Discriminator loss: 0.496607, acc: 0.644531]  [Adversarial loss: 0.413246, acc: 0.761719]\n",
            "960: [Discriminator loss: 0.479743, acc: 0.656250]  [Adversarial loss: 0.427522, acc: 0.761719]\n",
            "961: [Discriminator loss: 0.498358, acc: 0.664062]  [Adversarial loss: 0.482011, acc: 0.734375]\n",
            "962: [Discriminator loss: 0.468705, acc: 0.734375]  [Adversarial loss: 0.430390, acc: 0.757812]\n",
            "963: [Discriminator loss: 0.467602, acc: 0.675781]  [Adversarial loss: 0.411880, acc: 0.761719]\n",
            "964: [Discriminator loss: 0.503957, acc: 0.652344]  [Adversarial loss: 0.465854, acc: 0.761719]\n",
            "965: [Discriminator loss: 0.457172, acc: 0.671875]  [Adversarial loss: 0.452645, acc: 0.703125]\n",
            "966: [Discriminator loss: 0.438782, acc: 0.679688]  [Adversarial loss: 0.445229, acc: 0.730469]\n",
            "967: [Discriminator loss: 0.455649, acc: 0.683594]  [Adversarial loss: 0.414901, acc: 0.769531]\n",
            "968: [Discriminator loss: 0.532804, acc: 0.625000]  [Adversarial loss: 0.493137, acc: 0.718750]\n",
            "969: [Discriminator loss: 0.460778, acc: 0.671875]  [Adversarial loss: 0.408848, acc: 0.789062]\n",
            "970: [Discriminator loss: 0.483788, acc: 0.664062]  [Adversarial loss: 0.463704, acc: 0.734375]\n",
            "971: [Discriminator loss: 0.466255, acc: 0.648438]  [Adversarial loss: 0.430431, acc: 0.765625]\n",
            "972: [Discriminator loss: 0.442214, acc: 0.687500]  [Adversarial loss: 0.425732, acc: 0.730469]\n",
            "973: [Discriminator loss: 0.447109, acc: 0.671875]  [Adversarial loss: 0.450492, acc: 0.730469]\n",
            "974: [Discriminator loss: 0.435264, acc: 0.695312]  [Adversarial loss: 0.459036, acc: 0.730469]\n",
            "975: [Discriminator loss: 0.470950, acc: 0.625000]  [Adversarial loss: 0.423919, acc: 0.777344]\n",
            "976: [Discriminator loss: 0.512441, acc: 0.648438]  [Adversarial loss: 0.430490, acc: 0.714844]\n",
            "977: [Discriminator loss: 0.540852, acc: 0.621094]  [Adversarial loss: 0.429065, acc: 0.765625]\n",
            "978: [Discriminator loss: 0.431480, acc: 0.687500]  [Adversarial loss: 0.423546, acc: 0.757812]\n",
            "979: [Discriminator loss: 0.540686, acc: 0.625000]  [Adversarial loss: 0.445080, acc: 0.773438]\n",
            "980: [Discriminator loss: 0.490257, acc: 0.664062]  [Adversarial loss: 0.374439, acc: 0.785156]\n",
            "981: [Discriminator loss: 0.502238, acc: 0.628906]  [Adversarial loss: 0.464803, acc: 0.750000]\n",
            "982: [Discriminator loss: 0.489310, acc: 0.664062]  [Adversarial loss: 0.472846, acc: 0.742188]\n",
            "983: [Discriminator loss: 0.503182, acc: 0.644531]  [Adversarial loss: 0.459896, acc: 0.710938]\n",
            "984: [Discriminator loss: 0.480306, acc: 0.683594]  [Adversarial loss: 0.449793, acc: 0.722656]\n",
            "985: [Discriminator loss: 0.473694, acc: 0.699219]  [Adversarial loss: 0.381966, acc: 0.777344]\n",
            "986: [Discriminator loss: 0.487733, acc: 0.660156]  [Adversarial loss: 0.463987, acc: 0.753906]\n",
            "987: [Discriminator loss: 0.532282, acc: 0.632812]  [Adversarial loss: 0.394246, acc: 0.800781]\n",
            "988: [Discriminator loss: 0.477922, acc: 0.648438]  [Adversarial loss: 0.391768, acc: 0.832031]\n",
            "989: [Discriminator loss: 0.464872, acc: 0.648438]  [Adversarial loss: 0.379570, acc: 0.789062]\n",
            "990: [Discriminator loss: 0.476174, acc: 0.699219]  [Adversarial loss: 0.401008, acc: 0.816406]\n",
            "991: [Discriminator loss: 0.471325, acc: 0.621094]  [Adversarial loss: 0.409539, acc: 0.800781]\n",
            "992: [Discriminator loss: 0.434060, acc: 0.703125]  [Adversarial loss: 0.384637, acc: 0.761719]\n",
            "993: [Discriminator loss: 0.475830, acc: 0.691406]  [Adversarial loss: 0.398738, acc: 0.796875]\n",
            "994: [Discriminator loss: 0.505113, acc: 0.660156]  [Adversarial loss: 0.417909, acc: 0.789062]\n",
            "995: [Discriminator loss: 0.497521, acc: 0.671875]  [Adversarial loss: 0.382516, acc: 0.765625]\n",
            "996: [Discriminator loss: 0.477052, acc: 0.718750]  [Adversarial loss: 0.418053, acc: 0.738281]\n",
            "997: [Discriminator loss: 0.484122, acc: 0.671875]  [Adversarial loss: 0.406882, acc: 0.769531]\n",
            "998: [Discriminator loss: 0.414887, acc: 0.718750]  [Adversarial loss: 0.385304, acc: 0.773438]\n",
            "999: [Discriminator loss: 0.493552, acc: 0.652344]  [Adversarial loss: 0.421768, acc: 0.746094]\n",
            "1000: [Discriminator loss: 0.512679, acc: 0.699219]  [Adversarial loss: 0.438859, acc: 0.714844]\n",
            "1001: [Discriminator loss: 0.482807, acc: 0.628906]  [Adversarial loss: 0.466573, acc: 0.675781]\n",
            "1002: [Discriminator loss: 0.479662, acc: 0.652344]  [Adversarial loss: 0.418160, acc: 0.726562]\n",
            "1003: [Discriminator loss: 0.444151, acc: 0.699219]  [Adversarial loss: 0.361925, acc: 0.777344]\n",
            "1004: [Discriminator loss: 0.522898, acc: 0.648438]  [Adversarial loss: 0.417109, acc: 0.753906]\n",
            "1005: [Discriminator loss: 0.500508, acc: 0.640625]  [Adversarial loss: 0.489792, acc: 0.746094]\n",
            "1006: [Discriminator loss: 0.486291, acc: 0.656250]  [Adversarial loss: 0.387880, acc: 0.804688]\n",
            "1007: [Discriminator loss: 0.452181, acc: 0.687500]  [Adversarial loss: 0.425944, acc: 0.781250]\n",
            "1008: [Discriminator loss: 0.465887, acc: 0.628906]  [Adversarial loss: 0.411910, acc: 0.757812]\n",
            "1009: [Discriminator loss: 0.390795, acc: 0.714844]  [Adversarial loss: 0.377709, acc: 0.828125]\n",
            "1010: [Discriminator loss: 0.482475, acc: 0.636719]  [Adversarial loss: 0.412162, acc: 0.765625]\n",
            "1011: [Discriminator loss: 0.479084, acc: 0.625000]  [Adversarial loss: 0.400265, acc: 0.789062]\n",
            "1012: [Discriminator loss: 0.481152, acc: 0.644531]  [Adversarial loss: 0.443208, acc: 0.789062]\n",
            "1013: [Discriminator loss: 0.513004, acc: 0.636719]  [Adversarial loss: 0.408657, acc: 0.765625]\n",
            "1014: [Discriminator loss: 0.472356, acc: 0.636719]  [Adversarial loss: 0.423901, acc: 0.765625]\n",
            "1015: [Discriminator loss: 0.442924, acc: 0.652344]  [Adversarial loss: 0.383881, acc: 0.761719]\n",
            "1016: [Discriminator loss: 0.463439, acc: 0.636719]  [Adversarial loss: 0.450333, acc: 0.789062]\n",
            "1017: [Discriminator loss: 0.493659, acc: 0.632812]  [Adversarial loss: 0.425470, acc: 0.820312]\n",
            "1018: [Discriminator loss: 0.431632, acc: 0.707031]  [Adversarial loss: 0.401889, acc: 0.781250]\n",
            "1019: [Discriminator loss: 0.534364, acc: 0.617188]  [Adversarial loss: 0.422350, acc: 0.765625]\n",
            "1020: [Discriminator loss: 0.468216, acc: 0.621094]  [Adversarial loss: 0.410610, acc: 0.812500]\n",
            "1021: [Discriminator loss: 0.461505, acc: 0.628906]  [Adversarial loss: 0.405010, acc: 0.832031]\n",
            "1022: [Discriminator loss: 0.468337, acc: 0.609375]  [Adversarial loss: 0.424537, acc: 0.746094]\n",
            "1023: [Discriminator loss: 0.488005, acc: 0.625000]  [Adversarial loss: 0.398184, acc: 0.812500]\n",
            "1024: [Discriminator loss: 0.481210, acc: 0.640625]  [Adversarial loss: 0.465339, acc: 0.738281]\n",
            "1025: [Discriminator loss: 0.493713, acc: 0.617188]  [Adversarial loss: 0.458737, acc: 0.707031]\n",
            "1026: [Discriminator loss: 0.448471, acc: 0.656250]  [Adversarial loss: 0.396053, acc: 0.761719]\n",
            "1027: [Discriminator loss: 0.477887, acc: 0.660156]  [Adversarial loss: 0.403371, acc: 0.773438]\n",
            "1028: [Discriminator loss: 0.456777, acc: 0.660156]  [Adversarial loss: 0.406009, acc: 0.785156]\n",
            "1029: [Discriminator loss: 0.501047, acc: 0.625000]  [Adversarial loss: 0.420303, acc: 0.789062]\n",
            "1030: [Discriminator loss: 0.479226, acc: 0.664062]  [Adversarial loss: 0.444887, acc: 0.726562]\n",
            "1031: [Discriminator loss: 0.502495, acc: 0.609375]  [Adversarial loss: 0.435195, acc: 0.750000]\n",
            "1032: [Discriminator loss: 0.506892, acc: 0.589844]  [Adversarial loss: 0.452951, acc: 0.714844]\n",
            "1033: [Discriminator loss: 0.438981, acc: 0.660156]  [Adversarial loss: 0.374128, acc: 0.843750]\n",
            "1034: [Discriminator loss: 0.404468, acc: 0.718750]  [Adversarial loss: 0.370629, acc: 0.777344]\n",
            "1035: [Discriminator loss: 0.498581, acc: 0.613281]  [Adversarial loss: 0.433898, acc: 0.726562]\n",
            "1036: [Discriminator loss: 0.448925, acc: 0.636719]  [Adversarial loss: 0.417477, acc: 0.757812]\n",
            "1037: [Discriminator loss: 0.546117, acc: 0.636719]  [Adversarial loss: 0.405326, acc: 0.789062]\n",
            "1038: [Discriminator loss: 0.481174, acc: 0.695312]  [Adversarial loss: 0.444878, acc: 0.761719]\n",
            "1039: [Discriminator loss: 0.502531, acc: 0.605469]  [Adversarial loss: 0.434115, acc: 0.730469]\n",
            "1040: [Discriminator loss: 0.463837, acc: 0.664062]  [Adversarial loss: 0.384782, acc: 0.714844]\n",
            "1041: [Discriminator loss: 0.465549, acc: 0.667969]  [Adversarial loss: 0.428069, acc: 0.761719]\n",
            "1042: [Discriminator loss: 0.468717, acc: 0.671875]  [Adversarial loss: 0.422559, acc: 0.730469]\n",
            "1043: [Discriminator loss: 0.460195, acc: 0.640625]  [Adversarial loss: 0.432874, acc: 0.726562]\n",
            "1044: [Discriminator loss: 0.472160, acc: 0.679688]  [Adversarial loss: 0.415711, acc: 0.734375]\n",
            "1045: [Discriminator loss: 0.486287, acc: 0.652344]  [Adversarial loss: 0.408830, acc: 0.742188]\n",
            "1046: [Discriminator loss: 0.462223, acc: 0.667969]  [Adversarial loss: 0.445331, acc: 0.703125]\n",
            "1047: [Discriminator loss: 0.485893, acc: 0.683594]  [Adversarial loss: 0.456110, acc: 0.718750]\n",
            "1048: [Discriminator loss: 0.514892, acc: 0.648438]  [Adversarial loss: 0.467003, acc: 0.679688]\n",
            "1049: [Discriminator loss: 0.467885, acc: 0.660156]  [Adversarial loss: 0.419462, acc: 0.707031]\n",
            "1050: [Discriminator loss: 0.473392, acc: 0.644531]  [Adversarial loss: 0.416660, acc: 0.765625]\n",
            "1051: [Discriminator loss: 0.430574, acc: 0.710938]  [Adversarial loss: 0.403420, acc: 0.800781]\n",
            "1052: [Discriminator loss: 0.428711, acc: 0.710938]  [Adversarial loss: 0.380882, acc: 0.781250]\n",
            "1053: [Discriminator loss: 0.436327, acc: 0.710938]  [Adversarial loss: 0.386652, acc: 0.769531]\n",
            "1054: [Discriminator loss: 0.463810, acc: 0.738281]  [Adversarial loss: 0.388803, acc: 0.757812]\n",
            "1055: [Discriminator loss: 0.461814, acc: 0.667969]  [Adversarial loss: 0.438679, acc: 0.703125]\n",
            "1056: [Discriminator loss: 0.498743, acc: 0.656250]  [Adversarial loss: 0.434206, acc: 0.730469]\n",
            "1057: [Discriminator loss: 0.439153, acc: 0.707031]  [Adversarial loss: 0.397175, acc: 0.734375]\n",
            "1058: [Discriminator loss: 0.429144, acc: 0.707031]  [Adversarial loss: 0.363428, acc: 0.761719]\n",
            "1059: [Discriminator loss: 0.490063, acc: 0.695312]  [Adversarial loss: 0.428288, acc: 0.761719]\n",
            "1060: [Discriminator loss: 0.420197, acc: 0.718750]  [Adversarial loss: 0.391337, acc: 0.753906]\n",
            "1061: [Discriminator loss: 0.439234, acc: 0.675781]  [Adversarial loss: 0.403308, acc: 0.738281]\n",
            "1062: [Discriminator loss: 0.401103, acc: 0.703125]  [Adversarial loss: 0.360101, acc: 0.804688]\n",
            "1063: [Discriminator loss: 0.430152, acc: 0.722656]  [Adversarial loss: 0.381642, acc: 0.769531]\n",
            "1064: [Discriminator loss: 0.500705, acc: 0.691406]  [Adversarial loss: 0.418757, acc: 0.699219]\n",
            "1065: [Discriminator loss: 0.481211, acc: 0.683594]  [Adversarial loss: 0.400525, acc: 0.750000]\n",
            "1066: [Discriminator loss: 0.499158, acc: 0.675781]  [Adversarial loss: 0.395030, acc: 0.753906]\n",
            "1067: [Discriminator loss: 0.434411, acc: 0.726562]  [Adversarial loss: 0.420053, acc: 0.730469]\n",
            "1068: [Discriminator loss: 0.478376, acc: 0.628906]  [Adversarial loss: 0.442606, acc: 0.726562]\n",
            "1069: [Discriminator loss: 0.483255, acc: 0.652344]  [Adversarial loss: 0.456645, acc: 0.699219]\n",
            "1070: [Discriminator loss: 0.455320, acc: 0.683594]  [Adversarial loss: 0.405310, acc: 0.773438]\n",
            "1071: [Discriminator loss: 0.466951, acc: 0.667969]  [Adversarial loss: 0.414082, acc: 0.757812]\n",
            "1072: [Discriminator loss: 0.487672, acc: 0.664062]  [Adversarial loss: 0.466904, acc: 0.753906]\n",
            "1073: [Discriminator loss: 0.496740, acc: 0.660156]  [Adversarial loss: 0.474939, acc: 0.714844]\n",
            "1074: [Discriminator loss: 0.388298, acc: 0.738281]  [Adversarial loss: 0.426915, acc: 0.750000]\n",
            "1075: [Discriminator loss: 0.512403, acc: 0.617188]  [Adversarial loss: 0.413061, acc: 0.722656]\n",
            "1076: [Discriminator loss: 0.453820, acc: 0.671875]  [Adversarial loss: 0.400581, acc: 0.757812]\n",
            "1077: [Discriminator loss: 0.491459, acc: 0.675781]  [Adversarial loss: 0.459958, acc: 0.750000]\n",
            "1078: [Discriminator loss: 0.543263, acc: 0.636719]  [Adversarial loss: 0.428114, acc: 0.718750]\n",
            "1079: [Discriminator loss: 0.449350, acc: 0.687500]  [Adversarial loss: 0.407773, acc: 0.750000]\n",
            "1080: [Discriminator loss: 0.462740, acc: 0.667969]  [Adversarial loss: 0.405871, acc: 0.757812]\n",
            "1081: [Discriminator loss: 0.469403, acc: 0.664062]  [Adversarial loss: 0.406371, acc: 0.769531]\n",
            "1082: [Discriminator loss: 0.453036, acc: 0.710938]  [Adversarial loss: 0.412135, acc: 0.753906]\n",
            "1083: [Discriminator loss: 0.492981, acc: 0.644531]  [Adversarial loss: 0.440572, acc: 0.765625]\n",
            "1084: [Discriminator loss: 0.467417, acc: 0.656250]  [Adversarial loss: 0.384358, acc: 0.765625]\n",
            "1085: [Discriminator loss: 0.479919, acc: 0.695312]  [Adversarial loss: 0.426200, acc: 0.773438]\n",
            "1086: [Discriminator loss: 0.391515, acc: 0.707031]  [Adversarial loss: 0.340263, acc: 0.800781]\n",
            "1087: [Discriminator loss: 0.503016, acc: 0.675781]  [Adversarial loss: 0.433491, acc: 0.710938]\n",
            "1088: [Discriminator loss: 0.540618, acc: 0.609375]  [Adversarial loss: 0.435216, acc: 0.777344]\n",
            "1089: [Discriminator loss: 0.438256, acc: 0.699219]  [Adversarial loss: 0.402430, acc: 0.757812]\n",
            "1090: [Discriminator loss: 0.452297, acc: 0.679688]  [Adversarial loss: 0.436461, acc: 0.746094]\n",
            "1091: [Discriminator loss: 0.503435, acc: 0.691406]  [Adversarial loss: 0.414111, acc: 0.730469]\n",
            "1092: [Discriminator loss: 0.409584, acc: 0.734375]  [Adversarial loss: 0.456993, acc: 0.734375]\n",
            "1093: [Discriminator loss: 0.393087, acc: 0.757812]  [Adversarial loss: 0.424183, acc: 0.750000]\n",
            "1094: [Discriminator loss: 0.492038, acc: 0.640625]  [Adversarial loss: 0.419397, acc: 0.750000]\n",
            "1095: [Discriminator loss: 0.470650, acc: 0.683594]  [Adversarial loss: 0.412160, acc: 0.750000]\n",
            "1096: [Discriminator loss: 0.511627, acc: 0.710938]  [Adversarial loss: 0.420224, acc: 0.757812]\n",
            "1097: [Discriminator loss: 0.453217, acc: 0.683594]  [Adversarial loss: 0.418038, acc: 0.738281]\n",
            "1098: [Discriminator loss: 0.519019, acc: 0.691406]  [Adversarial loss: 0.399668, acc: 0.742188]\n",
            "1099: [Discriminator loss: 0.460936, acc: 0.699219]  [Adversarial loss: 0.416443, acc: 0.710938]\n",
            "1100: [Discriminator loss: 0.497842, acc: 0.726562]  [Adversarial loss: 0.400670, acc: 0.750000]\n",
            "1101: [Discriminator loss: 0.502629, acc: 0.671875]  [Adversarial loss: 0.417077, acc: 0.750000]\n",
            "1102: [Discriminator loss: 0.430855, acc: 0.683594]  [Adversarial loss: 0.366106, acc: 0.773438]\n",
            "1103: [Discriminator loss: 0.524796, acc: 0.664062]  [Adversarial loss: 0.445532, acc: 0.773438]\n",
            "1104: [Discriminator loss: 0.439521, acc: 0.644531]  [Adversarial loss: 0.400699, acc: 0.753906]\n",
            "1105: [Discriminator loss: 0.483427, acc: 0.667969]  [Adversarial loss: 0.471459, acc: 0.710938]\n",
            "1106: [Discriminator loss: 0.461068, acc: 0.660156]  [Adversarial loss: 0.454652, acc: 0.718750]\n",
            "1107: [Discriminator loss: 0.511615, acc: 0.636719]  [Adversarial loss: 0.443874, acc: 0.691406]\n",
            "1108: [Discriminator loss: 0.455646, acc: 0.683594]  [Adversarial loss: 0.451746, acc: 0.695312]\n",
            "1109: [Discriminator loss: 0.447140, acc: 0.703125]  [Adversarial loss: 0.410883, acc: 0.714844]\n",
            "1110: [Discriminator loss: 0.457032, acc: 0.691406]  [Adversarial loss: 0.445279, acc: 0.695312]\n",
            "1111: [Discriminator loss: 0.450287, acc: 0.679688]  [Adversarial loss: 0.382571, acc: 0.804688]\n",
            "1112: [Discriminator loss: 0.442069, acc: 0.671875]  [Adversarial loss: 0.361718, acc: 0.742188]\n",
            "1113: [Discriminator loss: 0.481636, acc: 0.652344]  [Adversarial loss: 0.422030, acc: 0.730469]\n",
            "1114: [Discriminator loss: 0.414457, acc: 0.769531]  [Adversarial loss: 0.398795, acc: 0.734375]\n",
            "1115: [Discriminator loss: 0.412993, acc: 0.699219]  [Adversarial loss: 0.409238, acc: 0.761719]\n",
            "1116: [Discriminator loss: 0.536983, acc: 0.609375]  [Adversarial loss: 0.456104, acc: 0.683594]\n",
            "1117: [Discriminator loss: 0.461107, acc: 0.714844]  [Adversarial loss: 0.466998, acc: 0.710938]\n",
            "1118: [Discriminator loss: 0.498851, acc: 0.667969]  [Adversarial loss: 0.457923, acc: 0.699219]\n",
            "1119: [Discriminator loss: 0.437111, acc: 0.695312]  [Adversarial loss: 0.427649, acc: 0.734375]\n",
            "1120: [Discriminator loss: 0.488402, acc: 0.679688]  [Adversarial loss: 0.439692, acc: 0.750000]\n",
            "1121: [Discriminator loss: 0.442463, acc: 0.687500]  [Adversarial loss: 0.392340, acc: 0.769531]\n",
            "1122: [Discriminator loss: 0.417236, acc: 0.734375]  [Adversarial loss: 0.367000, acc: 0.753906]\n",
            "1123: [Discriminator loss: 0.462597, acc: 0.656250]  [Adversarial loss: 0.401515, acc: 0.761719]\n",
            "1124: [Discriminator loss: 0.433829, acc: 0.726562]  [Adversarial loss: 0.439600, acc: 0.750000]\n",
            "1125: [Discriminator loss: 0.446457, acc: 0.710938]  [Adversarial loss: 0.423083, acc: 0.746094]\n",
            "1126: [Discriminator loss: 0.514167, acc: 0.613281]  [Adversarial loss: 0.493456, acc: 0.738281]\n",
            "1127: [Discriminator loss: 0.505058, acc: 0.687500]  [Adversarial loss: 0.382382, acc: 0.734375]\n",
            "1128: [Discriminator loss: 0.506119, acc: 0.652344]  [Adversarial loss: 0.418830, acc: 0.742188]\n",
            "1129: [Discriminator loss: 0.479949, acc: 0.687500]  [Adversarial loss: 0.426641, acc: 0.710938]\n",
            "1130: [Discriminator loss: 0.421809, acc: 0.718750]  [Adversarial loss: 0.372294, acc: 0.765625]\n",
            "1131: [Discriminator loss: 0.479134, acc: 0.691406]  [Adversarial loss: 0.383045, acc: 0.722656]\n",
            "1132: [Discriminator loss: 0.516725, acc: 0.648438]  [Adversarial loss: 0.421745, acc: 0.761719]\n",
            "1133: [Discriminator loss: 0.408708, acc: 0.730469]  [Adversarial loss: 0.377289, acc: 0.761719]\n",
            "1134: [Discriminator loss: 0.438153, acc: 0.714844]  [Adversarial loss: 0.389400, acc: 0.734375]\n",
            "1135: [Discriminator loss: 0.448831, acc: 0.699219]  [Adversarial loss: 0.448980, acc: 0.707031]\n",
            "1136: [Discriminator loss: 0.476668, acc: 0.652344]  [Adversarial loss: 0.494196, acc: 0.714844]\n",
            "1137: [Discriminator loss: 0.438825, acc: 0.742188]  [Adversarial loss: 0.388849, acc: 0.714844]\n",
            "1138: [Discriminator loss: 0.447527, acc: 0.664062]  [Adversarial loss: 0.411573, acc: 0.742188]\n",
            "1139: [Discriminator loss: 0.486148, acc: 0.667969]  [Adversarial loss: 0.366006, acc: 0.765625]\n",
            "1140: [Discriminator loss: 0.460476, acc: 0.707031]  [Adversarial loss: 0.386571, acc: 0.785156]\n",
            "1141: [Discriminator loss: 0.472917, acc: 0.625000]  [Adversarial loss: 0.417668, acc: 0.742188]\n",
            "1142: [Discriminator loss: 0.460127, acc: 0.644531]  [Adversarial loss: 0.422141, acc: 0.773438]\n",
            "1143: [Discriminator loss: 0.514037, acc: 0.644531]  [Adversarial loss: 0.413387, acc: 0.746094]\n",
            "1144: [Discriminator loss: 0.458598, acc: 0.656250]  [Adversarial loss: 0.368686, acc: 0.792969]\n",
            "1145: [Discriminator loss: 0.507952, acc: 0.644531]  [Adversarial loss: 0.457936, acc: 0.734375]\n",
            "1146: [Discriminator loss: 0.514587, acc: 0.582031]  [Adversarial loss: 0.444414, acc: 0.742188]\n",
            "1147: [Discriminator loss: 0.551292, acc: 0.648438]  [Adversarial loss: 0.417054, acc: 0.757812]\n",
            "1148: [Discriminator loss: 0.479127, acc: 0.636719]  [Adversarial loss: 0.420175, acc: 0.742188]\n",
            "1149: [Discriminator loss: 0.463921, acc: 0.660156]  [Adversarial loss: 0.414590, acc: 0.781250]\n",
            "1150: [Discriminator loss: 0.486735, acc: 0.640625]  [Adversarial loss: 0.472671, acc: 0.757812]\n",
            "1151: [Discriminator loss: 0.445969, acc: 0.699219]  [Adversarial loss: 0.416352, acc: 0.718750]\n",
            "1152: [Discriminator loss: 0.487652, acc: 0.687500]  [Adversarial loss: 0.410469, acc: 0.703125]\n",
            "1153: [Discriminator loss: 0.463442, acc: 0.687500]  [Adversarial loss: 0.434370, acc: 0.691406]\n",
            "1154: [Discriminator loss: 0.460482, acc: 0.648438]  [Adversarial loss: 0.423971, acc: 0.695312]\n",
            "1155: [Discriminator loss: 0.475692, acc: 0.652344]  [Adversarial loss: 0.492403, acc: 0.699219]\n",
            "1156: [Discriminator loss: 0.495299, acc: 0.671875]  [Adversarial loss: 0.446136, acc: 0.710938]\n",
            "1157: [Discriminator loss: 0.456773, acc: 0.730469]  [Adversarial loss: 0.415397, acc: 0.722656]\n",
            "1158: [Discriminator loss: 0.502126, acc: 0.687500]  [Adversarial loss: 0.432738, acc: 0.726562]\n",
            "1159: [Discriminator loss: 0.453013, acc: 0.664062]  [Adversarial loss: 0.399347, acc: 0.792969]\n",
            "1160: [Discriminator loss: 0.453603, acc: 0.714844]  [Adversarial loss: 0.378468, acc: 0.753906]\n",
            "1161: [Discriminator loss: 0.458757, acc: 0.679688]  [Adversarial loss: 0.441753, acc: 0.722656]\n",
            "1162: [Discriminator loss: 0.397560, acc: 0.718750]  [Adversarial loss: 0.370274, acc: 0.792969]\n",
            "1163: [Discriminator loss: 0.485494, acc: 0.644531]  [Adversarial loss: 0.436102, acc: 0.722656]\n",
            "1164: [Discriminator loss: 0.454572, acc: 0.699219]  [Adversarial loss: 0.432756, acc: 0.765625]\n",
            "1165: [Discriminator loss: 0.476261, acc: 0.675781]  [Adversarial loss: 0.407070, acc: 0.757812]\n",
            "1166: [Discriminator loss: 0.470134, acc: 0.664062]  [Adversarial loss: 0.436485, acc: 0.734375]\n",
            "1167: [Discriminator loss: 0.469955, acc: 0.710938]  [Adversarial loss: 0.395930, acc: 0.769531]\n",
            "1168: [Discriminator loss: 0.425677, acc: 0.695312]  [Adversarial loss: 0.384553, acc: 0.773438]\n",
            "1169: [Discriminator loss: 0.423358, acc: 0.691406]  [Adversarial loss: 0.386734, acc: 0.765625]\n",
            "1170: [Discriminator loss: 0.484059, acc: 0.648438]  [Adversarial loss: 0.389262, acc: 0.750000]\n",
            "1171: [Discriminator loss: 0.437752, acc: 0.671875]  [Adversarial loss: 0.389940, acc: 0.738281]\n",
            "1172: [Discriminator loss: 0.502970, acc: 0.695312]  [Adversarial loss: 0.383237, acc: 0.738281]\n",
            "1173: [Discriminator loss: 0.475628, acc: 0.652344]  [Adversarial loss: 0.436592, acc: 0.699219]\n",
            "1174: [Discriminator loss: 0.488293, acc: 0.617188]  [Adversarial loss: 0.407865, acc: 0.691406]\n",
            "1175: [Discriminator loss: 0.439524, acc: 0.679688]  [Adversarial loss: 0.406862, acc: 0.738281]\n",
            "1176: [Discriminator loss: 0.375524, acc: 0.738281]  [Adversarial loss: 0.325111, acc: 0.781250]\n",
            "1177: [Discriminator loss: 0.440092, acc: 0.710938]  [Adversarial loss: 0.355531, acc: 0.773438]\n",
            "1178: [Discriminator loss: 0.458457, acc: 0.714844]  [Adversarial loss: 0.396964, acc: 0.730469]\n",
            "1179: [Discriminator loss: 0.472611, acc: 0.648438]  [Adversarial loss: 0.390734, acc: 0.750000]\n",
            "1180: [Discriminator loss: 0.458304, acc: 0.683594]  [Adversarial loss: 0.394121, acc: 0.718750]\n",
            "1181: [Discriminator loss: 0.442500, acc: 0.726562]  [Adversarial loss: 0.420656, acc: 0.746094]\n",
            "1182: [Discriminator loss: 0.413370, acc: 0.707031]  [Adversarial loss: 0.386976, acc: 0.761719]\n",
            "1183: [Discriminator loss: 0.470865, acc: 0.664062]  [Adversarial loss: 0.427362, acc: 0.691406]\n",
            "1184: [Discriminator loss: 0.461768, acc: 0.695312]  [Adversarial loss: 0.388205, acc: 0.746094]\n",
            "1185: [Discriminator loss: 0.457118, acc: 0.734375]  [Adversarial loss: 0.371586, acc: 0.769531]\n",
            "1186: [Discriminator loss: 0.434318, acc: 0.703125]  [Adversarial loss: 0.400596, acc: 0.769531]\n",
            "1187: [Discriminator loss: 0.466840, acc: 0.722656]  [Adversarial loss: 0.397033, acc: 0.730469]\n",
            "1188: [Discriminator loss: 0.448374, acc: 0.687500]  [Adversarial loss: 0.362839, acc: 0.761719]\n",
            "1189: [Discriminator loss: 0.509815, acc: 0.660156]  [Adversarial loss: 0.420109, acc: 0.753906]\n",
            "1190: [Discriminator loss: 0.462230, acc: 0.679688]  [Adversarial loss: 0.387488, acc: 0.738281]\n",
            "1191: [Discriminator loss: 0.402005, acc: 0.734375]  [Adversarial loss: 0.340575, acc: 0.769531]\n",
            "1192: [Discriminator loss: 0.484730, acc: 0.675781]  [Adversarial loss: 0.374577, acc: 0.777344]\n",
            "1193: [Discriminator loss: 0.441950, acc: 0.691406]  [Adversarial loss: 0.416081, acc: 0.707031]\n",
            "1194: [Discriminator loss: 0.439191, acc: 0.687500]  [Adversarial loss: 0.381837, acc: 0.742188]\n",
            "1195: [Discriminator loss: 0.501080, acc: 0.691406]  [Adversarial loss: 0.457610, acc: 0.695312]\n",
            "1196: [Discriminator loss: 0.426791, acc: 0.695312]  [Adversarial loss: 0.383853, acc: 0.730469]\n",
            "1197: [Discriminator loss: 0.470467, acc: 0.710938]  [Adversarial loss: 0.420099, acc: 0.714844]\n",
            "1198: [Discriminator loss: 0.425241, acc: 0.714844]  [Adversarial loss: 0.428021, acc: 0.714844]\n",
            "1199: [Discriminator loss: 0.411139, acc: 0.753906]  [Adversarial loss: 0.407849, acc: 0.718750]\n",
            "1200: [Discriminator loss: 0.497935, acc: 0.667969]  [Adversarial loss: 0.469602, acc: 0.710938]\n",
            "1201: [Discriminator loss: 0.410687, acc: 0.738281]  [Adversarial loss: 0.359450, acc: 0.753906]\n",
            "1202: [Discriminator loss: 0.474369, acc: 0.695312]  [Adversarial loss: 0.400572, acc: 0.750000]\n",
            "1203: [Discriminator loss: 0.416022, acc: 0.695312]  [Adversarial loss: 0.403311, acc: 0.738281]\n",
            "1204: [Discriminator loss: 0.426449, acc: 0.675781]  [Adversarial loss: 0.382421, acc: 0.757812]\n",
            "1205: [Discriminator loss: 0.432140, acc: 0.722656]  [Adversarial loss: 0.453091, acc: 0.718750]\n",
            "1206: [Discriminator loss: 0.447380, acc: 0.683594]  [Adversarial loss: 0.393619, acc: 0.664062]\n",
            "1207: [Discriminator loss: 0.459519, acc: 0.707031]  [Adversarial loss: 0.367036, acc: 0.738281]\n",
            "1208: [Discriminator loss: 0.500490, acc: 0.730469]  [Adversarial loss: 0.439573, acc: 0.703125]\n",
            "1209: [Discriminator loss: 0.447548, acc: 0.695312]  [Adversarial loss: 0.410177, acc: 0.695312]\n",
            "1210: [Discriminator loss: 0.435381, acc: 0.710938]  [Adversarial loss: 0.380165, acc: 0.750000]\n",
            "1211: [Discriminator loss: 0.473646, acc: 0.703125]  [Adversarial loss: 0.430440, acc: 0.738281]\n",
            "1212: [Discriminator loss: 0.431195, acc: 0.730469]  [Adversarial loss: 0.449454, acc: 0.710938]\n",
            "1213: [Discriminator loss: 0.390861, acc: 0.789062]  [Adversarial loss: 0.364125, acc: 0.789062]\n",
            "1214: [Discriminator loss: 0.499568, acc: 0.640625]  [Adversarial loss: 0.458056, acc: 0.718750]\n",
            "1215: [Discriminator loss: 0.417074, acc: 0.714844]  [Adversarial loss: 0.411118, acc: 0.734375]\n",
            "1216: [Discriminator loss: 0.426207, acc: 0.683594]  [Adversarial loss: 0.436313, acc: 0.691406]\n",
            "1217: [Discriminator loss: 0.450540, acc: 0.683594]  [Adversarial loss: 0.402062, acc: 0.722656]\n",
            "1218: [Discriminator loss: 0.448902, acc: 0.691406]  [Adversarial loss: 0.419492, acc: 0.671875]\n",
            "1219: [Discriminator loss: 0.434200, acc: 0.695312]  [Adversarial loss: 0.400315, acc: 0.722656]\n",
            "1220: [Discriminator loss: 0.454029, acc: 0.687500]  [Adversarial loss: 0.425147, acc: 0.750000]\n",
            "1221: [Discriminator loss: 0.426255, acc: 0.777344]  [Adversarial loss: 0.389150, acc: 0.703125]\n",
            "1222: [Discriminator loss: 0.425893, acc: 0.746094]  [Adversarial loss: 0.376274, acc: 0.710938]\n",
            "1223: [Discriminator loss: 0.446358, acc: 0.714844]  [Adversarial loss: 0.428012, acc: 0.734375]\n",
            "1224: [Discriminator loss: 0.415461, acc: 0.722656]  [Adversarial loss: 0.403644, acc: 0.734375]\n",
            "1225: [Discriminator loss: 0.422506, acc: 0.722656]  [Adversarial loss: 0.369881, acc: 0.750000]\n",
            "1226: [Discriminator loss: 0.473441, acc: 0.695312]  [Adversarial loss: 0.369281, acc: 0.746094]\n",
            "1227: [Discriminator loss: 0.443338, acc: 0.714844]  [Adversarial loss: 0.384259, acc: 0.750000]\n",
            "1228: [Discriminator loss: 0.394790, acc: 0.714844]  [Adversarial loss: 0.401214, acc: 0.750000]\n",
            "1229: [Discriminator loss: 0.467297, acc: 0.699219]  [Adversarial loss: 0.386403, acc: 0.761719]\n",
            "1230: [Discriminator loss: 0.436581, acc: 0.679688]  [Adversarial loss: 0.376521, acc: 0.746094]\n",
            "1231: [Discriminator loss: 0.463531, acc: 0.718750]  [Adversarial loss: 0.406467, acc: 0.734375]\n",
            "1232: [Discriminator loss: 0.414394, acc: 0.730469]  [Adversarial loss: 0.387669, acc: 0.730469]\n",
            "1233: [Discriminator loss: 0.439711, acc: 0.664062]  [Adversarial loss: 0.372757, acc: 0.777344]\n",
            "1234: [Discriminator loss: 0.440471, acc: 0.726562]  [Adversarial loss: 0.413286, acc: 0.710938]\n",
            "1235: [Discriminator loss: 0.431405, acc: 0.730469]  [Adversarial loss: 0.359308, acc: 0.742188]\n",
            "1236: [Discriminator loss: 0.389542, acc: 0.742188]  [Adversarial loss: 0.371890, acc: 0.765625]\n",
            "1237: [Discriminator loss: 0.485525, acc: 0.636719]  [Adversarial loss: 0.428113, acc: 0.699219]\n",
            "1238: [Discriminator loss: 0.380307, acc: 0.742188]  [Adversarial loss: 0.339396, acc: 0.792969]\n",
            "1239: [Discriminator loss: 0.474288, acc: 0.679688]  [Adversarial loss: 0.412055, acc: 0.765625]\n",
            "1240: [Discriminator loss: 0.457802, acc: 0.691406]  [Adversarial loss: 0.419900, acc: 0.730469]\n",
            "1241: [Discriminator loss: 0.422216, acc: 0.695312]  [Adversarial loss: 0.397374, acc: 0.769531]\n",
            "1242: [Discriminator loss: 0.400906, acc: 0.707031]  [Adversarial loss: 0.362217, acc: 0.773438]\n",
            "1243: [Discriminator loss: 0.436846, acc: 0.648438]  [Adversarial loss: 0.380653, acc: 0.761719]\n",
            "1244: [Discriminator loss: 0.457665, acc: 0.679688]  [Adversarial loss: 0.408319, acc: 0.742188]\n",
            "1245: [Discriminator loss: 0.409235, acc: 0.734375]  [Adversarial loss: 0.380458, acc: 0.750000]\n",
            "1246: [Discriminator loss: 0.461275, acc: 0.667969]  [Adversarial loss: 0.470857, acc: 0.687500]\n",
            "1247: [Discriminator loss: 0.456039, acc: 0.671875]  [Adversarial loss: 0.412737, acc: 0.718750]\n",
            "1248: [Discriminator loss: 0.434712, acc: 0.699219]  [Adversarial loss: 0.389088, acc: 0.753906]\n",
            "1249: [Discriminator loss: 0.413882, acc: 0.722656]  [Adversarial loss: 0.357848, acc: 0.808594]\n",
            "1250: [Discriminator loss: 0.416197, acc: 0.722656]  [Adversarial loss: 0.400178, acc: 0.714844]\n",
            "1251: [Discriminator loss: 0.385312, acc: 0.707031]  [Adversarial loss: 0.361465, acc: 0.761719]\n",
            "1252: [Discriminator loss: 0.412679, acc: 0.695312]  [Adversarial loss: 0.404258, acc: 0.699219]\n",
            "1253: [Discriminator loss: 0.437067, acc: 0.707031]  [Adversarial loss: 0.415397, acc: 0.734375]\n",
            "1254: [Discriminator loss: 0.451347, acc: 0.679688]  [Adversarial loss: 0.402001, acc: 0.726562]\n",
            "1255: [Discriminator loss: 0.471806, acc: 0.695312]  [Adversarial loss: 0.423229, acc: 0.761719]\n",
            "1256: [Discriminator loss: 0.456733, acc: 0.667969]  [Adversarial loss: 0.425187, acc: 0.753906]\n",
            "1257: [Discriminator loss: 0.433476, acc: 0.679688]  [Adversarial loss: 0.379179, acc: 0.765625]\n",
            "1258: [Discriminator loss: 0.444861, acc: 0.691406]  [Adversarial loss: 0.404482, acc: 0.738281]\n",
            "1259: [Discriminator loss: 0.390899, acc: 0.703125]  [Adversarial loss: 0.378294, acc: 0.734375]\n",
            "1260: [Discriminator loss: 0.437980, acc: 0.656250]  [Adversarial loss: 0.374081, acc: 0.730469]\n",
            "1261: [Discriminator loss: 0.441239, acc: 0.691406]  [Adversarial loss: 0.401520, acc: 0.730469]\n",
            "1262: [Discriminator loss: 0.437100, acc: 0.714844]  [Adversarial loss: 0.402022, acc: 0.726562]\n",
            "1263: [Discriminator loss: 0.458858, acc: 0.691406]  [Adversarial loss: 0.398518, acc: 0.742188]\n",
            "1264: [Discriminator loss: 0.444654, acc: 0.695312]  [Adversarial loss: 0.439398, acc: 0.703125]\n",
            "1265: [Discriminator loss: 0.433231, acc: 0.683594]  [Adversarial loss: 0.392704, acc: 0.757812]\n",
            "1266: [Discriminator loss: 0.440075, acc: 0.730469]  [Adversarial loss: 0.490227, acc: 0.750000]\n",
            "1267: [Discriminator loss: 0.433519, acc: 0.726562]  [Adversarial loss: 0.393143, acc: 0.742188]\n",
            "1268: [Discriminator loss: 0.475968, acc: 0.687500]  [Adversarial loss: 0.407594, acc: 0.753906]\n",
            "1269: [Discriminator loss: 0.378308, acc: 0.707031]  [Adversarial loss: 0.366189, acc: 0.773438]\n",
            "1270: [Discriminator loss: 0.436828, acc: 0.683594]  [Adversarial loss: 0.428053, acc: 0.734375]\n",
            "1271: [Discriminator loss: 0.443642, acc: 0.648438]  [Adversarial loss: 0.376996, acc: 0.750000]\n",
            "1272: [Discriminator loss: 0.409542, acc: 0.683594]  [Adversarial loss: 0.369882, acc: 0.812500]\n",
            "1273: [Discriminator loss: 0.387355, acc: 0.703125]  [Adversarial loss: 0.336736, acc: 0.832031]\n",
            "1274: [Discriminator loss: 0.428839, acc: 0.652344]  [Adversarial loss: 0.388418, acc: 0.765625]\n",
            "1275: [Discriminator loss: 0.411603, acc: 0.714844]  [Adversarial loss: 0.381279, acc: 0.761719]\n",
            "1276: [Discriminator loss: 0.405856, acc: 0.679688]  [Adversarial loss: 0.360913, acc: 0.800781]\n",
            "1277: [Discriminator loss: 0.466420, acc: 0.652344]  [Adversarial loss: 0.376243, acc: 0.785156]\n",
            "1278: [Discriminator loss: 0.444464, acc: 0.613281]  [Adversarial loss: 0.375452, acc: 0.746094]\n",
            "1279: [Discriminator loss: 0.454429, acc: 0.636719]  [Adversarial loss: 0.414247, acc: 0.722656]\n",
            "1280: [Discriminator loss: 0.432028, acc: 0.679688]  [Adversarial loss: 0.397162, acc: 0.789062]\n",
            "1281: [Discriminator loss: 0.444268, acc: 0.644531]  [Adversarial loss: 0.444308, acc: 0.687500]\n",
            "1282: [Discriminator loss: 0.428666, acc: 0.656250]  [Adversarial loss: 0.388926, acc: 0.757812]\n",
            "1283: [Discriminator loss: 0.443575, acc: 0.648438]  [Adversarial loss: 0.398327, acc: 0.742188]\n",
            "1284: [Discriminator loss: 0.436160, acc: 0.691406]  [Adversarial loss: 0.460112, acc: 0.742188]\n",
            "1285: [Discriminator loss: 0.468297, acc: 0.656250]  [Adversarial loss: 0.474983, acc: 0.757812]\n",
            "1286: [Discriminator loss: 0.435719, acc: 0.656250]  [Adversarial loss: 0.405881, acc: 0.761719]\n",
            "1287: [Discriminator loss: 0.433223, acc: 0.707031]  [Adversarial loss: 0.408384, acc: 0.738281]\n",
            "1288: [Discriminator loss: 0.402285, acc: 0.714844]  [Adversarial loss: 0.407174, acc: 0.750000]\n",
            "1289: [Discriminator loss: 0.496113, acc: 0.660156]  [Adversarial loss: 0.403053, acc: 0.722656]\n",
            "1290: [Discriminator loss: 0.414120, acc: 0.703125]  [Adversarial loss: 0.374055, acc: 0.769531]\n",
            "1291: [Discriminator loss: 0.448722, acc: 0.671875]  [Adversarial loss: 0.373375, acc: 0.792969]\n",
            "1292: [Discriminator loss: 0.422702, acc: 0.746094]  [Adversarial loss: 0.428625, acc: 0.726562]\n",
            "1293: [Discriminator loss: 0.419740, acc: 0.726562]  [Adversarial loss: 0.366707, acc: 0.761719]\n",
            "1294: [Discriminator loss: 0.396408, acc: 0.746094]  [Adversarial loss: 0.391548, acc: 0.750000]\n",
            "1295: [Discriminator loss: 0.439308, acc: 0.683594]  [Adversarial loss: 0.438025, acc: 0.746094]\n",
            "1296: [Discriminator loss: 0.455245, acc: 0.683594]  [Adversarial loss: 0.387028, acc: 0.753906]\n",
            "1297: [Discriminator loss: 0.460808, acc: 0.660156]  [Adversarial loss: 0.374099, acc: 0.777344]\n",
            "1298: [Discriminator loss: 0.470411, acc: 0.632812]  [Adversarial loss: 0.417958, acc: 0.761719]\n",
            "1299: [Discriminator loss: 0.479478, acc: 0.664062]  [Adversarial loss: 0.437128, acc: 0.726562]\n",
            "1300: [Discriminator loss: 0.379697, acc: 0.750000]  [Adversarial loss: 0.346271, acc: 0.765625]\n",
            "1301: [Discriminator loss: 0.435342, acc: 0.691406]  [Adversarial loss: 0.353255, acc: 0.750000]\n",
            "1302: [Discriminator loss: 0.456621, acc: 0.687500]  [Adversarial loss: 0.399136, acc: 0.734375]\n",
            "1303: [Discriminator loss: 0.392892, acc: 0.722656]  [Adversarial loss: 0.370885, acc: 0.730469]\n",
            "1304: [Discriminator loss: 0.384772, acc: 0.722656]  [Adversarial loss: 0.349615, acc: 0.769531]\n",
            "1305: [Discriminator loss: 0.455956, acc: 0.714844]  [Adversarial loss: 0.363171, acc: 0.773438]\n",
            "1306: [Discriminator loss: 0.417075, acc: 0.738281]  [Adversarial loss: 0.363947, acc: 0.765625]\n",
            "1307: [Discriminator loss: 0.389560, acc: 0.710938]  [Adversarial loss: 0.389652, acc: 0.769531]\n",
            "1308: [Discriminator loss: 0.492754, acc: 0.695312]  [Adversarial loss: 0.439078, acc: 0.742188]\n",
            "1309: [Discriminator loss: 0.431389, acc: 0.718750]  [Adversarial loss: 0.436565, acc: 0.738281]\n",
            "1310: [Discriminator loss: 0.441623, acc: 0.726562]  [Adversarial loss: 0.408394, acc: 0.738281]\n",
            "1311: [Discriminator loss: 0.397513, acc: 0.761719]  [Adversarial loss: 0.386372, acc: 0.777344]\n",
            "1312: [Discriminator loss: 0.430669, acc: 0.679688]  [Adversarial loss: 0.370637, acc: 0.738281]\n",
            "1313: [Discriminator loss: 0.477270, acc: 0.683594]  [Adversarial loss: 0.498461, acc: 0.675781]\n",
            "1314: [Discriminator loss: 0.419769, acc: 0.742188]  [Adversarial loss: 0.415774, acc: 0.722656]\n",
            "1315: [Discriminator loss: 0.425724, acc: 0.726562]  [Adversarial loss: 0.370756, acc: 0.781250]\n",
            "1316: [Discriminator loss: 0.383636, acc: 0.718750]  [Adversarial loss: 0.368387, acc: 0.761719]\n",
            "1317: [Discriminator loss: 0.455620, acc: 0.683594]  [Adversarial loss: 0.420237, acc: 0.742188]\n",
            "1318: [Discriminator loss: 0.441120, acc: 0.648438]  [Adversarial loss: 0.391303, acc: 0.757812]\n",
            "1319: [Discriminator loss: 0.430625, acc: 0.710938]  [Adversarial loss: 0.455424, acc: 0.773438]\n",
            "1320: [Discriminator loss: 0.462956, acc: 0.703125]  [Adversarial loss: 0.434120, acc: 0.781250]\n",
            "1321: [Discriminator loss: 0.408269, acc: 0.730469]  [Adversarial loss: 0.386859, acc: 0.777344]\n",
            "1322: [Discriminator loss: 0.425567, acc: 0.652344]  [Adversarial loss: 0.460014, acc: 0.757812]\n",
            "1323: [Discriminator loss: 0.396079, acc: 0.710938]  [Adversarial loss: 0.363326, acc: 0.808594]\n",
            "1324: [Discriminator loss: 0.444339, acc: 0.707031]  [Adversarial loss: 0.386078, acc: 0.777344]\n",
            "1325: [Discriminator loss: 0.447624, acc: 0.667969]  [Adversarial loss: 0.416047, acc: 0.777344]\n",
            "1326: [Discriminator loss: 0.446177, acc: 0.675781]  [Adversarial loss: 0.398616, acc: 0.742188]\n",
            "1327: [Discriminator loss: 0.433079, acc: 0.683594]  [Adversarial loss: 0.409972, acc: 0.718750]\n",
            "1328: [Discriminator loss: 0.457723, acc: 0.699219]  [Adversarial loss: 0.392317, acc: 0.765625]\n",
            "1329: [Discriminator loss: 0.366019, acc: 0.746094]  [Adversarial loss: 0.361214, acc: 0.781250]\n",
            "1330: [Discriminator loss: 0.460691, acc: 0.714844]  [Adversarial loss: 0.364247, acc: 0.816406]\n",
            "1331: [Discriminator loss: 0.407830, acc: 0.691406]  [Adversarial loss: 0.384102, acc: 0.808594]\n",
            "1332: [Discriminator loss: 0.454541, acc: 0.687500]  [Adversarial loss: 0.389500, acc: 0.781250]\n",
            "1333: [Discriminator loss: 0.493014, acc: 0.628906]  [Adversarial loss: 0.406583, acc: 0.757812]\n",
            "1334: [Discriminator loss: 0.429389, acc: 0.667969]  [Adversarial loss: 0.414274, acc: 0.742188]\n",
            "1335: [Discriminator loss: 0.388425, acc: 0.699219]  [Adversarial loss: 0.345075, acc: 0.804688]\n",
            "1336: [Discriminator loss: 0.414066, acc: 0.687500]  [Adversarial loss: 0.355876, acc: 0.820312]\n",
            "1337: [Discriminator loss: 0.480943, acc: 0.664062]  [Adversarial loss: 0.398900, acc: 0.777344]\n",
            "1338: [Discriminator loss: 0.411009, acc: 0.695312]  [Adversarial loss: 0.437600, acc: 0.738281]\n",
            "1339: [Discriminator loss: 0.471265, acc: 0.636719]  [Adversarial loss: 0.406637, acc: 0.777344]\n",
            "1340: [Discriminator loss: 0.443231, acc: 0.679688]  [Adversarial loss: 0.409152, acc: 0.750000]\n",
            "1341: [Discriminator loss: 0.436986, acc: 0.652344]  [Adversarial loss: 0.436915, acc: 0.734375]\n",
            "1342: [Discriminator loss: 0.401463, acc: 0.695312]  [Adversarial loss: 0.417987, acc: 0.746094]\n",
            "1343: [Discriminator loss: 0.456507, acc: 0.710938]  [Adversarial loss: 0.410824, acc: 0.734375]\n",
            "1344: [Discriminator loss: 0.463074, acc: 0.703125]  [Adversarial loss: 0.357650, acc: 0.812500]\n",
            "1345: [Discriminator loss: 0.485727, acc: 0.660156]  [Adversarial loss: 0.429692, acc: 0.718750]\n",
            "1346: [Discriminator loss: 0.452132, acc: 0.656250]  [Adversarial loss: 0.455057, acc: 0.765625]\n",
            "1347: [Discriminator loss: 0.423145, acc: 0.757812]  [Adversarial loss: 0.352119, acc: 0.804688]\n",
            "1348: [Discriminator loss: 0.430819, acc: 0.718750]  [Adversarial loss: 0.356111, acc: 0.781250]\n",
            "1349: [Discriminator loss: 0.462878, acc: 0.679688]  [Adversarial loss: 0.426531, acc: 0.761719]\n",
            "1350: [Discriminator loss: 0.458564, acc: 0.671875]  [Adversarial loss: 0.429897, acc: 0.718750]\n",
            "1351: [Discriminator loss: 0.438415, acc: 0.648438]  [Adversarial loss: 0.403420, acc: 0.757812]\n",
            "1352: [Discriminator loss: 0.428734, acc: 0.738281]  [Adversarial loss: 0.406921, acc: 0.742188]\n",
            "1353: [Discriminator loss: 0.440933, acc: 0.703125]  [Adversarial loss: 0.414656, acc: 0.769531]\n",
            "1354: [Discriminator loss: 0.416662, acc: 0.707031]  [Adversarial loss: 0.343239, acc: 0.800781]\n",
            "1355: [Discriminator loss: 0.457446, acc: 0.671875]  [Adversarial loss: 0.403390, acc: 0.761719]\n",
            "1356: [Discriminator loss: 0.462877, acc: 0.648438]  [Adversarial loss: 0.387954, acc: 0.816406]\n",
            "1357: [Discriminator loss: 0.464435, acc: 0.648438]  [Adversarial loss: 0.383938, acc: 0.765625]\n",
            "1358: [Discriminator loss: 0.478008, acc: 0.664062]  [Adversarial loss: 0.361097, acc: 0.792969]\n",
            "1359: [Discriminator loss: 0.424985, acc: 0.664062]  [Adversarial loss: 0.408297, acc: 0.773438]\n",
            "1360: [Discriminator loss: 0.421846, acc: 0.691406]  [Adversarial loss: 0.372410, acc: 0.742188]\n",
            "1361: [Discriminator loss: 0.413657, acc: 0.699219]  [Adversarial loss: 0.379610, acc: 0.722656]\n",
            "1362: [Discriminator loss: 0.458336, acc: 0.695312]  [Adversarial loss: 0.404427, acc: 0.714844]\n",
            "1363: [Discriminator loss: 0.434322, acc: 0.710938]  [Adversarial loss: 0.398616, acc: 0.769531]\n",
            "1364: [Discriminator loss: 0.433410, acc: 0.726562]  [Adversarial loss: 0.407103, acc: 0.726562]\n",
            "1365: [Discriminator loss: 0.441429, acc: 0.707031]  [Adversarial loss: 0.426864, acc: 0.738281]\n",
            "1366: [Discriminator loss: 0.485771, acc: 0.648438]  [Adversarial loss: 0.457550, acc: 0.679688]\n",
            "1367: [Discriminator loss: 0.452226, acc: 0.675781]  [Adversarial loss: 0.462674, acc: 0.707031]\n",
            "1368: [Discriminator loss: 0.449883, acc: 0.710938]  [Adversarial loss: 0.435843, acc: 0.730469]\n",
            "1369: [Discriminator loss: 0.517882, acc: 0.667969]  [Adversarial loss: 0.465468, acc: 0.742188]\n",
            "1370: [Discriminator loss: 0.472427, acc: 0.691406]  [Adversarial loss: 0.461485, acc: 0.730469]\n",
            "1371: [Discriminator loss: 0.461315, acc: 0.710938]  [Adversarial loss: 0.428528, acc: 0.750000]\n",
            "1372: [Discriminator loss: 0.424529, acc: 0.695312]  [Adversarial loss: 0.391689, acc: 0.761719]\n",
            "1373: [Discriminator loss: 0.421636, acc: 0.707031]  [Adversarial loss: 0.392241, acc: 0.742188]\n",
            "1374: [Discriminator loss: 0.441947, acc: 0.671875]  [Adversarial loss: 0.421141, acc: 0.742188]\n",
            "1375: [Discriminator loss: 0.443102, acc: 0.718750]  [Adversarial loss: 0.391942, acc: 0.718750]\n",
            "1376: [Discriminator loss: 0.393401, acc: 0.765625]  [Adversarial loss: 0.400532, acc: 0.722656]\n",
            "1377: [Discriminator loss: 0.427255, acc: 0.710938]  [Adversarial loss: 0.407839, acc: 0.742188]\n",
            "1378: [Discriminator loss: 0.391796, acc: 0.765625]  [Adversarial loss: 0.391698, acc: 0.722656]\n",
            "1379: [Discriminator loss: 0.475288, acc: 0.710938]  [Adversarial loss: 0.414030, acc: 0.707031]\n",
            "1380: [Discriminator loss: 0.393483, acc: 0.757812]  [Adversarial loss: 0.397255, acc: 0.730469]\n",
            "1381: [Discriminator loss: 0.427447, acc: 0.714844]  [Adversarial loss: 0.380771, acc: 0.722656]\n",
            "1382: [Discriminator loss: 0.448136, acc: 0.746094]  [Adversarial loss: 0.396014, acc: 0.730469]\n",
            "1383: [Discriminator loss: 0.510860, acc: 0.660156]  [Adversarial loss: 0.393396, acc: 0.707031]\n",
            "1384: [Discriminator loss: 0.430569, acc: 0.664062]  [Adversarial loss: 0.407269, acc: 0.742188]\n",
            "1385: [Discriminator loss: 0.427937, acc: 0.667969]  [Adversarial loss: 0.383173, acc: 0.769531]\n",
            "1386: [Discriminator loss: 0.451340, acc: 0.707031]  [Adversarial loss: 0.398799, acc: 0.746094]\n",
            "1387: [Discriminator loss: 0.404043, acc: 0.753906]  [Adversarial loss: 0.366719, acc: 0.789062]\n",
            "1388: [Discriminator loss: 0.414350, acc: 0.691406]  [Adversarial loss: 0.349310, acc: 0.742188]\n",
            "1389: [Discriminator loss: 0.459411, acc: 0.722656]  [Adversarial loss: 0.382580, acc: 0.753906]\n",
            "1390: [Discriminator loss: 0.442044, acc: 0.699219]  [Adversarial loss: 0.383406, acc: 0.738281]\n",
            "1391: [Discriminator loss: 0.410670, acc: 0.679688]  [Adversarial loss: 0.383003, acc: 0.773438]\n",
            "1392: [Discriminator loss: 0.415737, acc: 0.714844]  [Adversarial loss: 0.403811, acc: 0.726562]\n",
            "1393: [Discriminator loss: 0.419501, acc: 0.726562]  [Adversarial loss: 0.379850, acc: 0.726562]\n",
            "1394: [Discriminator loss: 0.421863, acc: 0.714844]  [Adversarial loss: 0.400668, acc: 0.710938]\n",
            "1395: [Discriminator loss: 0.431169, acc: 0.695312]  [Adversarial loss: 0.383399, acc: 0.742188]\n",
            "1396: [Discriminator loss: 0.379945, acc: 0.726562]  [Adversarial loss: 0.360989, acc: 0.765625]\n",
            "1397: [Discriminator loss: 0.451433, acc: 0.699219]  [Adversarial loss: 0.426889, acc: 0.722656]\n",
            "1398: [Discriminator loss: 0.445050, acc: 0.722656]  [Adversarial loss: 0.416704, acc: 0.703125]\n",
            "1399: [Discriminator loss: 0.485516, acc: 0.632812]  [Adversarial loss: 0.406161, acc: 0.718750]\n",
            "1400: [Discriminator loss: 0.491735, acc: 0.675781]  [Adversarial loss: 0.417793, acc: 0.707031]\n",
            "1401: [Discriminator loss: 0.409266, acc: 0.671875]  [Adversarial loss: 0.398456, acc: 0.769531]\n",
            "1402: [Discriminator loss: 0.520087, acc: 0.640625]  [Adversarial loss: 0.421490, acc: 0.765625]\n",
            "1403: [Discriminator loss: 0.471026, acc: 0.640625]  [Adversarial loss: 0.432160, acc: 0.757812]\n",
            "1404: [Discriminator loss: 0.444052, acc: 0.667969]  [Adversarial loss: 0.421584, acc: 0.714844]\n",
            "1405: [Discriminator loss: 0.461322, acc: 0.652344]  [Adversarial loss: 0.385534, acc: 0.761719]\n",
            "1406: [Discriminator loss: 0.511017, acc: 0.644531]  [Adversarial loss: 0.413400, acc: 0.746094]\n",
            "1407: [Discriminator loss: 0.448060, acc: 0.691406]  [Adversarial loss: 0.392659, acc: 0.769531]\n",
            "1408: [Discriminator loss: 0.425143, acc: 0.714844]  [Adversarial loss: 0.429447, acc: 0.750000]\n",
            "1409: [Discriminator loss: 0.457914, acc: 0.625000]  [Adversarial loss: 0.403886, acc: 0.746094]\n",
            "1410: [Discriminator loss: 0.461574, acc: 0.695312]  [Adversarial loss: 0.395367, acc: 0.785156]\n",
            "1411: [Discriminator loss: 0.408460, acc: 0.707031]  [Adversarial loss: 0.370259, acc: 0.785156]\n",
            "1412: [Discriminator loss: 0.402858, acc: 0.714844]  [Adversarial loss: 0.394344, acc: 0.757812]\n",
            "1413: [Discriminator loss: 0.444048, acc: 0.671875]  [Adversarial loss: 0.417429, acc: 0.714844]\n",
            "1414: [Discriminator loss: 0.456102, acc: 0.636719]  [Adversarial loss: 0.451878, acc: 0.750000]\n",
            "1415: [Discriminator loss: 0.433357, acc: 0.695312]  [Adversarial loss: 0.386629, acc: 0.785156]\n",
            "1416: [Discriminator loss: 0.452173, acc: 0.695312]  [Adversarial loss: 0.374731, acc: 0.730469]\n",
            "1417: [Discriminator loss: 0.364231, acc: 0.738281]  [Adversarial loss: 0.348722, acc: 0.785156]\n",
            "1418: [Discriminator loss: 0.415256, acc: 0.691406]  [Adversarial loss: 0.353347, acc: 0.757812]\n",
            "1419: [Discriminator loss: 0.396660, acc: 0.722656]  [Adversarial loss: 0.420825, acc: 0.757812]\n",
            "1420: [Discriminator loss: 0.434099, acc: 0.722656]  [Adversarial loss: 0.382737, acc: 0.734375]\n",
            "1421: [Discriminator loss: 0.437652, acc: 0.687500]  [Adversarial loss: 0.375153, acc: 0.804688]\n",
            "1422: [Discriminator loss: 0.485122, acc: 0.652344]  [Adversarial loss: 0.430234, acc: 0.695312]\n",
            "1423: [Discriminator loss: 0.471672, acc: 0.671875]  [Adversarial loss: 0.395403, acc: 0.714844]\n",
            "1424: [Discriminator loss: 0.487160, acc: 0.667969]  [Adversarial loss: 0.426153, acc: 0.730469]\n",
            "1425: [Discriminator loss: 0.439811, acc: 0.726562]  [Adversarial loss: 0.397834, acc: 0.730469]\n",
            "1426: [Discriminator loss: 0.421526, acc: 0.710938]  [Adversarial loss: 0.338173, acc: 0.789062]\n",
            "1427: [Discriminator loss: 0.441269, acc: 0.667969]  [Adversarial loss: 0.367381, acc: 0.785156]\n",
            "1428: [Discriminator loss: 0.428952, acc: 0.679688]  [Adversarial loss: 0.408931, acc: 0.757812]\n",
            "1429: [Discriminator loss: 0.469027, acc: 0.664062]  [Adversarial loss: 0.436733, acc: 0.769531]\n",
            "1430: [Discriminator loss: 0.484193, acc: 0.664062]  [Adversarial loss: 0.453444, acc: 0.734375]\n",
            "1431: [Discriminator loss: 0.509636, acc: 0.683594]  [Adversarial loss: 0.486423, acc: 0.710938]\n",
            "1432: [Discriminator loss: 0.511382, acc: 0.636719]  [Adversarial loss: 0.419091, acc: 0.781250]\n",
            "1433: [Discriminator loss: 0.496006, acc: 0.644531]  [Adversarial loss: 0.458566, acc: 0.738281]\n",
            "1434: [Discriminator loss: 0.555233, acc: 0.605469]  [Adversarial loss: 0.485327, acc: 0.699219]\n",
            "1435: [Discriminator loss: 0.467687, acc: 0.648438]  [Adversarial loss: 0.448014, acc: 0.734375]\n",
            "1436: [Discriminator loss: 0.465909, acc: 0.652344]  [Adversarial loss: 0.458222, acc: 0.722656]\n",
            "1437: [Discriminator loss: 0.461837, acc: 0.675781]  [Adversarial loss: 0.466188, acc: 0.703125]\n",
            "1438: [Discriminator loss: 0.463497, acc: 0.691406]  [Adversarial loss: 0.446010, acc: 0.750000]\n",
            "1439: [Discriminator loss: 0.448116, acc: 0.718750]  [Adversarial loss: 0.442182, acc: 0.761719]\n",
            "1440: [Discriminator loss: 0.458958, acc: 0.648438]  [Adversarial loss: 0.449999, acc: 0.742188]\n",
            "1441: [Discriminator loss: 0.475030, acc: 0.683594]  [Adversarial loss: 0.480001, acc: 0.675781]\n",
            "1442: [Discriminator loss: 0.446157, acc: 0.703125]  [Adversarial loss: 0.460175, acc: 0.722656]\n",
            "1443: [Discriminator loss: 0.500059, acc: 0.632812]  [Adversarial loss: 0.505608, acc: 0.644531]\n",
            "1444: [Discriminator loss: 0.452545, acc: 0.675781]  [Adversarial loss: 0.439817, acc: 0.730469]\n",
            "1445: [Discriminator loss: 0.453430, acc: 0.679688]  [Adversarial loss: 0.436085, acc: 0.738281]\n",
            "1446: [Discriminator loss: 0.493879, acc: 0.636719]  [Adversarial loss: 0.478689, acc: 0.718750]\n",
            "1447: [Discriminator loss: 0.448544, acc: 0.640625]  [Adversarial loss: 0.504618, acc: 0.707031]\n",
            "1448: [Discriminator loss: 0.456735, acc: 0.671875]  [Adversarial loss: 0.408578, acc: 0.714844]\n",
            "1449: [Discriminator loss: 0.443413, acc: 0.742188]  [Adversarial loss: 0.400163, acc: 0.804688]\n",
            "1450: [Discriminator loss: 0.538794, acc: 0.660156]  [Adversarial loss: 0.426744, acc: 0.750000]\n",
            "1451: [Discriminator loss: 0.475314, acc: 0.687500]  [Adversarial loss: 0.461005, acc: 0.718750]\n",
            "1452: [Discriminator loss: 0.389066, acc: 0.699219]  [Adversarial loss: 0.389429, acc: 0.777344]\n",
            "1453: [Discriminator loss: 0.467818, acc: 0.695312]  [Adversarial loss: 0.408333, acc: 0.757812]\n",
            "1454: [Discriminator loss: 0.441793, acc: 0.707031]  [Adversarial loss: 0.446980, acc: 0.707031]\n",
            "1455: [Discriminator loss: 0.368350, acc: 0.750000]  [Adversarial loss: 0.362455, acc: 0.792969]\n",
            "1456: [Discriminator loss: 0.436170, acc: 0.718750]  [Adversarial loss: 0.430846, acc: 0.675781]\n",
            "1457: [Discriminator loss: 0.409612, acc: 0.765625]  [Adversarial loss: 0.384853, acc: 0.738281]\n",
            "1458: [Discriminator loss: 0.432518, acc: 0.722656]  [Adversarial loss: 0.389123, acc: 0.742188]\n",
            "1459: [Discriminator loss: 0.415036, acc: 0.746094]  [Adversarial loss: 0.398615, acc: 0.687500]\n",
            "1460: [Discriminator loss: 0.403204, acc: 0.750000]  [Adversarial loss: 0.441229, acc: 0.726562]\n",
            "1461: [Discriminator loss: 0.370890, acc: 0.781250]  [Adversarial loss: 0.385657, acc: 0.722656]\n",
            "1462: [Discriminator loss: 0.414632, acc: 0.699219]  [Adversarial loss: 0.430269, acc: 0.730469]\n",
            "1463: [Discriminator loss: 0.439480, acc: 0.738281]  [Adversarial loss: 0.385444, acc: 0.746094]\n",
            "1464: [Discriminator loss: 0.507580, acc: 0.632812]  [Adversarial loss: 0.473761, acc: 0.714844]\n",
            "1465: [Discriminator loss: 0.420555, acc: 0.734375]  [Adversarial loss: 0.386850, acc: 0.757812]\n",
            "1466: [Discriminator loss: 0.447280, acc: 0.695312]  [Adversarial loss: 0.423663, acc: 0.691406]\n",
            "1467: [Discriminator loss: 0.422219, acc: 0.742188]  [Adversarial loss: 0.392364, acc: 0.789062]\n",
            "1468: [Discriminator loss: 0.420194, acc: 0.710938]  [Adversarial loss: 0.406055, acc: 0.730469]\n",
            "1469: [Discriminator loss: 0.464628, acc: 0.695312]  [Adversarial loss: 0.446306, acc: 0.707031]\n",
            "1470: [Discriminator loss: 0.452463, acc: 0.718750]  [Adversarial loss: 0.463818, acc: 0.699219]\n",
            "1471: [Discriminator loss: 0.477456, acc: 0.679688]  [Adversarial loss: 0.443882, acc: 0.714844]\n",
            "1472: [Discriminator loss: 0.420541, acc: 0.718750]  [Adversarial loss: 0.408475, acc: 0.734375]\n",
            "1473: [Discriminator loss: 0.487847, acc: 0.648438]  [Adversarial loss: 0.464658, acc: 0.703125]\n",
            "1474: [Discriminator loss: 0.432102, acc: 0.683594]  [Adversarial loss: 0.453470, acc: 0.714844]\n",
            "1475: [Discriminator loss: 0.447012, acc: 0.675781]  [Adversarial loss: 0.448671, acc: 0.710938]\n",
            "1476: [Discriminator loss: 0.481255, acc: 0.667969]  [Adversarial loss: 0.424480, acc: 0.707031]\n",
            "1477: [Discriminator loss: 0.467763, acc: 0.660156]  [Adversarial loss: 0.474541, acc: 0.707031]\n",
            "1478: [Discriminator loss: 0.421409, acc: 0.710938]  [Adversarial loss: 0.406125, acc: 0.796875]\n",
            "1479: [Discriminator loss: 0.409238, acc: 0.710938]  [Adversarial loss: 0.405657, acc: 0.746094]\n",
            "1480: [Discriminator loss: 0.451741, acc: 0.671875]  [Adversarial loss: 0.426782, acc: 0.773438]\n",
            "1481: [Discriminator loss: 0.417548, acc: 0.691406]  [Adversarial loss: 0.396063, acc: 0.742188]\n",
            "1482: [Discriminator loss: 0.459447, acc: 0.679688]  [Adversarial loss: 0.448764, acc: 0.746094]\n",
            "1483: [Discriminator loss: 0.442594, acc: 0.687500]  [Adversarial loss: 0.430716, acc: 0.710938]\n",
            "1484: [Discriminator loss: 0.541018, acc: 0.589844]  [Adversarial loss: 0.440859, acc: 0.761719]\n",
            "1485: [Discriminator loss: 0.419347, acc: 0.648438]  [Adversarial loss: 0.410161, acc: 0.765625]\n",
            "1486: [Discriminator loss: 0.476875, acc: 0.656250]  [Adversarial loss: 0.434453, acc: 0.734375]\n",
            "1487: [Discriminator loss: 0.517616, acc: 0.632812]  [Adversarial loss: 0.469071, acc: 0.726562]\n",
            "1488: [Discriminator loss: 0.445118, acc: 0.675781]  [Adversarial loss: 0.433584, acc: 0.738281]\n",
            "1489: [Discriminator loss: 0.483291, acc: 0.671875]  [Adversarial loss: 0.425283, acc: 0.769531]\n",
            "1490: [Discriminator loss: 0.454606, acc: 0.683594]  [Adversarial loss: 0.433860, acc: 0.707031]\n",
            "1491: [Discriminator loss: 0.464408, acc: 0.679688]  [Adversarial loss: 0.416348, acc: 0.777344]\n",
            "1492: [Discriminator loss: 0.427035, acc: 0.671875]  [Adversarial loss: 0.410478, acc: 0.738281]\n",
            "1493: [Discriminator loss: 0.427834, acc: 0.683594]  [Adversarial loss: 0.398608, acc: 0.777344]\n",
            "1494: [Discriminator loss: 0.462548, acc: 0.660156]  [Adversarial loss: 0.392684, acc: 0.718750]\n",
            "1495: [Discriminator loss: 0.475869, acc: 0.699219]  [Adversarial loss: 0.410319, acc: 0.734375]\n",
            "1496: [Discriminator loss: 0.477457, acc: 0.683594]  [Adversarial loss: 0.426766, acc: 0.726562]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEmpty\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-be84d2bf2e43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdiscriminator_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madversarial_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madversarial_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgerman_credit_bbgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-4d127282bbfb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_steps, batch_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;31m# print(samples_fake)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1 if y has the real value, 0 otherwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    667\u001b[0m             delayed(_accumulate_prediction)(e.predict_proba, X, all_proba,\n\u001b[1;32m    668\u001b[0m                                             lock)\n\u001b[0;32m--> 669\u001b[0;31m             for e in self.estimators_)\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mproba\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_proba\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m                     tasks = BatchedCalls(islice[i:i + final_batch_size],\n\u001b[0;32m--> 848\u001b[0;31m                                          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_nested_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m                                          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reducer_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m                                          self._pickle_cache)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mget_nested_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# SequentialBackend should neither change the nesting level, the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# default backend or the number of jobs. Just return the current one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_active_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mget_active_backend\u001b[0;34m(prefer, require, verbose)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m# We are outside of the scope of any parallel_backend context manager,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# create the default backend instance now.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBACKENDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDEFAULT_BACKEND\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnesting_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0msupports_sharedmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_sharedmem'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0muses_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'uses_threads'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_effective_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DEFAULT_EFFECTIVE_BATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smoothed_batch_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DEFAULT_SMOOTHED_BATCH_DURATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nesting_level, inner_max_num_threads, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     def __init__(self, nesting_level=None, inner_max_num_threads=None,\n\u001b[1;32m     35\u001b[0m                  **kwargs):\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnesting_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnesting_level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_max_num_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner_max_num_threads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXeijM5E9R7y"
      },
      "source": [
        "####Plot German Credit BBGAN Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t76jNkM89RWA"
      },
      "source": [
        "plot_graphs(discriminator_losses, adversarial_losses, \"discriminator_loss\", \"adversarial_loss\", \"Discriminator and adversarial losses\", \"steps\", \"loss\")\n",
        "plot_graphs(discriminator_accuracies, adversarial_accuracies, \"discriminator_accuracy\", \"adversarial_accuracy\", \"Discriminator and adversarial accuracies\", \"steps\", \"accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tddr3ha69dbD"
      },
      "source": [
        "####Generate Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "UcDX0EbW9gb6",
        "outputId": "4c5998de-7ce2-4e72-bd7e-531d5686d406"
      },
      "source": [
        "gen_samples, c = german_credit_bbgan.generate_samples(num_samples=1000)\n",
        "gen_samples = pd.DataFrame(gen_samples)\n",
        "descaled_gen_samples = descale_german_credit_data(gen_samples, german_data_scaler)\n",
        "descaled_gen_samples = pd.DataFrame(descaled_gen_samples)\n",
        "decoded_descaled_gen_samples = decode_cat_features(descaled_gen_samples, bins_dict, group_names_dict)\n",
        "\n",
        "decoded_descaled_gen_samples\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>4.000306</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>251.029480</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>1.000234</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>1.000054</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>19.000454</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.000413</td>\n",
              "      <td>b'A171'</td>\n",
              "      <td>1.000071</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>4.000001</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>250.011444</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>19.000002</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.000003</td>\n",
              "      <td>b'A171'</td>\n",
              "      <td>1.000001</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>250.000412</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A171'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>250.000061</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A171'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A31'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>b'A142'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'A202'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>4.002503</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>255.558731</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>1.002426</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>1.000348</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>19.003376</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.002306</td>\n",
              "      <td>b'A171'</td>\n",
              "      <td>1.000381</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>4.000007</td>\n",
              "      <td>b'A34'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>250.049408</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>1.000002</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>1.000002</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>19.000013</td>\n",
              "      <td>b'A143'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.000014</td>\n",
              "      <td>b'A171'</td>\n",
              "      <td>1.000003</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A201'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>NaN</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A31'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>b'A142'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'A202'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>NaN</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A31'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>b'A142'</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b'A202'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>b'A11'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A31'</td>\n",
              "      <td>b'A43'</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>b'A65'</td>\n",
              "      <td>b'A75'</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>b'A93'</td>\n",
              "      <td>b'A101'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A121'</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>b'A142'</td>\n",
              "      <td>b'A152'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A173'</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>b'A192'</td>\n",
              "      <td>b'A202'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1       2       3   ...       16        17       18       19\n",
              "0    b'A11'  4.000306  b'A34'  b'A43'  ...  b'A171'  1.000071  b'A192'  b'A201'\n",
              "1    b'A11'  4.000001  b'A34'  b'A43'  ...  b'A171'  1.000001  b'A192'  b'A201'\n",
              "2    b'A11'  4.000000  b'A34'  b'A43'  ...  b'A171'  1.000000  b'A192'  b'A201'\n",
              "3    b'A11'  4.000000  b'A34'  b'A43'  ...  b'A171'  1.000000  b'A192'  b'A201'\n",
              "4       NaN  4.000000  b'A31'     NaN  ...      NaN  1.000000      NaN  b'A202'\n",
              "..      ...       ...     ...     ...  ...      ...       ...      ...      ...\n",
              "995  b'A11'  4.002503  b'A34'  b'A43'  ...  b'A171'  1.000381  b'A192'  b'A201'\n",
              "996  b'A11'  4.000007  b'A34'  b'A43'  ...  b'A171'  1.000003  b'A192'  b'A201'\n",
              "997     NaN  4.000000  b'A31'     NaN  ...      NaN  1.000000      NaN  b'A202'\n",
              "998     NaN  4.000000  b'A31'     NaN  ...  b'A173'  1.000000      NaN  b'A202'\n",
              "999  b'A11'  4.000000  b'A31'  b'A43'  ...  b'A173'  1.000000  b'A192'  b'A202'\n",
              "\n",
              "[1000 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v3NyVxhXe7Z"
      },
      "source": [
        "y = german_credit_rf_clf.predict_proba(descaled_gen_samples)[:, 1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "KjQCZktQXd2_",
        "outputId": "385a6bdf-474e-4b66-d611-f54f36e6c1b6"
      },
      "source": [
        "plt.hist(y, rwidth=0.8)\n",
        "plt.xlabel(\"Confidence\")\n",
        "plt.ylabel(\"Number of cases\")\n",
        "plt.title(\"Confidence scores distribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}